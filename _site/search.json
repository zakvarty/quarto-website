[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Steal like a generative artist\n\n\n\n\n\nA first forray into generative art. \n\n\n\n\n\n2024-10-01\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nMethod of Moments Estimation for Randomised Response Surveys\n\n\n\n\n\nMethod of moments estimation for common data models and randomised response designs.\n\n\n\n\n\n2023-11-09\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nNotably Inaccessible\n\n\n\n\n\nA data driven investigation into the (in)accessibility of data science notebooks. \n\n\n\n\n\n2023-10-03\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nBib but Better\n\n\n\n\n\nTemplates for all the bib entries you never knew you needed. \n\n\n\n\n\n2023-09-13\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nWhy is My Classifier Discrimanatory?\n\n\n\n\n\nA fresh view on understanding and addressing unfairness in classification (and regression) models. \n\n\n\n\n\n2023-08-17\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Improve Your Relationship with Your Future Self\n\n\n\n\n\nReproducible workflows are not just good for science. \n\n\n\n\n\n2023-08-15\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nOvercoming Barriers to Sharing Code\n\n\n\n\n\nTalk summary from the Toronto Workshop on Reproducibility in February 2023. \n\n\n\n\n\n2023-08-08\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nThe Mundanity of Excellence\n\n\n\n\n\nExceptional performance is acheived by doing the small, boring stuff really well. \n\n\n\n\n\n2023-07-26\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nCounting the Countless\n\n\n\n\n\nIs data science compatible with queerness? \n\n\n\n\n\n2023-07-14\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nPackaging your R code\n\n\n\n\n\nA tutorial to guide you as you make your first (or hundredth) minimal R package. \n\n\n\n\n\n2023-01-19\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\n\n\n\nGet your data ducks in a line using base R and the tidyverse. \n\n\n\n\n\n2023-01-06\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nAquiring Data via an API\n\n\n\n\n\nPart 2 of a mini-series on aquiring data from the web, focusing on data aquisition via APIs. \n\n\n\n\n\n2022-12-14\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nGit: putting your new feature out into the world.\n\n\n\n\n\nA git workflow to put a local feature out in the wild, then tidy up after yourself. \n\n\n\n\n\n2022-12-13\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping with {rvest}\n\n\n\n\n\nPart 1 of a mini-series on aquiring data from the web, focusing on scraping webpages using the {rvest} package. \n\n\n\n\n\n2022-12-01\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nGood Enough Practices in Scientific Computing\n\n\n\n\n\nReading Summary of Wilson et al. (2017). \n\n\n\n\n\n2022-10-19\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nData Journalism: Recreating a Professional Plot\n\n\n\n\n\nRecreating a plot of record breaking temperatures by the BBC data journalism team. \n\n\n\n\n\n2022-10-15\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Ravelry Yarn\n\n\n\n\n\nTidy Tuesday 2022 || Week 41 \n\n\n\n\n\n2022-10-12\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nRecommended Statistics Books\n\n\n\n\n\nBook recommendations for those looking to reinforce their knowledge of undergraduate and advanced statistics. \n\n\n\n\n\n2022-10-10\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nWriting a rhetorical précis\n\n\n\n\n\nStructured summaries to remember and retrieve what you read \n\n\n\n\n\n2022-10-07\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up a quarto blog\n\n\n\n\n\nAdding a blog within a quarto website \n\n\n\n\n\n2022-09-26\n\n\nZak Varty\n\n\n\n\n\n\n\n\n\n\n\n\nHello, World!\n\n\n\n\n\nA minimal first post \n\n\n\n\n\n2022-09-26\n\n\nZak Varty\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bookmarks.html",
    "href": "bookmarks.html",
    "title": "Bookmarks",
    "section": "",
    "text": "Bookmarks\nA collection of helpful resources that I might want to find again in the future or point other people towards.\n\nAdvice: Faculty\n\nThread by Sarah Sheffield (@sarahsheffield) advice for new faculty.\nCourse Free 10 week course on google drive about writing academic an academic syllabus. Course by @DLabree, recommended by Elena Aydarova (@aydarova).\nTweet by Gero Grams (@GeroGrams) on how to say no to things\nThread by Mine Dogucu (@MineDogucu) on learning/teaching (with) R. See also:\n\nCompilation of free R resources\nBayesian inference book\nTeaching webpage\nBlog on data pedagogy\n\nThread by Wes Kao (@wes_kao) on managing up\nThread by Vrinda Nair (@VnVrinda) on 22 tools for your PhD Journey\nManaging research careers tool by Edinburgh University Thread and Webpage\nTalk by Laura Albert (@lauraalbertphd) on time management: Do less, Do it faster, Do it at the right time.\n\n\n\nAdvice: Graduate Students\n\nThread by Matt Betts on 8 questions to consider before starting a PhD.\nThread by Maram Duncan (@MCDuncanLab) on the hidden curriculum for new grad students\nThread by @LifeAfterMyPhD on 5 low-stakes steps to set yourself up for an industry job search\nThread by @mathladyhazel on the best math books for self-learners\nThread by Alex Eble (@alexeble) on adivce for thriving in a PhD. Full document here. Note: has US and economics focus, but translates well.\nWebsite Stats Notes in the British Medical Journal. Like a dictionary but for stats words and methods.\nBook Esstential Math for Data Scienceby Thomas Nield. Recommended by Vicki Boykis for those tooking for an intro/refresher on linear algebra, probability and statistics.\n\n\n\nAgent based modelling\n\nPaper review of agent based model (preprint of JEL - chunky at 90 pages!)\n\n\n\nAnalysis and Asymptotics\n\nVideo Lectures by Steven Strogatz (@stevenstrogatz) on asymptotics and pertubation methods.\n\n\n\nCausal Inference\n\nLecture Notes by Matt Blackwell, “Causal Inference with Applications”\nPaper The taboo against explicit causal inference in nonexperimental psychology. Suggested by Brian Nosel (@BrianNosek)\nThread by Volodymyr Kuleshov (@volokuleshov) about the ICML 2022 tutorial on Causality and Fairness\nVideo Science before statistics: Causal Inference by Richard McElreath (3 hour crash course in causal inference)\nVideo Series Statistical Rethinking (2022) by Richard McElreath on Youtube\n\n\n\nCoding (General)\n\nShort-form\n\nArticle on setting up a private .gitignore to keep a clean codebase\nBook / website on package development in Python. (Think Hadley & Bryan’s R packages but for Python) Recommended by @EmilyRiederer\nBook The missing readme by Chris Riccomini and Dmitriy Ryaboy\nPaper Ten Simple Rules for Taking Advantage of Git and GitHub\nSildes by Ariel Muldoon (@aosmith16) on “More git and github - collaborators, merege conflicts and pull requests”\nDocs for {renv}\n\n\n\nLong-form\n\nIntermediate programming course\nMissing Semester (MIT Computer Science)\nBuilding reproducible analytical pipelines with R\nStatistical Programming Paradigms and Workflows\n\n\n\n\nDocker\n\nThread by Alex Gold (@alexkgold) on getting started with Docker. Free online book\n\n\n\nDatabases\n\nArticle by architecture notes (@arcnotes) on “Things you should know about databases”\n\n\n\nDatasets\n\nThread by R Ladies - Sources of Messy(ish) data\n\n\n\nData Ethics\n\nThe Verge - AI Drug development maske chemical weapons\nNature - AI Drug development maske chemical weapons\nRichard McElreath recommendation of paper by Xiao-Li Meng on how data quality influences effective sample size\nImperial Explainable AI Seminars\nThread by Santiago (@svpino) on imbalanced datasets\nTweet by Adam Kruchten (@AdamKruchten) on when we care about marginal or conditional effects.\nASA article on the 2020 work and salary survey, showing women tend to earn less in base salary and total income but that in a regression gender is not significant predictor of total income.\nNPR story Where Google find that men are underpaid\nThread by Paul Hunermund (@PHunermund) on the above google article.\nPreprint on reconstructing large portions of training data from a trained neural network\nPaper Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. Suggested by Abeba Birhane (@Abebab)\nPaper Big data loses to good data. Unrepresentative big surveys significantly overestimated US vaccine uptake.\nPaper On extending LinearSHAP, TreeSHAP and DeepSHAP to RKHS-SHAP. Found through tweet by Siu Lun Chau (@Chau9991).\nChapter 33 on interpretability of book Probabilistic Machine Learning: Advacned Topicsby Kevin Murphy (@sirbayes), Been Kim (@_beenkim) and others.\nBook by Claire McKay Bowen “Protecting your privacy in a data-driven world”\nTweet by Rasha Shrain (@rashaben) requesting reading materials on p-values and p-hacking\nCourse 12 week reading course on Ehics and Data Science by Rohan Alexander\n\n\n\nData Visualisation\n\nThread by Indrajeet Patil (@patilindrajeets) on the effective use of colours in data vis.\nR Package {performance} for aesthetically pleasing ggplot sytle diagnostic plots. (The qq plot even has tolerance intervals!)\nBlog Post by Thomas Mock on Creating and using custom ggplot2 themes\nBlog Posts by Ameila McNamara (@AmeliaMN) about Histograms and Kernel Density Estimation\n\n\n\nHistory of Statistics\n\nCourse 10-week reading list on “History of Statistics and Data Sciences” by Rohan Alexander\n\n\n\nMarkdown\n\nTweet by Steve Bauman (@realstevebauman) pointing out that Github’s markdown now supports note and warning blockquotes\nTweet by Zev Ross on using the character ├ to get aesthetically pleasing subsections in RStudio\nTutorial on embedding mentimeter into webpages (try with html slides???)\n\n\n\nMemes\n\nStatus code 400 meme by @da_667\nThe binary search tree actually exists by Ahmad Awais (@MrAhmadAwais)\nData Science Dinosaur A computer science python eating a statistics elephant\n\n\n\nScience Communication\n\nThread by Carl Bergstrom (@CT_Bergstrom) and Ryan McGee (@RS_McGee) telling the story of a paper using a comic strip and stick-figure Darwin.\nThread by Tessa Davis on slide design to keep your audience engaged\nThread by Dorsa Amir (@DorsaAmir) on slide design\nWebsite OpenPeeps - Open Source hand drawn individual characters\nBlog Post by Kate Jolly (@katejolly6) on designing slides in xaringan with xaringanthemer and css.\n\n\n\nMachine Learning\n\nBlog Post on double descent in neural network performance ### Optimisation\nVideo by Trefor Bazett (@TreforBazett) on using Lagrange multipliers to solve constrained optimisation problems\nVideo Series by @3blue1brown on constrained optimisation (hosted on khan academy)\nCourse Notes Advanced Data Analysis from an Elementary Point of View\n\n\n\nParquet\n\nThread by Pau Labarta Bajo (@paulabartabajo_)\n\n\n\nPoint processes\n\nCourse Material By Rick Schoenberg on point process models\nBlog Post by Benjamin Cretois on fitting point process models in stan.\n\n\n\nProfessional Development\n\nTweet by Francisco Yirá (@francisco_yira) about designing a personal learning plan.\n\n\n\nQuarto\n\nSamatha Csik - Creating Quarto Websites\n\n\n\nSQL\n\nThread by Tom Carpenter (@tcarpenter216) on translating dplyr skills to SQL\nOnline resources for learning SQL, as recommended by Ijeoma Okereafor (@MeetIjeoma)\n\nhttp://sqlbolt.com\nhttp://w3schools.com/sql\nhttp://mode.com/sql-tutorial\nhttp://sqlteaching.com\nhttp://SQLZoo.net\nhttp://selectstarsql.com\n\nhttps://pgexercises.com\n\nSQL games recommended by Vikas Rajputin (@vikasrajputin)\n\n(SQL Island)[https://sql-island.informatik.uni-kl.de/] (In German but chrome translation is pretty good)\n(SQL Murder Mystery)[https://mystery.knightlab.com/]\n(SQL Polic Department)[https://sqlpd.com/]\n\n\n\n\nWorkflow\n\nJenny Bryan - Naming Things (Slides)\nTalk by David Robinson on “The unreasonable effectiveness of public work”\n\n\n\nWriting\n\nBooks on writing suggested by Helen Sword, author of Stylish Academic Writing."
  },
  {
    "objectID": "blog/2023-08-17-why-is-my-classifier-discrimanatory/index.html",
    "href": "blog/2023-08-17-why-is-my-classifier-discrimanatory/index.html",
    "title": "Why is My Classifier Discrimanatory?",
    "section": "",
    "text": "chen2018why (Chen, Johansson, and Sontag 2018)\nTitle: Why is My Classifier Discriminatory? {ArXiV preprint, 2018} (9 pages).\nAuthors: Irene E. Chen, Fredrik D. Johansson and David Sontag. (Massachusetts Institute of Technology)\nKey words: Fairness, Data Ethics, Data Science.\n\n\n\n\n\nFairness is often viewed as a secondary objective in machine learning, which must be bought at some cost to a model’s predictive performance. Chen, Johansson, and Sontag (2018) argue that when machine learning is applied in areas such as medicine, finance or criminal justice, any misclassification will have serious and lasting consequences for the individuals involved and therefore any such compromises are not acceptable.\nThe authors argue that we should instead try to understand the source of the unfairness within a classifier and how this might be addressed by targeted collection of additional data. The authors decompose common loss functions into three components (which they call bias, variance and noise) and use these to advise on the collection of additional data."
  },
  {
    "objectID": "blog/2023-08-17-why-is-my-classifier-discrimanatory/index.html#reading-summary",
    "href": "blog/2023-08-17-why-is-my-classifier-discrimanatory/index.html#reading-summary",
    "title": "Why is My Classifier Discrimanatory?",
    "section": "",
    "text": "chen2018why (Chen, Johansson, and Sontag 2018)\nTitle: Why is My Classifier Discriminatory? {ArXiV preprint, 2018} (9 pages).\nAuthors: Irene E. Chen, Fredrik D. Johansson and David Sontag. (Massachusetts Institute of Technology)\nKey words: Fairness, Data Ethics, Data Science.\n\n\n\n\n\nFairness is often viewed as a secondary objective in machine learning, which must be bought at some cost to a model’s predictive performance. Chen, Johansson, and Sontag (2018) argue that when machine learning is applied in areas such as medicine, finance or criminal justice, any misclassification will have serious and lasting consequences for the individuals involved and therefore any such compromises are not acceptable.\nThe authors argue that we should instead try to understand the source of the unfairness within a classifier and how this might be addressed by targeted collection of additional data. The authors decompose common loss functions into three components (which they call bias, variance and noise) and use these to advise on the collection of additional data."
  },
  {
    "objectID": "blog/2023-08-17-why-is-my-classifier-discrimanatory/index.html#notes",
    "href": "blog/2023-08-17-why-is-my-classifier-discrimanatory/index.html#notes",
    "title": "Why is My Classifier Discrimanatory?",
    "section": "Notes",
    "text": "Notes\n\nIdentifying unfairness through loss-decomposition\nIf we use predictors \\(X\\) to model an outcome \\(y\\), resulting in predictions \\(\\hat y\\), then the loss function may be considered as the sum of three terms:\n\nBias is the loss contribution caused by using a particular model and a finite dataset to approximate the joint distribution \\(\\pi(X,y)\\) as \\(\\hat\\pi(X,y)\\).\nVariance is the expected loss contribution caused by sampling variability in model predictions under a particular model (\\(L(X,y)\\) vs \\(L(X^\\prime,y^\\prime)\\)).\nNoise is then remaining loss that is independent of the model used, caused by variability in the response that cannot be attributed to any of the measured predictors.\n\nSuch a loss-decomposition for each level of a protected attribute, across which we want to ensure fairness.\n\nDifferences in bias indicate that the model is insufficiently flexible to approximate \\(\\pi(X,y|A=a_i)\\) for all levels of the protected attribute. Differences in bias between protected subgroups could be addressed by considering a more flexible model, or separate models for each subgroup.\nDifferences in variance might be caused by differences in sample size (e.g. \\(n_{a_i} &gt;&gt; n_{a_j}\\)) or differences in predictor variability between groups (e.g. \\(\\text{Var}(X|A=a_i) &gt;&gt; \\text{Var}(X|A=a_j)\\)). Differences in variance might be addressed by targeted collection of further observations from under-represented or highly variable subgroups (i.e., adding rows to the data).\nDifferences in the noise term suggest that, for some level(s) of the protected attribute, there is additional variability in \\(y\\) that cannot be described by the measured set of predictors. Differences in the noise contribution are unrelated to the model choice or dataset size and “may only be reduced by measuring additional variables” (i.e., adding columns to the data).\n\n\n\nThe cliche of more data\nImproving model performance by collecting more data is far from an original suggestion. Chen, Johansson, and Sontag (2018) provide novelty by suggesting what data and how much of it to collect, in order to improve the fairness of the resulting classifier.\n\nWhen there are large difference in the variance term between subgroups, we must decide how many additional observations to make in each protected subgroup. Chen, Johansson, and Sontag (2018) model the expected prediction loss within each subgroup (\\(a \\in A\\)) as a function of sample size using a shifted inverse power-law distribution:\n\\[ \\bar L_{\\text{pred}}(n_a) = \\alpha_a n_a^{-\\beta_a} + \\delta_a,\\]\nwhere \\((\\alpha_a, \\beta_a, \\delta_a)\\) are subgroup-specific parameters that are to be estimated. The fitted power-law functions are extrapolated in \\(n_a\\) to identify the subgroups in which further data collection would be most impactful.\nWhen there are large differences in the error term between subgroups, we aim to collect additional predictors to describe variation in the response that is unexplained by the current predictors. Chen, Johansson, and Sontag (2018) suggest a method of identifying additional predictors. The data are clustered based on the current predictor set and discrimination is quantified within each cluster. Expert opinion can then be elicited to suggest additional predictors could further distinguish between outcomes for members of these clusters.\n\nThe authors go on to demonstrate these methods on three example datasets: predicting income using demographic information, predicting mortality using the text in clinical notes, and predicting book ratings using review text."
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html",
    "href": "blog/2022-12-13-git-remote-branches/index.html",
    "title": "Git: putting your new feature out into the world.",
    "section": "",
    "text": "So you’ve added a new feature to your project. You were responsible and didn’t do this in the main branch. Congrats on being responsible, have some brownie points.\nNow how the heck to you get everything moved across and delete your new-feature branch?"
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#check-that-your-local-main-is-up-to-date.",
    "href": "blog/2022-12-13-git-remote-branches/index.html#check-that-your-local-main-is-up-to-date.",
    "title": "Git: putting your new feature out into the world.",
    "section": "1. Check that your local main is up to date.",
    "text": "1. Check that your local main is up to date.\nBe in the main branch and pull any changes to the remote main branch.\ngit checkout main \ngit pull origin main\nThis might require you to resolve some merge conflicts, but these should be pretty straightforward if you are following a branch-and-merge workflow."
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#check-that-you-remote-new-feature-is-up-to-date.",
    "href": "blog/2022-12-13-git-remote-branches/index.html#check-that-you-remote-new-feature-is-up-to-date.",
    "title": "Git: putting your new feature out into the world.",
    "section": "2. Check that you remote new-feature is up to date.",
    "text": "2. Check that you remote new-feature is up to date.\nWe might mess things up while merging. Let’s make sure that if that happens we can get back to this good position with our new feature.\ngit checkout new-feature\ngit status\nIf needed: add, commit and push."
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#merge-any-changes-to-main-into-your-local-new-feature-branch",
    "href": "blog/2022-12-13-git-remote-branches/index.html#merge-any-changes-to-main-into-your-local-new-feature-branch",
    "title": "Git: putting your new feature out into the world.",
    "section": "3. Merge any changes to main into your local new-feature branch",
    "text": "3. Merge any changes to main into your local new-feature branch\nNext, we will make sure we have any changes to main moved across to our local new-feature branch.\ngit merge main\nThis might again require resolving some merge conflicts. Keep calm and take tea breaks are required."
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#commit-and-push-to-remote.",
    "href": "blog/2022-12-13-git-remote-branches/index.html#commit-and-push-to-remote.",
    "title": "Git: putting your new feature out into the world.",
    "section": "4. Commit and push to remote.",
    "text": "4. Commit and push to remote.\nNow that we have our local new-feature branch compatible with the remote main branch, lets push that to the remote.\ngit add &lt;YOUR_FILES_TO_COMMIT&gt;\ngit commit -m \"merge changes to main in preparation for PR\"\ngit push"
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#open-a-pull-request-on-github",
    "href": "blog/2022-12-13-git-remote-branches/index.html#open-a-pull-request-on-github",
    "title": "Git: putting your new feature out into the world.",
    "section": "5. Open a pull request on Github",
    "text": "5. Open a pull request on Github\nWait for someone to review approve your new feature (or wait a few hours/days and do it yourself for a solo project)."
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#delete-the-local-branch",
    "href": "blog/2022-12-13-git-remote-branches/index.html#delete-the-local-branch",
    "title": "Git: putting your new feature out into the world.",
    "section": "6. Delete the local branch",
    "text": "6. Delete the local branch\ngit checkout main\ngit branch --delete new-feature"
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#delete-the-remote-branch",
    "href": "blog/2022-12-13-git-remote-branches/index.html#delete-the-remote-branch",
    "title": "Git: putting your new feature out into the world.",
    "section": "7. Delete the remote branch",
    "text": "7. Delete the remote branch\ngit push origin --delete new-feature\nOn older versions of git (&lt; 1.7.0) you might need to use the alternative syntax below. This is effectively pushing nothing to the new-feature branch of origin.\ngit push origin :new-feature\nBam! You did it! Your new-feature is out there in the wild, making the world a marginally better place."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html",
    "href": "blog/2023-01-16-minimal-R-package/index.html",
    "title": "Packaging your R code",
    "section": "",
    "text": "This tutorial will guide you through the creation your first R package using the {usethis} and {testthat} packages.\nWe’ll walk through the steps of setting up a new R package, adding functions and documentation, and creating tests to ensure your package is reliable and easy to use. Whether you’re a seasoned R programmer or just getting started, this tutorial will provide you with the tools you need to create your own R package and share your work with others."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#introduction",
    "href": "blog/2023-01-16-minimal-R-package/index.html#introduction",
    "title": "Packaging your R code",
    "section": "",
    "text": "This tutorial will guide you through the creation your first R package using the {usethis} and {testthat} packages.\nWe’ll walk through the steps of setting up a new R package, adding functions and documentation, and creating tests to ensure your package is reliable and easy to use. Whether you’re a seasoned R programmer or just getting started, this tutorial will provide you with the tools you need to create your own R package and share your work with others."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#what-is-a-package",
    "href": "blog/2023-01-16-minimal-R-package/index.html#what-is-a-package",
    "title": "Packaging your R code",
    "section": "What is a package",
    "text": "What is a package\nAn R package is a collection of R functions, data, and documentation that can be easily shared with and installed by others.\nPackages let you extending the functionality of the base R system, and are a fundamental unit of reproducible research. They can be created by anyone, and are easily distributable to others through the CRAN or GitHub.\nThe packages you create can be imported and used across multiple R scripts, making them an ideal way to share and use functions and data across projects. They can also be a good way to organize and structure your code, making it easier to test, document and maintain.\nIn this session I hope to convince you that if you can write an R function, you can write an R package."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#principle",
    "href": "blog/2023-01-16-minimal-R-package/index.html#principle",
    "title": "Packaging your R code",
    "section": "20:80 Principle",
    "text": "20:80 Principle\nProgramming and package development are huge topics. In this session we will not even cover 20% of everything there is to know. However, by focusing on the most important and the most common aspects of package development we will rapidly get a minimal package up and running.\nThese basics will cover ~80% of everything you ever need to do during package development. Resources that were used to develop this tutorial, and which will support that last ~20%, are listed below.\n\nHilary Parker blog post\nKarl Broman primer\nShannon Pileggi blog post\nR packages by Wickham and Bryan (Ch 2)\nWriting R extensions CRAN\n\n\n\n\nArtwork by Ailson Horst"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#tools-to-build-a-minimal-r-package",
    "href": "blog/2023-01-16-minimal-R-package/index.html#tools-to-build-a-minimal-r-package",
    "title": "Packaging your R code",
    "section": "Tools to build a minimal R package",
    "text": "Tools to build a minimal R package\nYou will need:\n\nR and Rstudio\n{devtools}\n{usethis}\n{testthat}\n{roxygen2}\n\nThis collection of software and R packages makes it easy to create, develop, document, test, check and share the packages you create.\n\n\n\n\n\n\nNote\n\n\n\nFor the “hardcore” folks you can do all of this by hand, but it is an absolute pain. These tools were developed for a reason."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#create-a-template-directory",
    "href": "blog/2023-01-16-minimal-R-package/index.html#create-a-template-directory",
    "title": "Packaging your R code",
    "section": "Create a template directory",
    "text": "Create a template directory\nOnce per package\n\n\n\nConsole\n\nusethis::create_package(path = \"~/path/to_your_package/packageName\")\n\n\nThis should be:\n\nIn your home directory, near where your other projects live;\nNot in an existing project, package or git repo;\nNot your R library where your packages are installed.\n\nYou can find out where your R packages are installed using .libpaths(). This is where install.packages() saves packages and where library() looks for them by default.\n\n.libPaths()\n\n[1] \"/Users/zakvarty/R_libraries/4.2/cran\"                          \n[2] \"/Users/zakvarty/R_libraries/4.2/github\"                        \n[3] \"/Users/zakvarty/R_libraries/4.2/personal\"                      \n[4] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"\n\n\nNote that yours will probably only return one entry. I’ve set things up to keep my R packages separated according to where they came from and so that they save to the CRAN folder by default, because this is where I get most of my packages."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#what-to-call-your-package",
    "href": "blog/2023-01-16-minimal-R-package/index.html#what-to-call-your-package",
    "title": "Packaging your R code",
    "section": "What to call your package?",
    "text": "What to call your package?\nNaming things in hard.\nAim for something short, speakable. Snake, camel and pascal case are all acceptable, but aim for simplicity.\nSince packages group code, the package name should describe the group in some way.\n\nPersonal: zvtools, broman, ralph\nColours: rColourBrewer, PrettyCols, wesanderson\nData/Analysis type: lubridate, sp, spatstat, ismev\n\nI’ll follow the inspiration of {ismev} and name my minimal package eds, after the Effective Data Science module.\n\n\n\nConsole\n\nusethis::create_package(path = \"~/Work/teaching/2022_data_science/eds\")\n\n\nThis will\n\nCreate a new R project at the specified path,\nCreates a template package within that project directory,\nOpens the project in a new RStudio session.\n\nIt will also output something like the following in the console:\n\n\n\nConsole\n\n✔ Creating '/Users/zakvarty/Work/teaching/2022_data_science/eds/'\n✔ Setting active project to '/Users/zakvarty/Work/teaching/2022_data_science/eds'\n✔ Creating 'R/'\n✔ Writing 'DESCRIPTION'Package: eds\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R (parsed):\n    * First Last &lt;first.last@example.com&gt; [aut, cre] (YOUR-ORCID-ID)\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to\n    pick a license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n✔ Writing 'NAMESPACE'\n✔ Writing 'eds.Rproj'\n✔ Adding '^eds\\\\.Rproj$' to '.Rbuildignore'\n✔ Adding '.Rproj.user' to '.gitignore'\n✔ Adding '^\\\\.Rproj\\\\.user$' to '.Rbuildignore'\n✔ Opening '/Users/zakvarty/Work/teaching/2022_data_science/eds/' in new RStudio session\n✔ Setting active project to '&lt;no active project&gt;'\n&gt; usethis::create_package(path = \"~/Work/teaching/2022_data_science/eds\")\n✔ Setting active project to '/Users/zakvarty/Work/teaching/2022_data_science/eds'Package: eds\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R (parsed):\n    * First Last &lt;first.last@example.com&gt; [aut, cre] (YOUR-ORCID-ID)\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to\n    pick a license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n\n\nIf we now look in the “files” pane, we will see that this template is very different from what we recommended for a general project. Before we get familiar with the structure of a package directory, let’s first check our package with devtools::check(). This function forms a key part of the package development workflow and does several things.\n\nUpdates documentation for the package and its contents\nBuilds the package\nChecks the structure and content of the built package\nReturns errors, warnings and notes to guide your next steps.\n\nYou should check your package often during development. It takes a bit of time but it is much easier to correct a few points at a time than to diagnose why a huge package is not building.\nLet’s take that advice and perform our first check.\n\n\n\nConsole\n\ndevtools::check()\n\n\nWe will see a lot of output as the function works its way through over 50 different checks. Finally, the output ends with only one warning, telling us that we have not specified a (standard) license for our project within the DESCRIPTION file.\n\n\n\nConsole\n\n...\n── R CMD check results ───────────────────────────────────── eds 0.0.0.9000 ────\nDuration: 5.3s\n\n❯ checking DESCRIPTION meta-information ... WARNING\n  Non-standard license specification:\n    `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\n  Standardizable: FALSE\n\n0 errors ✔ | 1 warning ✖ | 0 notes ✔\n\n\nThe license file specifies how others may use our code. We will use the permissive MIT license, which we will add using another function from {usethis}. There are similar helper functions to add other licenses, which you can investigate further at choosealicense.com\nThe {usethis} helper function will add a markdown and plain text version of the license to our directory and reference these in the DESCRIPTION.\n\n\n\nConsole\n\nusethis::use_mit_license(copyright_holder = \"Zak Varty\")\n\n\n\n\n\nConsole\n\n✔ Setting active project to '/Users/zakvarty/Work/teaching/2022_data_science/eds'\n✔ Setting License field in DESCRIPTION to 'MIT + file LICENSE'\n✔ Writing 'LICENSE'\n✔ Writing 'LICENSE.md'\n✔ Adding '^LICENSE\\\\.md$' to '.Rbuildignore'\n\n\nOur DESCRIPTION file should now look something like this.\n\n\n\nDESCRIPTION\n\nPackage: eds\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: What the package does (one paragraph).\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n\n\nWhile we are here we can fill out a few additional details. Where the template used argument order to define a person(), I have named these explicitly and removed the ORCID field.\nThe package Title should be short and in title case, while the package Description can be more detailed (a single paragraph) and split over multiple lines. Anything beyond the first line should be indented with four spaces (note: four spaces != 1 tab).\n\n\n\nDESCRIPTION\n\nPackage: eds\nTitle: Helper Functions for Effective Data Science 2022-23\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\n    given = c(\"Zak\",\"D\"),\n    family = \"Varty\",\n    email = \"my.email@imperial.ac.uk\",\n    role = c(\"aut\", \"cre\"))\nDescription: A minimal example R package created during the first lives session \n    of Effective Data Science 2022-23. This countains a simple function to \n    calculate a moving average.\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n\n\nLet’s now check the package again and see that this has resolved our issues\n\ndevtools::check()\n\n\n\n\nConsole\n\n...\n✔  checking loading without being on the library search path ...\n─  checking examples ... NONE\n✔  checking for non-standard things in the check directory\n✔  checking for detritus in the temp directory\n   \n   \n── R CMD check results ─────────────────────────────────────────── eds 0.0.0.9000 ────\nDuration: 5.5s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔\n\n\nFantastic! Our package has no errors, warnings or notes."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#adding-a-first-function",
    "href": "blog/2023-01-16-minimal-R-package/index.html#adding-a-first-function",
    "title": "Packaging your R code",
    "section": "Adding a first function",
    "text": "Adding a first function\nThe first function we will add to the package is the helper function I wrote to left- and right-pad a vector with NAs.\n\npad_with_NAs &lt;- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\n\nTo add this function to eds, we save it within the R/ directory. Until your package becomes large, it is good form to save each function in its own R file, named after the function. If the package becomes large, you can then think about refactoring your code to have one script for each family or group of functions.\nWe can create R/pad_with_NAs.R manually, or with a helper function from {usethis}. The helper function is useful because it will either open an existing R file or create one if it does not yet exist.\n\n\n\nConsole\n\nusethis::use_r(\"pad_with_NAs\")\n#• Modify 'R/pad_with_NAs.R'\n#• Call `use_test()` to create a matching test file\n\n\nWe can then paste our function definition into this file and save it.\n\n\n\nR/pad_with_NAs.R\n\npad_with_NAs &lt;- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#try-it-out",
    "href": "blog/2023-01-16-minimal-R-package/index.html#try-it-out",
    "title": "Packaging your R code",
    "section": "Try it out",
    "text": "Try it out\nTo try out our padding function, we need to make it available in our current R session. One way we could do this is to call source(R/pad_with_NAs.R). This doesn’t match with our experience of loading a package though - it would make pad_with_NAs() appear in our global environment pane.\nInstead, we can load all functions from the package using load_all(). This simulates the process of building, installing and loading {eds}, but is much faster than actually doing so. This speed difference becomes more and more beneficial as your package grows, allowing you to get an accurate sense of how users will experience your package even when you have functions that depend on each other or depend on functions from other packages.\n\n\n\nConsole\n\ndevtools::load_all()\n#ℹ Loading eds\n\n\n\npad_with_NAs(x = 1:5, n_left = 2, n_right = 1)\n\n[1] NA NA  1  2  3  4  5 NA"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#adding-documentation",
    "href": "blog/2023-01-16-minimal-R-package/index.html#adding-documentation",
    "title": "Packaging your R code",
    "section": "Adding Documentation",
    "text": "Adding Documentation\nIt would be great if we had handy pop-ups to explain our function arguments and help pages to explain what our function does. We can add these using {roxygen2}. This package allows you to write markdown-like comments above function definitions that are automatically converted into documentation files in the man/ directory.\nTo add skeleton documentation, go to the menu and select Code &gt; Insert Roxygen Skeleton. (This can also be done using the keyboard shortcut on that menu button when your cursor is inside the curly braces of the function.)\nNow our function padding function file should look something like this:\n\n\n\nR/pad_with_NAs.R\n\n#' Title\n#'\n#' @param x \n#' @param n_left \n#' @param n_right \n#'\n#' @return\n#' @export\n#'\n#' @examples\npad_with_NAs &lt;- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\n\n\nFilling out the relevant fields, we get something like this.\n\n\n\npad_with_NAs.R\n\n#' Add NAs to a vector\n#'\n#' @param x Vector to which NAs will be added.\n#' @param n_left Number of NAs to add before x.\n#' @param n_right Number of NAs to add after x.\n#'\n#' @return A vector containing x with the requested number of NA values before and after.\n#'\n#' @export\n#' @examples\n#' pad_with_NAs(1:5, n_left = 0, n_right = 3)\n#' pad_with_NAs(c(\"spider\", \"mouse\", \"cat\", \"dog\"), n_left = 1, n_right = 2)\n#'\npad_with_NAs &lt;- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\n\n\nThe final step is to save these changes and then convert the comments to documentation using document().\n\n\n\nConsole\n\ndevtools::document()\n# ℹ Updating eds documentation\n# ℹ Loading eds\n# Writing pad_with_NAs.Rd\n\n\nThis will allow you to preview the help file for pad_with_NAs(). I say preview here, rather than view because the documentation really only gets made when you build the package (which also adds things like links between help files and a package index). This is what the “Rendering development documentation …”” message is trying to remind you.\n\n\n\nConsole\n\n?pad_with_NAs\n# ℹ Rendering development documentation for \"pad_with_NAs\""
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#install-your-package",
    "href": "blog/2023-01-16-minimal-R-package/index.html#install-your-package",
    "title": "Packaging your R code",
    "section": "Install your package",
    "text": "Install your package\nNow that we have a minimum viable package with a single function, let’s install the eds package. We do this using devtools::install().\n\n\n\nConsole\n\n?devtools::install()\n\n\nWe can now load and use {eds} just like any other package.\n\n\n\nConsole\n\nlibrary(eds)\nanimals &lt;- c(\"spider\", \"mouse\",\"cat\")\npad_with_NAs(animals, n_left = 1,n_right = 0)\n\n\n[1] NA       \"spider\" \"mouse\"  \"cat\"   \n\n\n\nAside on setting installation path\nIf, like me, you want to install this to a non-default location then you can do this using withr::with_libpaths().\nI would like to install this to my sub-folder for personal R packages, which is the third element of my .libPaths vector.\n\n\n\nConsole\n\n.libPaths()\n\n\n[1] \"/Users/zakvarty/R_libraries/4.2/cran\"                          \n[2] \"/Users/zakvarty/R_libraries/4.2/github\"                        \n[3] \"/Users/zakvarty/R_libraries/4.2/personal\"                      \n[4] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"\n\n\nI can do this by using {withr} to execute the same code but with my library paths temporarily replaced by only a single path, pointing to the personal packages sub-folder.\n\n\n\nConsole\n\nwithr::with_libpaths(new = .libPaths()[3], code = devtools::install())\n\n\nThis is a bit of a handful to type repeatedly, so I’ve made a wrapper function for it in my eds package: eds::install_local()."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#functions-with-dependencies",
    "href": "blog/2023-01-16-minimal-R-package/index.html#functions-with-dependencies",
    "title": "Packaging your R code",
    "section": "Functions with dependencies",
    "text": "Functions with dependencies\n\nWithin package dependencies\nThe excellent thing about having a functions in a package is that they are all loaded together and don’t clutter the workspace.\nI created pad_with_NAs() as a helper function for rolling_average(). Whenever I loaded rolling_average() using src(), I had to remember to also source the padding function. Putting both functions in a package saves this worry. It also keeps my working environment focused on the problem I am solving (the data I want to smooth) rather than the tools I am using to solve that problem (functions and their helpers).\n\n\nBetween package dependencies\nWe’ve seen that functions within a package recognise each other and that we can make functions within our package available to users using the @export Roxygen tag. (It did this by adding export(pad_with_NAs) to the NAMESPACE file. Check if you don’t believe me.)\nWhat happens if we want to use another package’s functions in our package? We have three options, depending how many functions we want to use and how often we use them.\n\nRun usethis::use_package(\"package_name\") to declare a dependency in the DESCRIPTION file. Then use the :: notation to clearly specify the namespace (package) of the function you want to use. (I’ve been doing this same thing above to make it clear to you that some function are from devtools and others are from withr.)\nIn the Roxygen section of your function, use #' @importFrom pkg fun1 fun2 - if you prefer this over using ::. This can be useful if you use a couple of functions frequently to keep your code shorter and easier to read.\nIn the Roxygen section of your function, #' @import pkg - this imports all functions from a package and should be used very sparingly because it makes your package bulkier and increases the chance of namespace conflicts (where there is a function of the same name in two loaded packages).\n\n\nIf you are submitting your package to CRAN you need to delcalre all ALL other packages your code depends on. This includes the packages that come as standard with R (other than {base}), for example {stats}, {MASS}, and {graphics}.\n\n\n\nExample imports\n\n\n\nConsole\n\nusethis::use_package(\"stats\")\n\n\n\n\n\nR/rnorm_rounded.R\n\n#' Simulate rounded Gaussian random variates\n#'\n#' @param n Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n#' @param mu Vector of means.\n#' @param sigma Vector of standard deviations.\n#' @param digits Integer indicating the number of decimal places to be used in rounding. Negative values are used to round to a power of ten, so for example `digits = -2` rounds to the nearest hundred. (See 'Details' of `base::round()`).\n#'\n#' @return Vector of Gaussian random variates, rounded to to specified number of decimal places.\n#' @export\n#'\n#' @examples\n#' rnorm_rounded(n = 10)\nrnorm_rounded &lt;- function(n, mu = 0, sigma = 1, digits = 0){ \n  raw_values &lt;- stats::rnorm(n, mean = mu, sd = sigma)\n  rounded_values &lt;- base::round(raw_values, digits)\n  return(rounded_values)\n}\n\n\nWe do not need to explicitly declare that round() is from base, or include in in a list of imported functions.\n\n\n\nR/rnorm_rounded.R\n\n#' Simulate rounded Gaussian random variates\n#'\n#' @param n Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n#' @param mu Vector of means.\n#' @param sigma Vector of standard deviations.\n#' @param digits Integer indicating the number of decimal places to be used in rounding. Negative values are used to round to a power of ten, so for example `digits = -2` rounds to the nearest hundred. (See 'Details' of `base::round()`).\n#'\n#' @return Vector of Gaussian random variates, rounded to to specified number of decimal places.\n#' \n#' @importFrom stats rnorm\n#' @export\n#\"\n#' @examples\n#' rnorm_rounded(n = 10)\nrnorm_rounded &lt;- function(n, mu = 0, sigma = 1, digits = 0){ \n  raw_values &lt;- rnorm(n, mean = mu, sd = sigma)\n  rounded_values &lt;- round(raw_values, digits)\n  return(rounded_values)\n}\n\n\nImporting the entire stats package would be overkill when we use only one function.\n\n\n\nR/rnorm_rounded.R\n\n#' Simulate rounded Gaussian random variates\n#'\n#' @param n Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n#' @param mu Vector of means.\n#' @param sigma Vector of standard deviations.\n#' @param digits Integer indicating the number of decimal places to be used in rounding. Negative values are used to round to a power of ten, so for example `digits = -2` rounds to the nearest hundred. (See 'Details' of `base::round()`).\n#'\n#' @return Vector of Gaussian random variates, rounded to to specified number of decimal places.\n#' \n#' @import stats\n#' @export\n#\"\n#' @examples\n#' rnorm_rounded(n = 10)\nrnorm_rounded &lt;- function(n, mu = 0, sigma = 1, digits = 0){ \n  raw_values &lt;- rnorm(n, mean = mu, sd = sigma)\n  rounded_values &lt;- round(raw_values, digits)\n  return(rounded_values)\n}\n\n\n\n\nAn import exception\nAn exception to the import procedure is the pipe from {magrittr}\nYou may want to import this to pipe within your function definitions, but having magritr::'%&gt;%' in your code looks a right mess and defeats the readability benefits of piping.\nInstead, we have to take two steps.\n\nusethis::use_pipe()  # Creates R/utils-pipe.R and adds magrittr to DESCRIPTION\ndevtools::document() # Adds the pipe to your package's NAMESPACE"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#adding-tests",
    "href": "blog/2023-01-16-minimal-R-package/index.html#adding-tests",
    "title": "Packaging your R code",
    "section": "Adding Tests",
    "text": "Adding Tests\nWe can also add tests to the functions within our package. These tests will stay in a new directory called tests/ and will be run each time the package is built. This helps us to ensure that they currently work as we expect and that we do not break them when making alterations in the future.\nTo write and keep track of these tests, we will use {testthat}. To get started we will use the helper function usethis::use_testthat() to:\n\ncreate ⁠tests/testthat\ncreate tests/testthat.R\nadd {testthat} to the Suggests field of our package.\n\nThis needs to be done only once per package.\n\n\n\nConsole\n\nusethis::use_testthat()\n# ✔ Adding 'testthat' to Suggests field in DESCRIPTION\n# ✔ Setting Config/testthat/edition field in DESCRIPTION to '3'\n# ✔ Creating 'tests/testthat/'\n# ✔ Writing 'tests/testthat.R'\n# • Call `use_test()` to initialize a basic test file and open it for editing.\n\n\nEach of our files of tests will live in the tests/testthat subdirectory in a file named after the function. We can create this file of tests by running usethis::use_test() while we have any of our package functions open. This will either open or create the relevant text file for us to edit or populate.\nThis needs to be done (at least) once for each function that we want to write tests for. Let’s focus on writing tests for pad_with_NAs().\nWith R/pad_with_NAs.R open, run usethis::use_test() to create and open a file to store the tests. Note that the naming convention here is different to our previous convention.\n\n\n\nConsole\n\nusethis::use_test()\n# ✔ Writing 'tests/testthat/test-pad_with_NAs.R'\n# • Modify 'tests/testthat/test-pad_with_NAs.R'\n\n\nThis will have a dummy test included by default.\n\n\n\ntests/testthat/test-pad_with-NAs.R\n\ntest_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n\n\nLet’s change this to match one of our examples and add another test to match the other example. To try out these tests as you write them, you’ll want to attach {testthat} to the current R session and make your package function available with load_all().\n\n\n\nConsole\n\nlibrary(testthat)\ndevtools::load_all()\n# ℹ Loading eds\n\n\nIf you then try running the dummy test you should get something like:\n\n\n\ntests/testthat/test-pad_with-NAs.R\n\ntest_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n# Test passed 😀\n\n\nWe can replace this by some simple checks that the funciton behaves as expected when given examples we wrote in the documentation and when we pad with zero NAs.\n\n\n\ntests/testthat/test-pad_with-NAs.R\n\ntest_that(\"padding by 0 works\", {\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 1, n_right = 0), c(NA, 1, 2, 3))\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 0, n_right = 1), c(1, 2, 3, NA))\n})\n\ntest_that(\"examples work\", {\n  expect_equal(\n    object = pad_with_NAs(1:5, n_left = 0, n_right = 3), \n    expected = c(1:5, NA, NA, NA))\n  expect_equal(\n    object = pad_with_NAs(c(\"spider\", \"mouse\", \"cat\", \"dog\"), n_left = 1, n_right = 2),\n    expected = c(NA, \"spider\", \"mouse\", \"cat\", \"dog\", NA)\n  )\n})\n\n\nWe can run these tests individually, as we did before, or we can run all test in the file using the testthat::test_file().\n\ntestthat::test_file(\"tests/testthat/test-pad_with_NAs.R\")\n\n# [ FAIL 1 | WARN 0 | SKIP 0 | PASS 3 ]\n#\n# ── Failure (test-pad_with_NAs.R:10): examples work ─────────────────────────────\n# pad_with_NAs(...) (`actual`) not equal to \n# c(NA, \"spider\", \"mouse\", \"cat\", \"dog\", NA) (`expected`).\n#\n# `actual[4:7]`:   \"cat\" \"dog\" NA NA\n# `expected[4:6]`: \"cat\" \"dog\" NA   \n#\n# [ FAIL 1 | WARN 0 | SKIP 0 | PASS 3 ]\n\nFrom the test output, we can see that three of the tests are passing but one is failing. Using the additional output, we can figure out that I forgot the second NA in the test of the string example. Let’s add that back in.\n\n\n\ntests/testthat/test-pad_with-NAs.R\n\ntest_that(\"padding by 0 works\", {\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 1, n_right = 0), c(NA, 1, 2, 3))\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 0, n_right = 1), c(1, 2, 3, NA))\n})\n\ntest_that(\"examples work\", {\n  expect_equal(\n    object = pad_with_NAs(1:5, n_left = 0, n_right = 3), \n    expected = c(1:5, NA, NA, NA))\n  expect_equal(\n    object = pad_with_NAs(c(\"spider\", \"mouse\", \"cat\", \"dog\"), n_left = 1, n_right = 2),\n    expected = c(NA, \"spider\", \"mouse\", \"cat\", \"dog\", NA, NA)\n  )\n})\n\n\nWe can also add some input checks to the function, in case the user tries to do something unexpected. We’ll add input checks to cover two cases where the user tries to:\n\nuse negative indexing to pad inwards with NAs;\npad an object that is not a vector.\n\nTo do this we will use the stopifnot() function. This is a useful shorthand to stop() function execution and return an error message based on a logical statement. For more detailed control of the error message or for less serious failings, which require only a warning() or a message(). Learn more about these in the Advanced R conditions chapter.\n\n\n\nR/pad_with_NAs.R\n\n#' Add NAs to a vector\n#'\n#' @param x Vector to which NAs will be added.\n#' @param n_left Number of NAs to add before x.\n#' @param n_right Number of NAs to add after x.\n#'\n#' @return A vector containing x with the requested number of NA values before and after.\n#'\n#' @export\n#' @examples\n#' pad_with_NAs(1:5, n_left = 0, n_right = 3)\n#' pad_with_NAs(c(\"spider\", \"mouse\", \"cat\", \"dog\"), n_left = 1, n_right = 2)\n#'\npad_with_NAs &lt;- function(x, n_left, n_right){\n  # Input checks\n  stopifnot(n_left &gt;= 0)\n  stopifnot(n_right &gt;= 0)\n  stopifnot(class(x) %in% c(\"character\", \"complex\", \"integer\", \"logical\", \"numeric\", \"factor\"))\n\n  # Function Body\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\n\n\nWe can then add additional tests to confirm that we get errors, warnings or messages where we expect them. As our test file grows, we might want to organise our tests according to their purpose.\n\n\n\ntests/testthat/test-pad_with-NAs.R\n\n# Typical Behaviour\n\ntest_that(\"examples work\", {\n  expect_equal(\n    object = pad_with_NAs(1:5, n_left = 0, n_right = 3),\n    expected = c(1:5, NA, NA, NA))\n  expect_equal(\n    object = pad_with_NAs(c(\"spider\", \"mouse\", \"cat\", \"dog\"), n_left = 1, n_right = 2),\n    expected = c(NA, \"spider\", \"mouse\", \"cat\", \"dog\", NA, NA)\n  )\n})\n\n# Edge Cases\n\ntest_that(\"padding by 0 works\", {\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 1, n_right = 0), c(NA, 1, 2, 3))\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 0, n_right = 1), c(1, 2, 3, NA))\n})\n\n# Malformed Inputs\n\ntest_that(\"negative padding produces error\", {\n  expect_error(pad_with_NAs(x = 1:5, n_left = -1, n_right = 1))\n  expect_error(pad_with_NAs(x = 1:5, n_left = 1, n_right = -1))\n})\n\ntest_that(\"giving non-vector x produces error\", {\n  matrix_input &lt;- diag(1:4)\n  list_input &lt;- x = list(1:5, \"cat\")\n\n  expect_error(pad_with_NAs(x = matrix_input, n_left = 1, n_right = 1))\n  expect_error(pad_with_NAs(x = list_input, n_left = 1, n_right = 1))\n})\n\n\nWe can continue to run test files individually, but they will all be run when we call devtools::check().\n\n\n\nConsole\n\ntestthat::test_file(\"tests/testthat/test-pad_with_NAs.R\")\n# [ FAIL 0 | WARN 0 | SKIP 0 | PASS 8 ]"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#wrapping-up",
    "href": "blog/2023-01-16-minimal-R-package/index.html#wrapping-up",
    "title": "Packaging your R code",
    "section": "Wrapping up",
    "text": "Wrapping up\nTo wrap up let’s run one last check and install the package.\n\n\n\nConsole\n\ndevtools::check()\n# ─ R CMD check results ─────────────────────────────────── eds 0.0.0.9000 ────\n# Duration: 12.1s\n#\n# 0 errors ✔ | 0 warnings ✔ | 0 notes ✔\n\n\nCongratulations, you’ve created your first R package!\nYou should now have a solid understanding of how to set up a new R package, add functions and documentation by using {usethis} and {devtools}. You also know how to use {testthat} and {roxygen22} to create tests to ensure your package is reliable and easy to use.\nYou should now be able to create your own R package and might want to consider using git to track its development and sharing it with others on Github.\nRemember, creating an R package is an iterative process, and it may take several rounds of testing and editing before you are satisfied with the final product. But with a little patience and persistence, you’ll be able to create a package that you and others can use with confidence."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#follow-up-tasks",
    "href": "blog/2023-01-16-minimal-R-package/index.html#follow-up-tasks",
    "title": "Packaging your R code",
    "section": "Follow up Tasks",
    "text": "Follow up Tasks\n\nAdd input checks and tests for my rolling_mean function.\nAdd your own function for finding the geometric mean of a vector.\nAdd a readme file for your package, using usethis::use_readme_rmd() and the relevant section of the R packages book for guidance.\n\n–&gt;"
  },
  {
    "objectID": "blog/2022-10-10-statistics-books/index.html",
    "href": "blog/2022-10-10-statistics-books/index.html",
    "title": "Recommended Statistics Books",
    "section": "",
    "text": "While teaching a course on supervised learning last year, several students asked about what books I would recommended on statistical inference and modelling.\nFor context, the students on this course are all highly numerate and studying at the postgraduate level. What makes this request challenging is the broad range of student backgrounds, some students had a maths degree but the majority are trained and work as engineers, physicists or computer scientists.\nThis variety in backgrounds and exposure to undergraduate level statistics made recommending a single book difficult. Instead, I compiled a list of books that I have enjoyed or found useful. For each book I tried to give some guidance on whether it might match with their current statistical knowledge and what they are trying to achieve. I gave a brief description of the level and target audience of each text, which I reproduce below.\nWhen evaluating whether these resources might suit your current needs, I find it helpful to skim through a section on a topic that you already know (such as linear regression). This is usually the fastest and most reliable way to assess if the book is going to be a good fit for you.\nThis list is by no means exhaustive. If you know of any gems that I have not included in this list, please do let me know!\n\n\n\n\n\n\nRice (2007) covers the basics of probability and statistics usually contained in the first couple of undergraduate statistics courses. Generally the first university courses are a bit dry, building up the required knowledge to do interesting things. This book is slightly better than the average treatment in terms of readability and is fairly comprehensive, making it well suited as a reference text. This is a book full of all the stuff you might once have known but have now forgotten, or never studied before.\n\n\n\n\n\n\n\n\nKirkwood and Sterne (2010) focuses on more advanced topics in statistics, such as inference, hypothesis testing and modelling. However, it approaches these from an applications perspective. While all of the applications it uses are from medical statistics, the authors give sufficient context that you do not need to be familiar with this area before reading. This is a very readable book, with a moderate amount of mathematical detail. I find myself revisiting it quite often.\n\n\n\n\n\n\n\nWood (2015) gives an introduction to the core topics in statistics aimed at new graduate-level students. It is mathematically dense but written in an approachable manner and (unsurprisingly) covers all the core ideas of statistics. This means that is often a good source to get an overview of a topic and to cover the key points in that area quickly. It is probably wise to supplement this with a more applied text to see worked examples and to a more detailed text for topics that you need to explore in greater detail.\n\n\n\n\n\n\n\n\nPawitan (2001) focuses entirely on likelihood inference and covers both theory and applications in a great deal of detail. I highly recommend this to supplement frequentist topics covered in core statistics and the elements of statistical learning. It builds up from very little assumed knowledge but also goes on to cover some very advanced topics in later chapters.\n \n\n\n\n\n\n\n\nKendall, Stuart, and Ord (1987) is an alternative to In All Likelihood, aimed at a similar audience and level. Split over several volumes this is good to do a deep-dive into a particular topic but probably not one to try and read cover to cover!"
  },
  {
    "objectID": "blog/2022-10-10-statistics-books/index.html#statistics-and-statistical-inference-books",
    "href": "blog/2022-10-10-statistics-books/index.html#statistics-and-statistical-inference-books",
    "title": "Recommended Statistics Books",
    "section": "",
    "text": "While teaching a course on supervised learning last year, several students asked about what books I would recommended on statistical inference and modelling.\nFor context, the students on this course are all highly numerate and studying at the postgraduate level. What makes this request challenging is the broad range of student backgrounds, some students had a maths degree but the majority are trained and work as engineers, physicists or computer scientists.\nThis variety in backgrounds and exposure to undergraduate level statistics made recommending a single book difficult. Instead, I compiled a list of books that I have enjoyed or found useful. For each book I tried to give some guidance on whether it might match with their current statistical knowledge and what they are trying to achieve. I gave a brief description of the level and target audience of each text, which I reproduce below.\nWhen evaluating whether these resources might suit your current needs, I find it helpful to skim through a section on a topic that you already know (such as linear regression). This is usually the fastest and most reliable way to assess if the book is going to be a good fit for you.\nThis list is by no means exhaustive. If you know of any gems that I have not included in this list, please do let me know!\n\n\n\n\n\n\nRice (2007) covers the basics of probability and statistics usually contained in the first couple of undergraduate statistics courses. Generally the first university courses are a bit dry, building up the required knowledge to do interesting things. This book is slightly better than the average treatment in terms of readability and is fairly comprehensive, making it well suited as a reference text. This is a book full of all the stuff you might once have known but have now forgotten, or never studied before.\n\n\n\n\n\n\n\n\nKirkwood and Sterne (2010) focuses on more advanced topics in statistics, such as inference, hypothesis testing and modelling. However, it approaches these from an applications perspective. While all of the applications it uses are from medical statistics, the authors give sufficient context that you do not need to be familiar with this area before reading. This is a very readable book, with a moderate amount of mathematical detail. I find myself revisiting it quite often.\n\n\n\n\n\n\n\nWood (2015) gives an introduction to the core topics in statistics aimed at new graduate-level students. It is mathematically dense but written in an approachable manner and (unsurprisingly) covers all the core ideas of statistics. This means that is often a good source to get an overview of a topic and to cover the key points in that area quickly. It is probably wise to supplement this with a more applied text to see worked examples and to a more detailed text for topics that you need to explore in greater detail.\n\n\n\n\n\n\n\n\nPawitan (2001) focuses entirely on likelihood inference and covers both theory and applications in a great deal of detail. I highly recommend this to supplement frequentist topics covered in core statistics and the elements of statistical learning. It builds up from very little assumed knowledge but also goes on to cover some very advanced topics in later chapters.\n \n\n\n\n\n\n\n\nKendall, Stuart, and Ord (1987) is an alternative to In All Likelihood, aimed at a similar audience and level. Split over several volumes this is good to do a deep-dive into a particular topic but probably not one to try and read cover to cover!"
  },
  {
    "objectID": "blog/2022-10-10-statistics-books/index.html#bayesian-statistics",
    "href": "blog/2022-10-10-statistics-books/index.html#bayesian-statistics",
    "title": "Recommended Statistics Books",
    "section": "Bayesian Statistics",
    "text": "Bayesian Statistics\nWe only consider frequentist approaches to inference in this course. However, I would be remiss to not include some Bayesian texts and leave you with the impression that classical or frequentist approaches to statistics are the only option.\nMany of the topics we cover in supervised learning can be considered from a Bayesian perspective. A Bayesian statistician does not treat our model parameters as fixed but unknown quantities, instead they consider the parameters as random variables and use probability distributions to describe their (or our) beliefs about the parameter values.\nYou might find the following books useful, either during or after the Bayesian inference course. The former is more theoretical, while the latter has a more applied focus.\nKendall’s advanced theory of statistics. Vol. 2B, Bayesian inference. (O’Hagan and Forster 2004)\nBayesian Data Analysis - Gelman et al. (Gelman et al. 2013)"
  },
  {
    "objectID": "blog/2022-10-15-BBC-temperature-plot/index.html",
    "href": "blog/2022-10-15-BBC-temperature-plot/index.html",
    "title": "Data Journalism: Recreating a Professional Plot",
    "section": "",
    "text": "BBC Temperature Records\nOn Friday 2022-10-14, the BBC Data Journalism Team released this excellent article about the record temperatures in the UK during this summer’s heatwave. The article has some amazing data visualisations, and draws on a recent Met Office report.\nI wanted to try and recreate one of the plots to test the limits of my ggplot knowledge. Since I had already tackled a stacked bar plot, I figured I might have a go at their dumbbell plot that shows the weather stations which exceeded their previous records largest margins.\n\n\n\nbbc temperature records dumbbell plot\n\n\nI couldn’t find the data source, so spent far too long with a printed copy of the original figure to make my own version of the data set.\nIt took a while, but I got most of the way there with it and am happy with the final result.\n\n\n\nmy attempt at recreating the same plot\n\n\nThere were a few things that still have me stumped, that I might revisit at some later date:\n\nLeft aligning title and caption (Thanks to Jack Davison for this!)\nUsing gradients on multiple parts of the plot\nUsing the YeOrRd gradient, rather than default blues\nAdding a non-BBC logo to the bottom right.\n\nIf anyone with superior ggplot skills would like to help with those or give pointers, then I would be most grateful!\nCode and figure down below ↓\n\n\nCode\n# Load packages ----\nlibrary(bbplot)\nlibrary(tidyverse)\nlibrary(showtext)\n\n# Import fonts ----\nfont_add_google(name = \"Roboto Slab\", family = \"roboto-slab\")\nfont_add_google(name = \"Roboto\", family = \"roboto\")\nshowtext_auto()\ntitle_font &lt;- \"roboto-slab\"\nfont &lt;- \"roboto\"\n\n# Input data (estimated values from article) ---\ntemperatures &lt;- tibble::tribble(\n  ~location, ~max_prev, ~max_2022,\n  \"Cranwell\", 36.6, 39.9,\n  \"Nottingham\", 36.0, 39.8,\n  \"Bramham\", 33.5, 39.8,\n  \"Sutton Boningon\", 35.9, 39.4,\n  \"Sheffield\", 35.6, 39.4,\n  \"Leeming\", 34.4, 38.7,\n  \"Goudhurst\", 34.7, 37.9,\n  \"Whitby\", 33.1, 37.8,\n  \"Bradford\", 33.9, 37.8,\n  \"High Mowthorpe\", 33.1, 37.2,\n  \"Blackpool\", 33.6, 37.2,\n  \"Durham\", 32.9, 36.9,\n  \"Preston\", 33.1, 36.5,\n  \"Morecambe\", 32.7, 36.4,\n  \"Stonyhurst\", 32.6, 36.3,\n  \"Keele\", 32.9, 36.2,\n  \"Bude\", 32.2, 36.1,\n  \"Buxton\", 32.7, 36.0,\n  \"Kielder Castle\", 29.6, 35.0,\n  \"Bala\", 31.9, 34.9\n)\n\n# Data preparation ----\n\n## For the points ----\ntemperatures &lt;- temperatures |&gt;\n  dplyr::mutate(max_ever = pmax(max_2022, max_prev))\n\ntemperatures$location &lt;- forcats::fct_reorder(as.factor(temperatures$location), .x = temperatures$max_ever)\n\ntemp_long &lt;- tidyr::pivot_longer(temperatures, cols = c(max_2022, max_prev), names_to = \"year\",values_to = \"temperature\")\n\n## For the bars ----\nn_interp &lt;- 501\ntemp_interpolated &lt;- tibble(rep(NA, n_interp*20))\ntemp_interpolated[[1]] &lt;- rep(temperatures$location, each = n_interp)\ntemp_interpolated[[2]] &lt;- rep(NA_real_, n_interp*20)\nnames(temp_interpolated) &lt;- c(\"location\", \"interp_value\")\nfor (i in 1:20) {\n  temp_interpolated$interp_value[(1 + n_interp * (i - 1)):(n_interp*i)] &lt;-\n    seq(temperatures$max_prev[i], temperatures$max_2022[i], length.out = n_interp)\n}\n\nstr_wrap_break &lt;- function(x, break_limit) {\n  # Function from {usefunc} by N Rennie (https://github.com/nrennie/usefunc)\n  sapply(strwrap(x, break_limit, simplify = FALSE), paste, collapse = \"\\n\")\n}\n\ntitle_string &lt;- \"Huge breaks from previous records in 2022\"\nsubtitle_string &lt;- str_wrap_break(\"Stations with largest gaps between previous and new records, ordered by highest new temperature\",60)\ncaption_string &lt;- \"Only includes active weather stations with at least 50 years of observations\"\n\n\n\n\n\np &lt;- ggplot() +\n  geom_line(data = temp_interpolated, aes(x = interp_value, y = location, color = interp_value), lwd = 3) +\n  #\n  geom_label(aes(label = \"2022 record\", x = 38.7, y = 7.2), family = font, size = 6.5, label.size = NA) +\n  geom_curve(aes(x = 38, y = 7.9, xend = 36.9, yend = 8.9)) +\n  #\n  geom_text(aes(label = \"Previous \\n record\", x = 31, y = 11), family = font, size = 6.5) +\n  geom_curve(aes(xend = 32, yend = 11, x = 32.9, y = 8.9)) +\n  #\n  geom_label(aes(label = \"Biggest leap\", x = 33.7, y = 20), family = font, size = 6.5, label.size = NA) +\n  geom_label(aes(label = \"6.3C\", x = 34.0, y = 19), family = font, fontface=\"bold\", size = 6.5, label.size = NA) +\n  geom_curve(aes(xend = 34.7, yend = 19.5, x = 35.7, y = 18)) +\n  #\n  geom_point(data = temp_long, aes(x = temperature, y = location, fill = temperature), shape = 21, color = \"black\", size = 6) +\n  #\n  scale_x_continuous(breaks = seq(30, 40, by = 2.5),labels = paste0(seq(30, 40, by = 2.5),\"C\")) +\n  #\n  labs(title = title_string,\n       subtitle = subtitle_string,\n       caption = caption_string) +\n  #\n  theme(plot.title = element_text(family=title_font,\n                                  size=28,\n                                  face=\"bold\",\n                                  color=\"#222222\"),\n        plot.subtitle = element_text(family=font,\n                                     size=18,\n                                     margin=ggplot2::margin(9,0,9,0)),\n        plot.caption = element_text(family = font, size = 14,hjust = 0),\n        plot.title.position = 'plot',\n        plot.caption.position = 'plot',\n        axis.title = ggplot2::element_blank(),\n        axis.text = ggplot2::element_text(family=font,\n                                          size=18,\n                                          color=\"grey47\"),\n        legend.position = \"none\",\n        title = element_text(),\n        axis.line.x = element_line(size = 0.7, linetype = \"solid\"),\n        axis.ticks.y = element_blank(),\n        axis.ticks.length.x = unit(7, units = \"points\" ),\n        axis.text.x = element_text(margin=margin(t = 15, b = 10)),\n        panel.grid.minor = ggplot2::element_blank(),\n        panel.grid.major.y = ggplot2::element_line(color=\"#cbcbcb\"),\n        panel.grid.major.x = ggplot2::element_blank(),\n        panel.background = ggplot2::element_blank(),\n  )\n\np"
  },
  {
    "objectID": "blog/2023-07-14-counting-the-countless/index.html",
    "href": "blog/2023-07-14-counting-the-countless/index.html",
    "title": "Counting the Countless",
    "section": "",
    "text": "keyes2019counting\nKeyes (2019)\nTitle: Counting the Countless. {2019}.\nAuthors: Os Keyes\nKey words: Data Ethics, Data Science.\nIn “Counting the Countless” (2019), Os Keyes argues that data science poses a significant threat to queer people because it does not allow for the autonomy, contextuality and fluidity that can both define queerness and keep queer people safe. Keyes asserts this point by describing how “data violence” is inflicted upon individuals by reducing complex human traits (like race and gender) down to something that can be counted and used for decision making; they do this through an analogy to administrative violence, which already suppresses and allows the targeting of the poor, immigrants and people of colour. The author aims to convince the reader that data science is inherently reductive, harmful and incompatible with queerness by criticising current attempts at reform and arguing that further, better surveillance serves only to “make violent systems more efficiently violent”. Keyes is addressing queer or otherwise minoritised people who are interested in or work in data science and acknowledges that “people need to eat to survive” so each individual must “make the decision that is right for [their] ethics of care”.\n\n\n\n\nReferences\n\nKeyes, Os. 2019. “Counting the Countless.” https://reallifemag.com/counting-the-countless/.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "blog/2022-09-26-adding-a-quarto-blog/index.html",
    "href": "blog/2022-09-26-adding-a-quarto-blog/index.html",
    "title": "Setting up a quarto blog",
    "section": "",
    "text": "What am I trying to do?\n\n\n\nMy aim here is to set up a blog within an existing quarto website. I want the blog to be a sub-domain of the main site (zakvarty.com/blog) and for it to inherit the styling of that site.\nIt probably would have been easier to make the blog a sub domain (blog.zakvarty.com) and use two separate but matching style files to make those sites look coherent. However, I wanted to make the most of the built in search features on quarto websites and am a glutton for punishment.\nCredit to Drew Dimmery, whose website I used alongside the quarto docs to work out how to get all these pieces working together.\n\n\nSteps\n\nCreate a subdirectory of the website called blog/. This has sub-folders for each blog post and will contain the files of metadata that are common to all blog posts (e.g. default settings for YAML headers information and a bibliography file). \nCreate a listing page called blog.qmd in the root directory. This will become the blog “landing page” and what we will point to from the website header. \nAdd a “Blog” header item to the _quarto.yml file for the website and set the link: for this to be blog.qmd \nAdded a simple example post to the blog/ directory. See for example my hello-world post. \nAdjust the default YAML parameters for the blog posts by making the file blog/_metadata.yml. These default values can be overwritten by specifying them again in the YAML header at the top of any individual post. For examples of what you might want to include see my file or the projects section of the quarto docs. \nAdd a simple bibliography file, called library.bib or similar to the blog/ directory. Set this as the default bibliography file for each blog post by adding bibliography: ../library.bib to blog/_metadata.yml. \n(optional) Create a post template so that you don’t have to memorise header fields. \nSet your “Hello, World!” and template posts to have draft: true in their headers. This will prevent them from showing up on your website. \nSet your “Hello, World!” and template posts to have freeze: true in their headers. This will prevent any code in them from re-running each time the website is rendered.\n\nFreezing the code within posts will improve the build speed, as well as make the website more stable and portable. See the quarto docs on freezing posts for more details. My current plan is to have this as false by default and change to true on publication of each post.\n\n\nChecking that references work\nI have set up a single bibtex file in which to store references for all posts. This lives in the blog/ directory and is set as the default bibliography parameter for each post in the file blog/_metadata.yml.\nThis is an in-line reference to Wan et al. (2020) written as @citationkey. Parenthetical references, such as (Wan et al. 2020), are written using [@citationkey]. These can be strung together by separating each citation key with a semicolon, for example (Wan et al. 2020, 2020).\nTo let people know the license your work is under and how they should cite your blog posts you can use the appendix-style argument. This can be added to the YAML header of individual blog posts or you can specify a default value in blog/_metadata.yml. There are three options for this parameter:\n\ndefault does some nice formatting and makes the text a bit smaller than the rest of the article;\nplain matches the style of the rest of your post;\nnone does not add any citation details to the end of your post.\n\nI’m currently using some pretty hacky CSS to style this website so am limited to the latter two options for now. In the process of writing this article I stumbled across some neat SCSS that I hope will fix this issue that I have made for myself! [Update: I changed to SCSS and this is now fixed!]\nNote: When adding references to your posts, make sure that the site-URL field in your website’s quarto.yml does not have a trailing slash - this will be copied into the reference and break the links.\n\n\n\n\n\nReferences\n\nWan, Phyllis, Tiandong Wang, Richard A Davis, and Sidney I Resnick. 2020. “Are Extreme Value Estimation Methods Useful for Network Data?” Extremes 23 (1): 171–95.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{varty2022,\n  author = {Varty, Zak},\n  title = {Setting up a Quarto Blog},\n  date = {2022-09-26},\n  url = {https://www.zakvarty.com/blog/2022-09-26-adding-a-quarto-blog},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVarty, Zak. 2022. “Setting up a Quarto Blog.” September 26,\n2022. https://www.zakvarty.com/blog/2022-09-26-adding-a-quarto-blog."
  },
  {
    "objectID": "blog/2023-10-03-notably-inaccessible/index.html",
    "href": "blog/2023-10-03-notably-inaccessible/index.html",
    "title": "Notably Inaccessible",
    "section": "",
    "text": "potluri2023notably (Potluri et al. 2023)\nTitle: Notably Inaccessible - Data driven understanding of data science notebook (in)accessibility. {ArXiv preprint, 2023} (19 pages).\nAuthors: Venkatesh Potluri, Sudheesh Singanamalla, Nussara Tieanklin and Jennifer Mankoff. (University of Washington)\nKey words: data science, workflows, computing, accessibility.\n\n\n\n\n\nIn this paper, the authors explore accessibility of literate programming approaches to blind and visually impaired users, both for the authoring and consumption of data science notebooks. They do this by conducting the first large-scale, empirical investigation into how notebooks are created and used in the wild, focusing strongly on Python-based analyses collated by Rule, Tabard, and Hollan (2018). This study is of interest data scientists and IDE developers who care about improving the accessibility of our workflows. The key findings suggest that the most immediate, individual impact can be made through consistent use of existing HTML tools, for example by providing alt-text for all generated figures and by using heading tags to facilitate document navigation."
  },
  {
    "objectID": "blog/2023-10-03-notably-inaccessible/index.html#reading-summary",
    "href": "blog/2023-10-03-notably-inaccessible/index.html#reading-summary",
    "title": "Notably Inaccessible",
    "section": "",
    "text": "potluri2023notably (Potluri et al. 2023)\nTitle: Notably Inaccessible - Data driven understanding of data science notebook (in)accessibility. {ArXiv preprint, 2023} (19 pages).\nAuthors: Venkatesh Potluri, Sudheesh Singanamalla, Nussara Tieanklin and Jennifer Mankoff. (University of Washington)\nKey words: data science, workflows, computing, accessibility.\n\n\n\n\n\nIn this paper, the authors explore accessibility of literate programming approaches to blind and visually impaired users, both for the authoring and consumption of data science notebooks. They do this by conducting the first large-scale, empirical investigation into how notebooks are created and used in the wild, focusing strongly on Python-based analyses collated by Rule, Tabard, and Hollan (2018). This study is of interest data scientists and IDE developers who care about improving the accessibility of our workflows. The key findings suggest that the most immediate, individual impact can be made through consistent use of existing HTML tools, for example by providing alt-text for all generated figures and by using heading tags to facilitate document navigation."
  },
  {
    "objectID": "blog/2023-10-03-notably-inaccessible/index.html#notes",
    "href": "blog/2023-10-03-notably-inaccessible/index.html#notes",
    "title": "Notably Inaccessible",
    "section": "Notes",
    "text": "Notes\n\nThe study makes a holistic assessment of the use of notebooks, investigating accessibility of:\n\nthe integrated development environments used when authoring,\nthe analysis and outputs performed within the notebooks,\nhow those are presented in html format.\n\n” Elavsky, Bennett, and Moritz (2022) extend web accessibility guidelines to make data visualizations accessible.” - Potential further reading.\npa11y accessibility scanning infrastructure, interesting for further investigation. I think this was one of the topics in a previous Tidy Tuesday visualisation challenge.\n“Only studying notebooks that are presentation-ready assumes that BVI people’s involvement only as consumers of these notebooks and limits discovery of the extent of notebook accessibility problems.” - Yes! Strongly agree.\nThe authors use “a fully connected conventional neural network combined with a Fisher-Vector Convolution Neural Network, pre-trained on the DocFigure data set” to classify the chart types in the notebooks. A replication of this might be a fun exercise / mini-project in deep learning.\nnbconvert is a pythonic, more focused version of pandoc."
  },
  {
    "objectID": "blog/2024-10-01-steal-like-a-generative-artist/index.html",
    "href": "blog/2024-10-01-steal-like-a-generative-artist/index.html",
    "title": "Steal like a generative artist",
    "section": "",
    "text": "I recently realised that my banner image on twitter was a bit dated and that I didn’t even have one on LinkedIn. To fix that, I’m going to to follow up on the session that Nicola Rennie ran at the RSS pre-conference workshop and get started with generative art. (Slides from the session are available at this link)\nI’ve done lots of plotting of simulated data before, which I guess technically counts as generative art, but this is my first attempt at making plots for purely *aesthetic* purposes. One of the main takeaways for me from Nicola’s talk was to steal like an artist. In a nutshell, a great way to learn any new creative skill is to copy and riff on what other people have done already. When developing generative art skills it can be helpful to:\n\nstart by using tools that other people have made to create generative art,\nrecreate or adapt those tools for yourself,\ntake inspiration for generative designs from more traditional art forms,\nrecreate designs or patterns from objects in the world around you.\n\nIn that spirit, I’ll be recreating and adapting one of her title slide artworks."
  },
  {
    "objectID": "blog/2024-10-01-steal-like-a-generative-artist/index.html#making-a-new-banner-image",
    "href": "blog/2024-10-01-steal-like-a-generative-artist/index.html#making-a-new-banner-image",
    "title": "Steal like a generative artist",
    "section": "",
    "text": "I recently realised that my banner image on twitter was a bit dated and that I didn’t even have one on LinkedIn. To fix that, I’m going to to follow up on the session that Nicola Rennie ran at the RSS pre-conference workshop and get started with generative art. (Slides from the session are available at this link)\nI’ve done lots of plotting of simulated data before, which I guess technically counts as generative art, but this is my first attempt at making plots for purely *aesthetic* purposes. One of the main takeaways for me from Nicola’s talk was to steal like an artist. In a nutshell, a great way to learn any new creative skill is to copy and riff on what other people have done already. When developing generative art skills it can be helpful to:\n\nstart by using tools that other people have made to create generative art,\nrecreate or adapt those tools for yourself,\ntake inspiration for generative designs from more traditional art forms,\nrecreate designs or patterns from objects in the world around you.\n\nIn that spirit, I’ll be recreating and adapting one of her title slide artworks."
  },
  {
    "objectID": "blog/2024-10-01-steal-like-a-generative-artist/index.html#scoping-the-project",
    "href": "blog/2024-10-01-steal-like-a-generative-artist/index.html#scoping-the-project",
    "title": "Steal like a generative artist",
    "section": "Scoping the project",
    "text": "Scoping the project\nThe parameters set by the user for this artwork would likely include: the number of rows and columns of “pyramids”, the four colours to be used or the sides of the pyramids and perhaps the spacing between the pyramids relative to their base size.\nThe randomness seems to come from the location of the “apex” of each square-based pyramid and the order in which the four face colours are applied.\nThe rules are the additional structure that stays the same between each generation. This might be things like the spacing between the pyramids, how close the apex can get to the edge of each square and that each face colour must appears exactly once on every pyramid.\nMy modifications to this will be to allow the user to specify two sets of colour inputs, one set of neutral colours and one set of accent colours. Each pyramid will get three neutral sides and one accent side."
  },
  {
    "objectID": "blog/2024-10-01-steal-like-a-generative-artist/index.html#keep-it-simple-stupid",
    "href": "blog/2024-10-01-steal-like-a-generative-artist/index.html#keep-it-simple-stupid",
    "title": "Steal like a generative artist",
    "section": "Keep It Simple, Stupid",
    "text": "Keep It Simple, Stupid\nI’ve used R to plot polygons quite a few times before but usually in the context of something like adding a credible region around a function estimate or to show the outline of a geographic region. In both cases that’s a either a single or small number of polygons where each has very many sides. What I need here is the exact opposite of that: a large number of polygons, each with a small number of sides (three to b e exact).\nBefore worrying about any of that though, there are a few parameters that I can set up-front, for example I can set a colour scheme that is aligned with my usual styling.\n\nlibrary(zvplot)\nneutrals = c(zv_black, zv_white, zv_light_grey, zv_mid_grey)\naccents = c(zv_navy, zv_orange, zv_fuchia, zv_blue)\n\nLet’s start with the smallest example possible where I will be able to tell that everything is working as it should, a 2x3 grid.\n\nn_rows = 2\nn_cols = 3\n\nTo make this easier on myself I’m going to start off just trying to generate the squares that will form the base of each pyramid. To make finding coordinates slightly easier, I’ll use a coordinate system where each square is 1 units. That will also make the parameter controlling the gap between pyramids easier to interpret.\n\ngap_size = 0.1\n\nI will be using {ggplot2} to make the plot, so I’ll need to organise the coordinates of my polygon corners into a data frame. As a first step, I’ll making a data frame that indexes and describes each of the squares I will need to plot.\n\nn_squares &lt;- n_rows * n_cols\nsquare_id &lt;- seq_len(n_squares)\nrow_num &lt;- rep(1:n_rows, each = n_cols)\ncol_num &lt;- rep(1:n_cols, times = n_rows)\nsquares &lt;- data.frame(id = square_id, row_num, col_num)\nsquares\n\n  id row_num col_num\n1  1       1       1\n2  2       1       2\n3  3       1       3\n4  4       2       1\n5  5       2       2\n6  6       2       3\n\n\nI can then add in the coordinates of each corner as new columns\n\nsquares$x_min &lt;- (1 + gap_size) * (squares$col_num - 1) + gap_size\nsquares$y_min &lt;- (1 + gap_size) * (squares$row_num - 1) + gap_size\nsquares$x_max &lt;- squares$x_min + 1\nsquares$y_max &lt;- squares$y_min + 1\n  \nsquares\n\n  id row_num col_num x_min y_min x_max y_max\n1  1       1       1   0.1   0.1   1.1   1.1\n2  2       1       2   1.2   0.1   2.2   1.1\n3  3       1       3   2.3   0.1   3.3   1.1\n4  4       2       1   0.1   1.2   1.1   2.2\n5  5       2       2   1.2   1.2   2.2   2.2\n6  6       2       3   2.3   1.2   3.3   2.2\n\n\nLooking at the examples from the geom_poly() documentation, I’ll need the x- and y-coordinates for each vertex of every square in two separate vectors and an id vector that links each vertex (row) to it’s specific polygon. As with polygon() in base R, it seems like polygons should be specified anti-clockwise.\nAfter a bit of head scratching, the most natural (but probably not most efficient) approach to me would be to interleave the bottom-left, bottom-right, top-right and top-left coordinates of each square that we just generated. The {vctrs} package has a helpful function to do this, which works for two or more vector-like objects of the same type.\n\nlibrary(vctrs)\nvec_interleave(as.character(1:5), letters[1:5])\n\n [1] \"1\" \"a\" \"2\" \"b\" \"3\" \"c\" \"4\" \"d\" \"5\" \"e\"\n\n\n\nvec_interleave(as.character(1:5), letters[1:5], LETTERS[1:5])\n\n [1] \"1\" \"a\" \"A\" \"2\" \"b\" \"B\" \"3\" \"c\" \"C\" \"4\" \"d\" \"D\" \"5\" \"e\" \"E\"\n\n\nTaking care to specify the coordinates in the right order, we can construct a dataframe that contains all the information needed to create the simplified plot.\n\nid &lt;- rep(squares$id, each = 4)\nx &lt;- vec_interleave(squares$x_min, squares$x_max, squares$x_max, squares$x_min)\ny &lt;- vec_interleave(squares$y_min, squares$y_min, squares$y_max, squares$y_max)\nvalue &lt;- sample(x = c(neutrals, accents), size = n_squares)\nvalue &lt;- rep(value, each = 4)\n\nplotting &lt;- data.frame(id, x, y, value)\nhead(plotting)\n\n  id   x   y   value\n1  1 0.1 0.1 #3D52D5\n2  1 1.1 0.1 #3D52D5\n3  1 1.1 1.1 #3D52D5\n4  1 0.1 1.1 #3D52D5\n5  2 1.2 0.1 #202020\n6  2 2.2 0.1 #202020\n\n\nThat is all the up-front work done, we should now have everything we need to get our minimal working example up and running. The code to create the plot itself is remarkably simple.\n\nlibrary(ggplot2)\n\nggplot(plotting, aes(x = x, y = y)) +\n  geom_polygon(aes(group = id), fill = value) +\n  coord_equal() +\n  theme_void()"
  },
  {
    "objectID": "blog/2024-10-01-steal-like-a-generative-artist/index.html#add-a-little-more-complexity",
    "href": "blog/2024-10-01-steal-like-a-generative-artist/index.html#add-a-little-more-complexity",
    "title": "Steal like a generative artist",
    "section": "Add a little more complexity",
    "text": "Add a little more complexity\nIt’s great that the simple square version works but it’s not particularly exciting. Let’s add in the second random aspect, the location of the apex or peak of each pyramid. To do this I’ll add the x- and y-coordinates of that peak as a column in the squares data frame.\nWhen generating peak locations, I could locate them uniformly at random over the unit square. One issue with this approach is that it might cause ugly realisations where the peak is very close to one of the edges. There are lots of ways to avoid this but I’ll keep it simple again and add a buffer zone that keeps the peak away from the edges.\n\nbuffer = 0.1 \n\nsquares$peak_x &lt;- runif(\n  n = n_squares,\n  min = squares$x_min + buffer,\n  max = squares$x_max - buffer)\n\nsquares$peak_y &lt;- runif(\n  n = n_squares,\n  min = squares$y_min + buffer,\n  max = squares$y_max - buffer)\n\nsquares\n\n  id row_num col_num x_min y_min x_max y_max    peak_x    peak_y\n1  1       1       1   0.1   0.1   1.1   1.1 0.4442918 0.3409578\n2  2       1       2   1.2   0.1   2.2   1.1 1.7946610 0.2581493\n3  3       1       3   2.3   0.1   3.3   1.1 3.0101058 0.6435676\n4  4       2       1   0.1   1.2   1.1   2.2 0.7543079 1.4341315\n5  5       2       2   1.2   1.2   2.2   2.2 1.9005211 1.5990998\n6  6       2       3   2.3   1.2   3.3   2.2 2.8278091 1.5572086\n\n\nOur logic and variables are all in place to make the more complicated pyramid plot. It’s exactly the same idea as before but with 24 triangles rather than 6 squares.\n\nid &lt;- rep(1:(n_squares * 4), each = 3)\nx &lt;- vec_interleave(squares$x_min, squares$peak_x, squares$x_min,\n                    squares$x_min, squares$peak_x, squares$x_max,\n                    squares$x_max, squares$x_max, squares$peak_x,\n                    squares$x_max, squares$x_min, squares$peak_x)\ny &lt;- vec_interleave(squares$y_min, squares$peak_y, squares$y_max,\n                    squares$y_max, squares$peak_y, squares$y_max,\n                    squares$y_max, squares$y_min, squares$peak_y,\n                    squares$y_min, squares$y_min, squares$peak_y)\nvalue &lt;- c()\nfor (i in 1:n_squares) {\n  colours &lt;- c(sample(neutrals, 3), sample(accents, 1)) # pick colours\n  colours &lt;- sample(colours, 4)                         # shuffle\n  value &lt;- c(value, colours)                            # record\n}\nvalue &lt;- rep(value, each = 3) # colour value for every vertex of every triangle\n\nplotting &lt;- data.frame(id, x, y, value)\nhead(plotting)\n\n  id         x         y   value\n1  1 0.1000000 0.1000000 #EEEEEE\n2  1 0.4442918 0.3409578 #EEEEEE\n3  1 0.1000000 1.1000000 #EEEEEE\n4  2 0.1000000 1.1000000 #003E74\n5  2 0.4442918 0.3409578 #003E74\n6  2 1.1000000 1.1000000 #003E74\n\n\nSince we set everything up in the same way, we can use the same simple code to plot our pyramids as we did for the squares.\n\nggplot(plotting, aes(x = x, y = y)) +\n  geom_polygon(aes(group = id), fill = value) +\n  coord_equal() +\n  theme_void()"
  },
  {
    "objectID": "blog/2024-10-01-steal-like-a-generative-artist/index.html#make-it-a-function",
    "href": "blog/2024-10-01-steal-like-a-generative-artist/index.html#make-it-a-function",
    "title": "Steal like a generative artist",
    "section": "Make it a function",
    "text": "Make it a function\nOkay, we have made one version of the artwork, but the joy of generative art is being able to easily get many realisations of the same artwork and to experiment with different parameter values.\nWe can make that a lot easier to do by consolidating the code above into a function.\n\npyramids &lt;- function(n_rows = 5, n_cols = 10, gap_size = 0.1, buffer = 0.1, neutrals = NULL, accents = NULL){\n  \n  # set palette if not defined by the user \n  if (is.null(neutrals)) { \n    neutrals &lt;- c(\"#202020\", \"#FAFAFA\", \"#EEEEEE\", \"#555555\")\n  }\n  if (is.null(accents)) {\n    accents &lt;- c(\"#003E74\", \"#E87800\", \"#C81E87\", \"#3D52D5\")\n  }\n\n  # define square bases of pyramids\n  n_squares &lt;- n_rows * n_cols\n  square_id &lt;- seq_len(n_squares)\n  row_num &lt;- rep(1:n_rows, each = n_cols)\n  col_num &lt;- rep(1:n_cols, times = n_rows)\n  squares &lt;- data.frame(id = square_id, row_num, col_num)\n\n  # add columns for coordinates\n  squares$x_min &lt;- (1 + gap_size) * (squares$col_num - 1) + gap_size\n  squares$y_min &lt;- (1 + gap_size) * (squares$row_num - 1) + gap_size\n  squares$x_max &lt;- squares$x_min + 1\n  squares$y_max &lt;- squares$y_min + 1\n  squares$peak_x &lt;- runif(n_squares, squares$x_min + buffer, squares$x_max - buffer)\n  squares$peak_y &lt;- runif(n_squares, squares$y_min + buffer, squares$y_max - buffer)\n\n  # create dataframe of polygons\n  id &lt;- rep(x = 1:(n_squares * 4), each = 3)\n  x &lt;- vec_interleave(squares$x_min, squares$peak_x, squares$x_min,\n                      squares$x_min, squares$peak_x, squares$x_max,\n                      squares$x_max, squares$x_max, squares$peak_x,\n                      squares$x_max, squares$x_min, squares$peak_x)\n  y &lt;- vec_interleave(squares$y_min, squares$peak_y, squares$y_max,\n                      squares$y_max, squares$peak_y, squares$y_max,\n                      squares$y_max, squares$y_min, squares$peak_y,\n                      squares$y_min, squares$y_min, squares$peak_y)\n  value &lt;- c()\n  for (i in 1:n_squares) {\n    colours &lt;- c(sample(neutrals, 3), sample(accents, 1)) # pick colours\n    colours &lt;- sample(colours, 4)                         # shuffle\n    value &lt;- c(value, colours)                            #record\n  }\n  value &lt;- rep(value, each = 3) # colour value for each vertex of every triangle\n  \n  plotting &lt;- data.frame(id, x, y, value)\n  \n  # return plot\n  ggplot(plotting, aes(x = x, y = y)) +\n    geom_polygon(aes(group = id), fill = value) +\n    coord_equal() +\n    theme_void()\n}\n\nThis lets us check that when we run the function multiple times we get different realisations.\npyramids(n_rows = 3, n_cols = 3)\npyramids(n_rows = 3, n_cols = 3)\npyramids(n_rows = 3, n_cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd when we set the same seed, we should be able to recreate a specific version of the artwork.\nset.seed(1234)\npyramids(n_rows = 3, n_cols = 3)\nset.seed(1234)\npyramids(n_rows = 3, n_cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe really fun part is exploring new parameter combinations. Can you figure out which settings I used below?"
  },
  {
    "objectID": "blog/2024-10-01-steal-like-a-generative-artist/index.html#try-it-yourself",
    "href": "blog/2024-10-01-steal-like-a-generative-artist/index.html#try-it-yourself",
    "title": "Steal like a generative artist",
    "section": "Try it yourself",
    "text": "Try it yourself\nIf you would like to make your own artwork in this style, you can either copy the code I’ve provided above. Alternatively, you can get started faster by using the packaged versions of the pyramids() function from {zvplot} - there’s a function for the simpler squares design too!\n\ndevtools::install_github(\"zakvarty/zvplot\")\n\nsquares(n_rows = 12, n_cols = 22)\npyramids(n_rows = 2, n_cols = 2)"
  },
  {
    "objectID": "blog/2024-10-01-steal-like-a-generative-artist/index.html#session-information",
    "href": "blog/2024-10-01-steal-like-a-generative-artist/index.html#session-information",
    "title": "Steal like a generative artist",
    "section": "Session Information",
    "text": "Session Information\n\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin20 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, utils, datasets, methods and base\nother attached packages: ggplot2(v.3.5.1), vctrs(v.0.6.5) and zvplot(v.0.0.0.9000)\nloaded via a namespace (and not attached): cli(v.3.6.3), knitr(v.1.45), rlang(v.1.1.4), xfun(v.0.43), showtextdb(v.3.0), sysfonts(v.0.8.9), generics(v.0.1.3), jsonlite(v.1.8.8), labeling(v.0.4.3), glue(v.1.8.0), colorspace(v.2.1-1), htmltools(v.0.5.8.1), scales(v.1.3.0), fansi(v.1.0.6), rmarkdown(v.2.26), pander(v.0.6.5), grid(v.4.3.3), evaluate(v.0.23), munsell(v.0.5.1), tibble(v.3.2.1), fastmap(v.1.1.1), yaml(v.2.3.8), lifecycle(v.1.0.4), compiler(v.4.3.3), dplyr(v.1.1.4), Rcpp(v.1.0.12), htmlwidgets(v.1.6.4), pkgconfig(v.2.0.3), rstudioapi(v.0.16.0), farver(v.2.1.2), digest(v.0.6.35), R6(v.2.5.1), tidyselect(v.1.2.1), utf8(v.1.2.4), showtext(v.0.9-7), pillar(v.1.9.0), magrittr(v.2.0.3), withr(v.3.0.1), tools(v.4.3.3) and gtable(v.0.3.5)"
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html",
    "href": "blog/2022-12-14-apis-and-httr/index.html",
    "title": "Aquiring Data via an API",
    "section": "",
    "text": "You can’t always rely on tidy, tabular data to land on your desk. Sometimes you are going to have to go out and gather data for yourself.\nI’m not suggesting you will need to do this manually, but you will likely need to get data from the internet that’s been made publicly or privately available to you.\nThis might be information from a webpage that you gather yourself, or data shared with you by a collaborator using an API.\nIn this second blog post we will cover the basics of obtaining data via an API. This material draws together the Introduction to APIs book by Brian Cooksey and the DIY web data section of STAT545 at the University of British Columbia."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#aquiring-data-part-2---using-apis",
    "href": "blog/2022-12-14-apis-and-httr/index.html#aquiring-data-part-2---using-apis",
    "title": "Aquiring Data via an API",
    "section": "",
    "text": "You can’t always rely on tidy, tabular data to land on your desk. Sometimes you are going to have to go out and gather data for yourself.\nI’m not suggesting you will need to do this manually, but you will likely need to get data from the internet that’s been made publicly or privately available to you.\nThis might be information from a webpage that you gather yourself, or data shared with you by a collaborator using an API.\nIn this second blog post we will cover the basics of obtaining data via an API. This material draws together the Introduction to APIs book by Brian Cooksey and the DIY web data section of STAT545 at the University of British Columbia."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#why-do-i-need-to-know-about-apis",
    "href": "blog/2022-12-14-apis-and-httr/index.html#why-do-i-need-to-know-about-apis",
    "title": "Aquiring Data via an API",
    "section": "Why do I need to know about APIs?",
    "text": "Why do I need to know about APIs?\n\nAn API, or application programming interface, is a set of rules that allows different software applications to communicate with each other.\n\nAs a data scientist, you will often need to access data that is stored on remote servers or in cloud-based services. APIs provide a convenient way for data scientists to programmatically retrieve this data, without having to manually download data sets or and process them locally on their own computer.\nThis has multiple benefits including automation and standardisation of data sharing.\n\nAutomation: It is much faster for a machine to process a data request than a human. Having a machine handling data requests also scales much better as either the number or the complexity of data requests grows. Additionally, there is a lower risk of introducing human error. For example, a human might accidentally share the wrong data, which can have serious legal repercussions.\nStandardisation: Having a machine process data requests requires the format of these requests and the associated responses to be standardised. This allows data sharing and retrieval to become a reproducible and programmatic aspect of our work."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#what-is-an-api",
    "href": "blog/2022-12-14-apis-and-httr/index.html#what-is-an-api",
    "title": "Aquiring Data via an API",
    "section": "What is an API?",
    "text": "What is an API?\nSo then, if APIs are so great, what exactly are they?\nIn human-to-human communication, the set of rules governing acceptable behaviour is known as etiquette. Depending on when or where you live, social etiquette can be rather strict. The rules for computer-to-computer communication take this to a whole new level, because with machines there can be no room left for interpretation.\nThe set of rules governing interactions between computers or programmes is known as a protocol.\nAPIs provide a standard protocol for different programs to interact with one another. This makes it easier for developers to build complex systems by leveraging the functionality of existing services and platforms. The benefits of working in a standardised and modular way apply equally well to sharing data as they do to writing code or organising files.\nThere are two sides to communication and when machines communicate these are known as the server and the client.\n\n\n\nServers can seem intimidating, because unlike your laptop or mobile phone they don’t have their own input and output devices; they have no keyboard, no monitor, and no a mouse. Despite this, servers are just regular computers that are designed to store data and run programmes. Servers don’t have their own input or output devices because they are intended to be used remotely, via another computer. There is no need for a screen or a mouse if the user is miles away. Nothing scary going on here!\nPeople often find clients much less intimidating - they are simply any other computer or application that might contact the sever."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#http",
    "href": "blog/2022-12-14-apis-and-httr/index.html#http",
    "title": "Aquiring Data via an API",
    "section": "HTTP",
    "text": "HTTP\nThis leads us one step further down the rabbit-hole. An API is a protocol that defines the rules of how applications communicate with one another. But how does this communication happen?\nHTTP (Hypertext Transfer Protocol) is the dominant mode communication on the World Wide Web. You can see the secure version of HTTP, HTTPS, at the start of most web addresses up at the top of your browser. For example:\nhttps://www.zakvarty.com/blog\nHTTP is the foundation of data communication on the web and is used to transfer files (such as text, images, and videos) between web servers and clients.\n\n\n\nTo understand HTTP communications, I find it helpful to imagine the client and the server as being a customer and a waiter at a restaurant. The client makes some request to the server, which then tries to comply before giving a response. The server might respond to confirm that the request was completed successfully. Alternatively, the server might respond with an error message, which is (hopefully) informative about why the request could not be completed.\nThis request-response model is the basis for HTTP, the communication system used by the majority of APIs."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#http-requests",
    "href": "blog/2022-12-14-apis-and-httr/index.html#http-requests",
    "title": "Aquiring Data via an API",
    "section": "HTTP Requests",
    "text": "HTTP Requests\nAn HTML request consists of:\n\nUniform Resource Locator (URL) [unique identifier for a thing]\nMethod [tells server the type of action requested by client]\nHeaders [meta-information about request, e.g. device type]\nBody [Data the client wants to send to the server]\n\n\n\n\n\nURL\nThe URL in a HTTP request specifies where that request is going to be made, for example http://example.com.\n\n\nMethod\nThe action that the client wants to take is indicated by a set of well-defined methods or HTTP verbs. The most common HTTP verbs are GET, POST, PUT, PATCH, and DELETE.\nThe GET verb is used to retrieve a resource from the server, such as a web page or an image. The POST verb is used to send data to the server, such as when submitting a form or uploading a file. The PUT verb is used to replace a resource on the server with a new one, while the PATCH verb is used to update a resource on the server without replacing it entirely. Finally, the DELETE verb is used to delete a resource from the server.\nIn addition to these common HTTP verbs, there are also several less frequently used verbs. These are used for specialized purposes, such as requesting only the headers of a resource, or testing the connectivity between the client and the server.\n\n\nHeader\nThe request headers contain meta-information about the request. This is where information about the device type would be included within the request.\n\n\nBody\nFinally, the body of the request contains the data that the client is providing to the server."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#http-responses",
    "href": "blog/2022-12-14-apis-and-httr/index.html#http-responses",
    "title": "Aquiring Data via an API",
    "section": "HTTP Responses",
    "text": "HTTP Responses\nWhen the server receives a request it will attempt to fulfil it and then send a response back to the client.\n\n\n\nA response has a similar structure to a request apart from:\n\nresponses do not have a URL,\nresponses do not have a method,\nresponses have a status code.\n\n\nStatus Codes\nThe status code is a 3 digit number, each of which has a specific meaning. Some common error codes that you might (already have) come across are:\n\n200: Success,\n404: Page not found (all 400s are errors),\n503: Page down.\n\nIn a data science context, a successful response will return the requested data within the data field. This will most likely be given in JSON or XML format."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#authentication",
    "href": "blog/2022-12-14-apis-and-httr/index.html#authentication",
    "title": "Aquiring Data via an API",
    "section": "Authentication",
    "text": "Authentication\nNow that we know how applications communicate, you might ask how we can control who has access to the API and what types of request they can make. This can be done by the server setting appropriate permissions for each client. But then how does the server verify that the client is really who is claims to be?\nAuthentication is a way to ensure that only authorized clients are able to access an API. This is typically done by the server requiring each client to provide some secret information that uniquely identifies them, whenever they make requests to the API. This information allows the API server to validate the authenticity this user before it authorises the request.\n\nBasic Authentication\nThere are various ways to implement API authentication.\nBasic authentication involves each legitimate client having a username and password. An encrypted version of these is included in the Authorization header of the HTTP request. If the hear matches with the server’s records then the request is processed. If not, then a special status code (401) is returned to the client.\nBasic authentication is dangerous because it does not put any restrictions on what a client can do once they are authorised. Additional, individualised restrictions can be added by using an alternative authentication scheme.\n\n\nAPI Key Authentication\nAn API key is long, random string of letters and numbers that is assigned to each authorised user. An API key is distinct from the user’s password and keys are typically issued by the service that provides an API. Using keys rather than basic authentication allows the API provider to track and limit the usage of their API.\nFor example, a provider may issue a unique API key to each developer or organization that wants to use the API. The provider can then limit access to certain data. They could also limit the number of requests that each key can make in a given time period or prevent access to certain administrative functions, like changing passwords or deleting accounts.\nUnlike Basic Authentication, there is no standard way of a client sharing a key with the server. Depending on the API this might be in the Authorization field of the header, at the end of the URL (http://example.com?api_key=my_secret_key), or within the body of the data."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#api-wrappers",
    "href": "blog/2022-12-14-apis-and-httr/index.html#api-wrappers",
    "title": "Aquiring Data via an API",
    "section": "API wrappers",
    "text": "API wrappers\nWe’ve learned a lot about how the internet works. Fortunately, a lot of the time we won’t have to worry about all of that new information other than for debugging purposes.\nIn the best case scenario, a very kind developer has written a “wrapper” function for the API. These wrappers are functions in R that will construct the HTML request for you. If you are particularly lucky, the API wrapper will also format the response for you, converting it from XML or JSON back into an R object that is ready for immediate use."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#geonames-wrapper",
    "href": "blog/2022-12-14-apis-and-httr/index.html#geonames-wrapper",
    "title": "Aquiring Data via an API",
    "section": "{geonames} wrapper",
    "text": "{geonames} wrapper\nrOpenSci has a curated list of many wrappers for accessing scientific data using R. We will focus on the GeoNames API, which gives open access to a geographical database. To access this data, we will use wrapper functions provided by the {geonames} package.\nThe aim here is to illustrate the important steps of getting started with a new API.\n\nSet up\nBefore we can get any data from the GeoNames API, we first need to do a little bit of set up.\n\nInstall and load {geonames} from CRAN\n\n\n#install.packages(\"geonames\")\nlibrary(geonames)\n\n\nCreate a user account for the GeoNames API\n\n\n\nActivate the account (see activation email)\n\n\n\nEnable the free web services for your GeoNames account by logging in at this link.\nTell R your credentials for GeoNames.\n\n\n\n\n\n\n\nWarning\n\n\n\nWe could use the following code to tell R our credentials, but we absolutely should not.\n\noptions(geonamesUsername=\"example_username\")\n\nThis would save our username as an environment variable, but it also puts our API credentials directly into the script. If we share the script with our others (internally, externally or publicly) we would be sharing our credentials too. Not good!"
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#keep-it-secret-keep-it-safe",
    "href": "blog/2022-12-14-apis-and-httr/index.html#keep-it-secret-keep-it-safe",
    "title": "Aquiring Data via an API",
    "section": "Keep it Secret, Keep it Safe",
    "text": "Keep it Secret, Keep it Safe\nThe solution to this problem is to add our credentials as environment variables in our .Rprofile rather than in this script. The .Rprofile is an R script that is run at the start of every session. IT can be created and edited directly, but can also be created and edited from within R.\nTo make/open your .Rprofile use the edit_r_profile() function from the {usethis} package.\n\nlibrary(usethis)\nusethis::edit_r_profile()\n\nWithin this file, add options(geonamesUsername=\"example_username\") on a new line, remembering to replace example_username with your own GeoNames username.\nThe final step is to check this this file ends with a blank line, save it and restart R. Then we are all set to start using {geonames}.\nThis set up procedure is indicative of most API wrappers, but of course the details will vary between each API. This is why good documentation is important!"
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#using-geonames",
    "href": "blog/2022-12-14-apis-and-httr/index.html#using-geonames",
    "title": "Aquiring Data via an API",
    "section": "Using {geonames}",
    "text": "Using {geonames}\nGeoNames has a whole host of different geo-datasets that you can explore. As a first example, let’s get all of the geo-tagged wikipedia articles that are within 1km of Imperial College London.\n\nimperial_coords &lt;- list(lat = 51.49876, lon = -0.1749)\nsearch_radius_km &lt;- 1\n\nimperial_neighbours &lt;- geonames::GNfindNearbyWikipedia(\n  lat = imperial_coords$lat,\n  lng = imperial_coords$lon, \n  radius = search_radius_km,\n  lang = \"en\",                # english language articles\n  maxRows = 500              # maximum number of results to return \n)\n\nLooking at the structure of imperial_neighbours we can see that it is a data frame with one row per geo-tagged wikipedia article.\n\nstr(imperial_neighbours)\n\n'data.frame':   204 obs. of  13 variables:\n $ summary     : chr  \"The Department of Mechanical Engineering is responsible for teaching and research in mechanical engineering at \"| __truncated__ \"Imperial College Business School is a global business school located in London. The business school was opened \"| __truncated__ \"Exhibition Road is a street in South Kensington, London which is home to several major museums and academic est\"| __truncated__ \"Imperial College School of Medicine (ICSM) is the medical school of Imperial College London in England, and one\"| __truncated__ ...\n $ elevation   : chr  \"20\" \"18\" \"19\" \"24\" ...\n $ feature     : chr  \"edu\" \"edu\" \"landmark\" \"edu\" ...\n $ lng         : chr  \"-0.1746\" \"-0.1748\" \"-0.17425\" \"-0.1757\" ...\n $ distance    : chr  \"0.0335\" \"0.0494\" \"0.0508\" \"0.0558\" ...\n $ rank        : chr  \"81\" \"91\" \"90\" \"96\" ...\n $ lang        : chr  \"en\" \"en\" \"en\" \"en\" ...\n $ title       : chr  \"Department of Mechanical Engineering, Imperial College London\" \"Imperial College Business School\" \"Exhibition Road\" \"Imperial College School of Medicine\" ...\n $ lat         : chr  \"51.498524\" \"51.4992\" \"51.4989722222222\" \"51.4987\" ...\n $ wikipediaUrl: chr  \"en.wikipedia.org/wiki/Department_of_Mechanical_Engineering%2C_Imperial_College_London\" \"en.wikipedia.org/wiki/Imperial_College_Business_School\" \"en.wikipedia.org/wiki/Exhibition_Road\" \"en.wikipedia.org/wiki/Imperial_College_School_of_Medicine\" ...\n $ countryCode : chr  NA \"AE\" NA \"GB\" ...\n $ thumbnailImg: chr  NA NA NA NA ...\n $ geoNameId   : chr  NA NA NA NA ...\n\n\nTo confirm we have the correct location we can inspect the title of the first five neighbours.\n\nimperial_neighbours$title[1:5]\n\n[1] \"Department of Mechanical Engineering, Imperial College London\"             \n[2] \"Imperial College Business School\"                                          \n[3] \"Exhibition Road\"                                                           \n[4] \"Imperial College School of Medicine\"                                       \n[5] \"Department of Civil and Environmental Engineering, Imperial College London\"\n\n\nNothing too surprising here, mainly departments of the college and Exhibition Road, which runs along one side of the campus. These sorts of check are important - I initially forgot the minus in the longitude and was getting results in East London!"
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#what-if-there-is-no-wrapper",
    "href": "blog/2022-12-14-apis-and-httr/index.html#what-if-there-is-no-wrapper",
    "title": "Aquiring Data via an API",
    "section": "What if there is no wrapper?",
    "text": "What if there is no wrapper?\nIf there is not a wrapper function, we can still access APIs fairly easilty using the {httr} package.\nWe will look at an example using OMDb, which is an open source version of IMDb, to get information about the movie Mean Girls.\nTo use the OMDB API you will once again need to request a free API key, follow a verification link and add your API key to your .Rprofile.\n\n# Add this to .Rprofile, pasting in your own API key\noptions(OMDB_API_Key = \"PASTE YOUR KEY HERE\")\n\nYou can then restart R and safely access your API key from within your R session.\n\n# Load your API key into the current R session,\nombd_api_key &lt;- getOption(\"OMDB_API_Key\")\n\nUsing the documentation for the API, requests have URLs of the following form, where terms in angular brackets should be replaced by you.\nhttp://www.omdbapi.com/?t=&lt;TITLE&gt;&y=&lt;YEAR&gt;&plot=&lt;LENGTH&gt;&r=&lt;FORMAT&gt;&apikey=&lt;API_KEY&gt;\nWith a little bit of effort, we can write a function that composes this type of request URL for us. We will using the {glue} package to help us join strings together.\n\n#' Compose search requests for the OMBD API\n#'\n#' @param title String defining title to search for. Words are separated by \"+\".\n#' @param year String defining release year to search for\n#' @param plot String defining whether \"short\" or \"full\" plot is returned\n#' @param format String defining return format. One of \"json\" or \"xml\"\n#' @param api_key String defining your OMDb API key.\n#'\n#' @return String giving a OMBD search request URL\n#'\n#' @examples \n#' omdb_url(\"mean+girls\", \"2004\", \"short\", \"json\", getOption(OMBD_API_Key))\n#' \nomdb_url &lt;- function(title, year, plot, format, api_key) {\n  glue::glue(\"http://www.omdbapi.com/?t={title}&y={year}&plot={plot}&r={format}&apikey={api_key}\")\n}\n\nRunning the example we get:\n\nmean_girls_request &lt;- omdb_url(\n  title = \"mean+girls\",\n  year =  \"2004\",\n  plot = \"short\",\n  format =  \"json\",\n  api_key =  getOption(\"OMDB_API_Key\"))\n\nWe can then use the {httr} package to construct our request and store the response we get.\n\nresponse &lt;- httr::GET(url = mean_girls_request)\nhttr::status_code(response)\n\n[1] 200\n\n\nThankfully it was a success! If you get a 401 error code here, check that you have clicked the activation link for your API key.\nThe full structure of the response is quite complicated, but we can easily extract the requested data using content()\n\nhttr::content(response)\n\n$Title\n[1] \"Mean Girls\"\n\n$Year\n[1] \"2004\"\n\n$Rated\n[1] \"PG-13\"\n\n$Released\n[1] \"30 Apr 2004\"\n\n$Runtime\n[1] \"97 min\"\n\n$Genre\n[1] \"Comedy\"\n\n$Director\n[1] \"Mark Waters\"\n\n$Writer\n[1] \"Rosalind Wiseman, Tina Fey\"\n\n$Actors\n[1] \"Lindsay Lohan, Jonathan Bennett, Rachel McAdams\"\n\n$Plot\n[1] \"Cady Heron is a hit with The Plastics, the A-list girl clique at her new school, until she makes the mistake of falling for Aaron Samuels, the ex-boyfriend of alpha Plastic Regina George.\"\n\n$Language\n[1] \"English, German, Vietnamese, Swahili\"\n\n$Country\n[1] \"United States, Canada\"\n\n$Awards\n[1] \"7 wins & 25 nominations\"\n\n$Poster\n[1] \"https://m.media-amazon.com/images/M/MV5BMjE1MDQ4MjI1OV5BMl5BanBnXkFtZTcwNzcwODAzMw@@._V1_SX300.jpg\"\n\n$Ratings\n$Ratings[[1]]\n$Ratings[[1]]$Source\n[1] \"Internet Movie Database\"\n\n$Ratings[[1]]$Value\n[1] \"7.1/10\"\n\n\n$Ratings[[2]]\n$Ratings[[2]]$Source\n[1] \"Rotten Tomatoes\"\n\n$Ratings[[2]]$Value\n[1] \"84%\"\n\n\n$Ratings[[3]]\n$Ratings[[3]]$Source\n[1] \"Metacritic\"\n\n$Ratings[[3]]$Value\n[1] \"66/100\"\n\n\n\n$Metascore\n[1] \"66\"\n\n$imdbRating\n[1] \"7.1\"\n\n$imdbVotes\n[1] \"385,107\"\n\n$imdbID\n[1] \"tt0377092\"\n\n$Type\n[1] \"movie\"\n\n$DVD\n[1] \"21 Sep 2004\"\n\n$BoxOffice\n[1] \"$86,058,055\"\n\n$Production\n[1] \"N/A\"\n\n$Website\n[1] \"N/A\"\n\n$Response\n[1] \"True\""
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#wrapping-up",
    "href": "blog/2022-12-14-apis-and-httr/index.html#wrapping-up",
    "title": "Aquiring Data via an API",
    "section": "Wrapping up",
    "text": "Wrapping up\nWe have learned a bit more about how the internet works, the benefits of using an API to share data and how to request data from Open APIs.\nWhen obtaining data from the internet it’s vital that you keep your credentials safe, and that don’t do more work than is needed.\n\nKeep your API keys out of your code. Store them in your .Rprofile (and make sure this is not under version control!)\nScraping is always a last resort. Is there an API already?\nWriting your own code to access an API can be more painful than necessary.\nDon’t repeat other people, if a suitable wrapper exists then use it."
  },
  {
    "objectID": "blog/2023-08-08-overcoming-barriers-to-sharing-code/index.html",
    "href": "blog/2023-08-08-overcoming-barriers-to-sharing-code/index.html",
    "title": "Overcoming Barriers to Sharing Code",
    "section": "",
    "text": "Image credit: Toronto Workshop on Reproducibility.\n\n\n\nalexander2023overcoming\n[pdf notes]\n\nTitle: Overcoming barriers to sharing code. {2023}.\nAuthor: Monica Alexander\nKey words: Data Science, Reproducibility, Pedagogy.\nIn this conference talk, Monica Alexander discusses the structural and psychological barriers that prevent people from sharing their code. Drawing from her lived- and teaching-experience, Alexander argues that the most common reasons are low confidence and lack of exposure to open and reproducible workflows; she suggests how each of these could be built into existing teaching and research project workflows. These adaptations are important because people develop skills through imitation and repetition, overcoming their fear with each successful iteration: leading by example and creating opportunities for success both encourage code sharing. Experienced data scientists or course leaders are the intended audience for this talk, but its candid and supportive tone mean that it accessible to learners too.\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "academic-tutorials.html",
    "href": "academic-tutorials.html",
    "title": "Year 1 Academic Tutorials",
    "section": "",
    "text": "Problem sheet\nSolution sheet\nTutor sheet\n\n\n\n\n\nProblem sheet\nSolution sheet\nTutor sheet\n\n\n\n\n\nProblem sheet\nSolution sheet\nTutor sheet\n\n\n\n\n\nProblem sheet\nSolution sheet\nTutor sheet\n\n\n\n\n\nProblem sheet\nSolution sheet\nTutor sheet"
  },
  {
    "objectID": "academic-tutorials.html#term-1",
    "href": "academic-tutorials.html#term-1",
    "title": "Year 1 Academic Tutorials",
    "section": "",
    "text": "Problem sheet\nSolution sheet\nTutor sheet\n\n\n\n\n\nProblem sheet\nSolution sheet\nTutor sheet\n\n\n\n\n\nProblem sheet\nSolution sheet\nTutor sheet\n\n\n\n\n\nProblem sheet\nSolution sheet\nTutor sheet\n\n\n\n\n\nProblem sheet\nSolution sheet\nTutor sheet"
  },
  {
    "objectID": "professional/topic-reading-list.html",
    "href": "professional/topic-reading-list.html",
    "title": "Topic Reading List",
    "section": "",
    "text": "Topic Reading List\n\nProbability Trees\n\nmaths.org Introduction to probability trees.\nCatech MA103 Notes Introductory course on probability in statistics. Lecture 04 covers conditional probability and probability trees. Written by Kim C Border found via Paul J Healy.\nKhan academy video Worked example of constructing and using a probability tree.\n\n\n\nRandomised Response\n\nKearns, M. & Roth, A. (2020). The ethical algorithm: the science of socially aware algorithm design. Oxford University Press. [Chapter 1 - Algorithmic Privacy]\nWarner, S. (1965) Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias. Journal of the American Statistical Association, 60(309), 63–69.\nBlair, G., Imai, K. & Zhou, Y. (2015) Design and Analysis of the Randomized Response Technique. Journal of the American Statistical Association, 110(511), 1304-1319.\nLeslie, J., Loewenstein, G., Acquisti, A. & Vosgerau, J. (2018). When and why randomized response techniques (fail to) elicit the truth. Organizational Behavior and Human Decision Processes, 148, 101-123."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My research interests lie in the intersection between applied and methodological statistics. My past projects focused on missing data, predictive analytics, extreme value analysis and point process modelling.\nI’m particularly interested in developing efficient inference methods for non-standard data generating mechanisms to better model the rich, complex data sets that arise from environmental and industrial processes.\n\n\n\nAutomated threshold selection and associated inference uncertainty for univariate extremes. (arXiv) Work with Conor Murphy, developing a novel methodology for automated threshold selection in univariate extreme value modelling and to propagate uncertainty in this threshold choice through to estimates of extreme quantiles. The effectiveness of our method is demonstrated though an extensive simulation study, compared to the leading existing methods and applied to the well-known and troublesome example of the River Nidd dataset.\nInference for extreme earthquake magnitudes accounting for a time-varying measurement process. (arXiv) Motivated by earthquake catalogues, we consider variable data quality in the form of rounded and incompletely observed data. We develop an approach to select a time-varying modelling threshold that makes best use of the available data in an extreme value analysis, accounting for uncertainty in the magnitude model and for the rounding of observations.\nStatistical Modelling of Induced Earthquakes (PhD Thesis). My PhD thesis focused on how to model anthropogenic earthquakes while making best use of the limited available data. Firstly, a selection of physically-motivated parametric models are explored for describing the link between gas extraction and induced earthquake locations. Secondly, new inference methods are developed that allow improvements to the earthquake detection network to be included when modelling extreme earthquake magnitudes. Finally, a reparameterised and extended version of the Epidemic Type Aftershock model introduced. This both allows for more efficient inference and relaxes the common assumption of independent and identically distributed magnitudes.\nA review of simulated annealing techniques: Simulated annealing is a metahuristic technique mainly used for combinatorial optimisation. Applications, parallelisation and extensions of the technique were reviewed.\nInference on censored networks: Networks are censored when existing nodes or edges are not observed. Methods for inference under different types of missingness were explored. Master’s project supervised by Dr. Christopher Nemeth.\nComputationally intensive methods for modelling household epidemics: Approximate Bayesian Computation was utilised to allow inference on disease models with intractable likelihoods. Master’s dissertation supervised by Prof. Peter Neal.\n\n\n\n\n\n\n\n\n\n\n\nDate\nEvent\nLocation\n\n\n\n\nSep 2024\nRoyal Statistical Society conference\nBrighton, UK.\n\n\nJun 2024\nUK Conference on Teaching Statistics\nManchester, UK.\n\n\nNov 2023\nRSS Local Group Meeting\nBath, UK.\n\n\nSep 2023\nRoyal Statistical Society conference\nHarrogate, UK.\n\n\nSep 2023\nRSS pre-conference workshop\nHarrogate, UK.\n\n\nJun 2023\nIMA Idea Exchange: Mathematicians and Statisticians Teaching in Higher Education\nRemote.\n\n\nSep 2022\nRoyal Statistical Society conference\nAberdeen, UK.\n\n\nJun 2022\nM_max workshop\nAmsterdam, NL.\n\n\nJan 2021\nCRG Extremes workshop\nRemote.\n\n\nMay 2020\nSTOR-i time-series and spatial statistics workshop\nRemote.\n\n\nSept 2019\nInterfaces in extreme value theory workshop\nLancaster, UK.\n\n\nSept 2019\nRoyal Statistical Society conference\nBelfast, UK.\n\n\nAug 2019\nInternational statistical seismology workshop (StatSei11)\nHakone, JPN.\n\n\nJul 2019\nGRASPA (Italian Environmetics Society)\nPescara, IT.\n\n\nJan 2019\nSTOR-i annual conference\nLancaster, UK.\n\n\nJan 2018\nSTOR-i annual conference\nLancaster, UK.\n\n\n\n\n\n\nI have had the good fortune to supervise some exceptional early career researchers in their postgraduate and undergraduate research projects. Below is a list of the students I have supervised along with their project titles.\n\n\n\nConor Murphy (Oct 2021 - Present) - Assessment of hazard and risk due to induced seismicity for underground CO2 Storage and oil and gas production assets.\nWanchen Yue (Oct 2023 - Present) - Statistical earthquake models to account for measurement errors and dependence.\n\n\n\n\n\n\n\n\nBrian Mac Carvill - Modelling extreme changes in crude oil prices.\nYinglai Qi - Investigating the risk of developing diabetes using UK Biobank data.\nEtienne Caprioli - Modelling extreme natural hazards in the United States of America.\nIoannis Spanos - Peaks over threshold modelling with missing data.\nXinran Huang - Nonstationary ETAS models for temporal variations in earthquake occurrences\nTommy Tong - Analyzing the Dynamics and Catastrophic Events of Space Weather.\nZihan Yan - Analysis of significant wave heights in the North Atlantic Ocean.\nJiahui Chen - Statistical modelling of earthquakes in the North Anatolian Fault Zone.\nJianjing Yu - Rare events in financial time series.\n\n\n\n\n\n\nWanchen Yue - Statistical earthquake models to account for measurement errors and dependence.\nAsh Bellett - Contextual bandits with non-stationary Gaussian process rewards.\nChristian Liman - Multiple imputation for tree-based models.\nLi Vern Teo - Catch me if you CNN: adversarial machine learning for detecting synthetic content.\nMinjian Wu - Outlier detection and adaptation under covariate shift.\nPaula Cordero Encinar - Representing ignorance about extreme earthquake magnitudes.\nLucy He - Measurement errors in earthquake modelling.\nShantianfang Gao - Inhomogeneous Poisson point process estimation using spline methods.\n\n\n\n\n\n\nDiana Xu - An extreme value mixture model for natural gas prices.\nHugo Barnett - Hypothesis testing with the self-inhibiting Hawkes process.\nNan Zhou - Self-driving vehicles road safety analysis by application of extreme value theory.\nXuan Hou - An extreme value analysis of rainfall in London.\nConor Murphy - Assessment of hazard and risk due to induced seismicity for underground CO2 Storage and oil and gas production assets.\n\n\n\n\n\n\nNatalie Young - Point pattern analysis in statistical ecology.\nPeter Greenstreet - Self-exciting point process models."
  },
  {
    "objectID": "research.html#research-interests",
    "href": "research.html#research-interests",
    "title": "Research",
    "section": "",
    "text": "My research interests lie in the intersection between applied and methodological statistics. My past projects focused on missing data, predictive analytics, extreme value analysis and point process modelling.\nI’m particularly interested in developing efficient inference methods for non-standard data generating mechanisms to better model the rich, complex data sets that arise from environmental and industrial processes."
  },
  {
    "objectID": "research.html#projects",
    "href": "research.html#projects",
    "title": "Research",
    "section": "",
    "text": "Automated threshold selection and associated inference uncertainty for univariate extremes. (arXiv) Work with Conor Murphy, developing a novel methodology for automated threshold selection in univariate extreme value modelling and to propagate uncertainty in this threshold choice through to estimates of extreme quantiles. The effectiveness of our method is demonstrated though an extensive simulation study, compared to the leading existing methods and applied to the well-known and troublesome example of the River Nidd dataset.\nInference for extreme earthquake magnitudes accounting for a time-varying measurement process. (arXiv) Motivated by earthquake catalogues, we consider variable data quality in the form of rounded and incompletely observed data. We develop an approach to select a time-varying modelling threshold that makes best use of the available data in an extreme value analysis, accounting for uncertainty in the magnitude model and for the rounding of observations.\nStatistical Modelling of Induced Earthquakes (PhD Thesis). My PhD thesis focused on how to model anthropogenic earthquakes while making best use of the limited available data. Firstly, a selection of physically-motivated parametric models are explored for describing the link between gas extraction and induced earthquake locations. Secondly, new inference methods are developed that allow improvements to the earthquake detection network to be included when modelling extreme earthquake magnitudes. Finally, a reparameterised and extended version of the Epidemic Type Aftershock model introduced. This both allows for more efficient inference and relaxes the common assumption of independent and identically distributed magnitudes.\nA review of simulated annealing techniques: Simulated annealing is a metahuristic technique mainly used for combinatorial optimisation. Applications, parallelisation and extensions of the technique were reviewed.\nInference on censored networks: Networks are censored when existing nodes or edges are not observed. Methods for inference under different types of missingness were explored. Master’s project supervised by Dr. Christopher Nemeth.\nComputationally intensive methods for modelling household epidemics: Approximate Bayesian Computation was utilised to allow inference on disease models with intractable likelihoods. Master’s dissertation supervised by Prof. Peter Neal."
  },
  {
    "objectID": "research.html#conference-and-workshop-contributions",
    "href": "research.html#conference-and-workshop-contributions",
    "title": "Research",
    "section": "",
    "text": "Date\nEvent\nLocation\n\n\n\n\nSep 2024\nRoyal Statistical Society conference\nBrighton, UK.\n\n\nJun 2024\nUK Conference on Teaching Statistics\nManchester, UK.\n\n\nNov 2023\nRSS Local Group Meeting\nBath, UK.\n\n\nSep 2023\nRoyal Statistical Society conference\nHarrogate, UK.\n\n\nSep 2023\nRSS pre-conference workshop\nHarrogate, UK.\n\n\nJun 2023\nIMA Idea Exchange: Mathematicians and Statisticians Teaching in Higher Education\nRemote.\n\n\nSep 2022\nRoyal Statistical Society conference\nAberdeen, UK.\n\n\nJun 2022\nM_max workshop\nAmsterdam, NL.\n\n\nJan 2021\nCRG Extremes workshop\nRemote.\n\n\nMay 2020\nSTOR-i time-series and spatial statistics workshop\nRemote.\n\n\nSept 2019\nInterfaces in extreme value theory workshop\nLancaster, UK.\n\n\nSept 2019\nRoyal Statistical Society conference\nBelfast, UK.\n\n\nAug 2019\nInternational statistical seismology workshop (StatSei11)\nHakone, JPN.\n\n\nJul 2019\nGRASPA (Italian Environmetics Society)\nPescara, IT.\n\n\nJan 2019\nSTOR-i annual conference\nLancaster, UK.\n\n\nJan 2018\nSTOR-i annual conference\nLancaster, UK."
  },
  {
    "objectID": "research.html#research-project-supervision",
    "href": "research.html#research-project-supervision",
    "title": "Research",
    "section": "",
    "text": "I have had the good fortune to supervise some exceptional early career researchers in their postgraduate and undergraduate research projects. Below is a list of the students I have supervised along with their project titles.\n\n\n\nConor Murphy (Oct 2021 - Present) - Assessment of hazard and risk due to induced seismicity for underground CO2 Storage and oil and gas production assets.\nWanchen Yue (Oct 2023 - Present) - Statistical earthquake models to account for measurement errors and dependence.\n\n\n\n\n\n\n\n\nBrian Mac Carvill - Modelling extreme changes in crude oil prices.\nYinglai Qi - Investigating the risk of developing diabetes using UK Biobank data.\nEtienne Caprioli - Modelling extreme natural hazards in the United States of America.\nIoannis Spanos - Peaks over threshold modelling with missing data.\nXinran Huang - Nonstationary ETAS models for temporal variations in earthquake occurrences\nTommy Tong - Analyzing the Dynamics and Catastrophic Events of Space Weather.\nZihan Yan - Analysis of significant wave heights in the North Atlantic Ocean.\nJiahui Chen - Statistical modelling of earthquakes in the North Anatolian Fault Zone.\nJianjing Yu - Rare events in financial time series.\n\n\n\n\n\n\nWanchen Yue - Statistical earthquake models to account for measurement errors and dependence.\nAsh Bellett - Contextual bandits with non-stationary Gaussian process rewards.\nChristian Liman - Multiple imputation for tree-based models.\nLi Vern Teo - Catch me if you CNN: adversarial machine learning for detecting synthetic content.\nMinjian Wu - Outlier detection and adaptation under covariate shift.\nPaula Cordero Encinar - Representing ignorance about extreme earthquake magnitudes.\nLucy He - Measurement errors in earthquake modelling.\nShantianfang Gao - Inhomogeneous Poisson point process estimation using spline methods.\n\n\n\n\n\n\nDiana Xu - An extreme value mixture model for natural gas prices.\nHugo Barnett - Hypothesis testing with the self-inhibiting Hawkes process.\nNan Zhou - Self-driving vehicles road safety analysis by application of extreme value theory.\nXuan Hou - An extreme value analysis of rainfall in London.\nConor Murphy - Assessment of hazard and risk due to induced seismicity for underground CO2 Storage and oil and gas production assets.\n\n\n\n\n\n\nNatalie Young - Point pattern analysis in statistical ecology.\nPeter Greenstreet - Self-exciting point process models."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I am fortunate to have had the opportunity to teach in a variety of roles. These have included:\n\nDeveloping and teaching modules in statistics, data science and data ethics.\nDesigning courses for both in-person and remote learning, predominantly at the postgraduate level.\nSupervising undergraduate, postgraduate and doctoral research projects.\nAdapting and leading short courses on scientific writing and communication.\nRunning workshops and computer labs for undergraduate and postgraduate modules.\nSpeaking at university open days and providing one-to-one tuition to high school students.\n\nI am an associate fellow of the Higher Education Academy, which you can learn more about here.  You can find descriptions and selected materials from the courses I have taught down below.\n\n\n\n\n\n\n\n Notes   Videos   Resources\nModel building and evaluation are necessary but not sufficient skills for the effective practice of data science. In this module you will develop the technical and personal skills that are required to work successfully as a data scientist within an organisation.\nDuring this module you will critically explore how to:\n\neffectively scope and manage a data science project;\nwork openly and reproducibly;\nefficiently acquire, manipulate, and present data;\ninterpret and explain your work for a variety of stakeholders;\nensure that your work can be put into production;\nassess the ethical implications of your work as a data scientist.\n\nThis interdisciplinary course draws from fields including statistics, computing, management science and data ethics. Each topic will is investigated through a selection of lecture videos, conference presentations and academic papers, supported by hands-on lab exercises and readings on industry best-practices as published by recognised professional bodies.\n\n\n\n\n\n\n Reading List   Part 1   Part 2   Part 3\nData-driven decision making is now pervasive and impacts us all. Your data is used by others to make decisions about who you are, how you will behave, and what options should be made available to you. Predictive models are used to decide anything from the promotions that are offered to you by a retailer through to whether your loan application is granted by a bank.\nThe ways in which these predictive models can fail mathematically form a core part of the training for an aspiring statistician, data scientist or machine learning practitioner. In contrast, the potential for ethical failures in these same models is rarely covered in-depth as part of this initial training. As a result, these ethical modes of failure are often not considered until those predictive models have been put into production and are actively causing harm.\nTo prevent this harm, the ethical impacts of using data to make decisions must be made core to the curriculum of both statistics and data science. This course aims to address that gap.\nThe course takes a practical and technical approach to identifying these ethical issues. It has a strong mathematical focus and will not not require the authoring of extended essays or moral treaties. Throughout the course, you will discover actionable ways in which these topics may be integrated into a data science workflow at a range of levels.\n\n\n\n\n\n\n Notes\nCourse materials to guide a first exploration of stochastic point processes.\nWe will have four sessions, each focusing in on one aspect of stochastic point processes: theory, simulation, inference and applications. The materials for these sessions focus mainly on the Poisson process in one dimension, which considered both as a special example of a stochastic process and also as a statistical model for point pattern data. We will explore this simple model together and I will point you in the direction of interesting areas to further explore for yourself.\nThese materials were created as a starting point for undergraduate research projects at Imperial College London. The intended reader is an undergraduate student at the end of their first year of study, who is familiar with the basics of probability and statistical inference and has some programming experience. The materials might also be useful to anyone with a similar background who is learning about point process models for the first time.\n\n\n\n\n\n\n Videos\nThis course introduces the framework of supervised learning with an emphasis on principled and uncertainty-aware modelling. (It is a stats course after all!)\nWe discuss both classical and modern approaches to regression and classification problems, along with issues that arise when working with flexible non-parametric models. We start with a review of the linear modelling framework for regression and classification problems before extending this class of models, first to generalized linear models and then generalised additive models. Following this, we consider a range of modern non-parametric alternatives, discussing both their benefits and drawbacks.\nThroughout the course, we will evaluate and compare the performance of these models on datasets from a variety of scientific problem domains.\n\n\n\n\n\n\n\n Notes   Videos\nThe majority of approaches in statistics and data science aim to learn about the “typical” levels of a process. In some cases though, we really want to understand those outcomes that are unusually large or small. We might want to understand when the weakest link in a chain will break, to predict record-breaking athletic performances or to describe the size and frequency of the largest financial losses.\nTo answer these questions we must extrapolate beyond the range of observed outcomes. This can be a risky business and requires careful, domain-specific considerations. Extreme value theory gives us a principled, mathematically motivated framework for modelling the largest (or smallest) values within a dataset.\nThis course will introduce you to a formal strategy for modelling the tails of random variables. We will develop this modelling framework based on an area of probability theory that is rarely studied at undergraduate level, paying close attention to the issues that come with using such asymptotically motivated models. We will see how techniques from standard statistical inference can be adapted into this modelling framework and apply it to a range of examples from finance and the environmental sciences."
  },
  {
    "objectID": "teaching.html#my-courses",
    "href": "teaching.html#my-courses",
    "title": "Teaching",
    "section": "",
    "text": "Notes   Videos   Resources\nModel building and evaluation are necessary but not sufficient skills for the effective practice of data science. In this module you will develop the technical and personal skills that are required to work successfully as a data scientist within an organisation.\nDuring this module you will critically explore how to:\n\neffectively scope and manage a data science project;\nwork openly and reproducibly;\nefficiently acquire, manipulate, and present data;\ninterpret and explain your work for a variety of stakeholders;\nensure that your work can be put into production;\nassess the ethical implications of your work as a data scientist.\n\nThis interdisciplinary course draws from fields including statistics, computing, management science and data ethics. Each topic will is investigated through a selection of lecture videos, conference presentations and academic papers, supported by hands-on lab exercises and readings on industry best-practices as published by recognised professional bodies.\n\n\n\n\n\n\n Reading List   Part 1   Part 2   Part 3\nData-driven decision making is now pervasive and impacts us all. Your data is used by others to make decisions about who you are, how you will behave, and what options should be made available to you. Predictive models are used to decide anything from the promotions that are offered to you by a retailer through to whether your loan application is granted by a bank.\nThe ways in which these predictive models can fail mathematically form a core part of the training for an aspiring statistician, data scientist or machine learning practitioner. In contrast, the potential for ethical failures in these same models is rarely covered in-depth as part of this initial training. As a result, these ethical modes of failure are often not considered until those predictive models have been put into production and are actively causing harm.\nTo prevent this harm, the ethical impacts of using data to make decisions must be made core to the curriculum of both statistics and data science. This course aims to address that gap.\nThe course takes a practical and technical approach to identifying these ethical issues. It has a strong mathematical focus and will not not require the authoring of extended essays or moral treaties. Throughout the course, you will discover actionable ways in which these topics may be integrated into a data science workflow at a range of levels.\n\n\n\n\n\n\n Notes\nCourse materials to guide a first exploration of stochastic point processes.\nWe will have four sessions, each focusing in on one aspect of stochastic point processes: theory, simulation, inference and applications. The materials for these sessions focus mainly on the Poisson process in one dimension, which considered both as a special example of a stochastic process and also as a statistical model for point pattern data. We will explore this simple model together and I will point you in the direction of interesting areas to further explore for yourself.\nThese materials were created as a starting point for undergraduate research projects at Imperial College London. The intended reader is an undergraduate student at the end of their first year of study, who is familiar with the basics of probability and statistical inference and has some programming experience. The materials might also be useful to anyone with a similar background who is learning about point process models for the first time.\n\n\n\n\n\n\n Videos\nThis course introduces the framework of supervised learning with an emphasis on principled and uncertainty-aware modelling. (It is a stats course after all!)\nWe discuss both classical and modern approaches to regression and classification problems, along with issues that arise when working with flexible non-parametric models. We start with a review of the linear modelling framework for regression and classification problems before extending this class of models, first to generalized linear models and then generalised additive models. Following this, we consider a range of modern non-parametric alternatives, discussing both their benefits and drawbacks.\nThroughout the course, we will evaluate and compare the performance of these models on datasets from a variety of scientific problem domains.\n\n\n\n\n\n\n\n Notes   Videos\nThe majority of approaches in statistics and data science aim to learn about the “typical” levels of a process. In some cases though, we really want to understand those outcomes that are unusually large or small. We might want to understand when the weakest link in a chain will break, to predict record-breaking athletic performances or to describe the size and frequency of the largest financial losses.\nTo answer these questions we must extrapolate beyond the range of observed outcomes. This can be a risky business and requires careful, domain-specific considerations. Extreme value theory gives us a principled, mathematically motivated framework for modelling the largest (or smallest) values within a dataset.\nThis course will introduce you to a formal strategy for modelling the tails of random variables. We will develop this modelling framework based on an area of probability theory that is rarely studied at undergraduate level, paying close attention to the issues that come with using such asymptotically motivated models. We will see how techniques from standard statistical inference can be adapted into this modelling framework and apply it to a range of examples from finance and the environmental sciences."
  },
  {
    "objectID": "ethics-reading-list.html",
    "href": "ethics-reading-list.html",
    "title": "Ethics Reading List",
    "section": "",
    "text": "Assessed Reading\n\nFloridi, L., & Cowls, J. (2019). A Unified Framework of Five Principles for AI in Society. Harvard Data Science Review, 1(1).\n\nCore Reading\n\nKearns, M. & Roth, A. (2020). The ethical algorithm: the science of socially aware algorithm design. Oxford University Press. [Introduction chapter]\n\nLive Session Documents\n\nEuropean Union Guidelines\nRoyal Statistical Society Guidelines\nDutch Government Guidelines\nUK Government Guidelines\nAmerican Mathematical Society Guidelines\n\n\n\n\nAssessed Reading\n\nNarayanan, & Shmatikov, V. (2008). Robust De-anonymization of Large Sparse Datasets. 2008 IEEE Symposium on Security and Privacy (sp 2008), 111–125.\n\nCore Reading\n\nKearns, M. & Roth, A. (2020). The ethical algorithm: the science of socially aware algorithm design. Oxford University Press. [Chapter 1 - Algorithmic Privacy]\n\n\n\n\nAssessed Reading\n\nBuolamwini, J., & Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency (pp. 77-91). PMLR.\n\nCore Reading\n\nKearns, M. & Roth, A. (2020). The ethical algorithm: the science of socially aware algorithm design. Oxford University Press. [Chapter 2 - Algorithmic Fairness]\n\nSupplementary Reading\n\nVerma, S., & Rubin, J. (2018). Fairness definitions explained. In 2018 ieee/acm international workshop on software fairness (fairware) (pp. 1-7). IEEE.\n\n\n\n\nAssessed Reading\n\nAwad, E., et al. (2018). The moral machine experiment. Nature 563, 59-64.\n\nCore Reading\n\nGunantara, N. (2018). A review of multi-objective optimization: methods and its applications. Cogent Engineering 5:1.\n\n\n\n\nAssessed Reading\n\nRibeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why should I trust you?” Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD, International Conference on Knowledge Discovery and Data Mining (pp. 1135-1144).\n\nCore Reading\n\nMolnar, C. (2020). Interpretable machine learning.\n\nChapter 8.1 on Partial Dependence Plots\nChapter 9.2 on LIME\n\n\nSupplementary Reading\n\nRecording of LIME conference presentation (in supplementary material section)\n\n\n\n\nAssessed Reading\n\nAmeisen. (2020). Building machine learning powered applications: going from idea to product (First edition). O’Reilly. [Chapter 11 - Monitor and Update Models]\n\nCore Reading\n\nWilson, G., Bryan, J., Cranston, K., Kitzes, J., Nederbragt, L., & Teal, T. K. (2017). Good enough practices in scientific computing. PLoS computational biology, 13(6).\n\n\n\n\n\n\n\n\n\n\n\nAssessed Reading\n\nGuthery, F. & Bingham, R. (2007). A Primer on Interpreting Regression Models. Journal of Wildlife Management 71(3):684–692.\n\nCore Reading\n\nMolnar, C. (2020). Interpretable machine learning.\n\nChapter 5.1 - Linear Regression\nChapter 5.2 - Logistic Regression\nChapter 8.1 - Partial Dependence Plots\n\n\nSupplementary Resources:\n\nTu, Y. K., Gunnell, D., & Gilthorpe, M. S. (2008). Simpson’s Paradox, Lord’s Paradox, and Suppression Effects are the same phenomenon–the reversal paradox. Emerging themes in epidemiology.\nHastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer. [Sections 10.13 & 10.14 - Partial Dependence Plots.]\npdp package homepage and documentation.\nsklearn documentation on partial dependence plots.\n\n\n\n\nAssessed Reading\n\nBerrington De Gonzalez, A and Cox, DR. (2007). Interpretation of interaction: a review. Annals of Applied Statistics, 371-385.\n\nCore Reading\n\nMolnar, C. (2020). Interpretable machine learning.\n\nChapter 8.2 - Accumulated Local Effect Plots\nChapter 8.3 - Feature Interaction\n\nGoldstein et al. (2015). Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation. Journal of Computational and Graphical Statistics, 24:1, 44-65.\n\nSupplementary Reading\n\nMolnar, C. (2020). Interpretable machine learning.\n\nChapter 8.5 - Permuation Feature Importance\nChapter 9.5 & 9.6 - Shapley values and SHAP\n\n\n\n\n\nAssessed Reading\n\nChesnaye, N. C., et. al. (2022). An introduction to inverse probability of treatment weighting in observational research. Clinical Kidney Journal, 15(1), 14-20.\n\nCore Reading\n\nHuntington-Klein, N.C. (2022) The Effect: An Introduction to Research Design and Causality.\n\nChapter 6 - Causal Diagrams.\nChapter 7 - Drawing Causal Diagrams.\nChapter 8 - Causal Paths and Closing Back Doors.\n\n\nSupplementary Reading\n\nCunninham, S. (2021) Causal Inference: The Mixtape.\n\nChapter 9 - Difference-in-Differences.\n\n\n\n\n\nAssessed Reading\n\nKohavi, Tang, D., & Xu, Y. (2020). Trustworthy online controlled experiments: a practical guide to A/B testing. Cambridge University Press. [Chapter 9 - Ethics of experimental studies in a business setting]\n\nCore Reading\n\nKendall J. M. (2003). Designing a research project: randomised controlled trials and their principles. Emergency medicine journal : EMJ, 20(2), 164–168.\nKohavi, R., Tang, D., Xu, Y., Hemkens, L. G., & Ioannidis, J. (2020). Online randomized controlled experiments at scale: lessons and extensions to medicine. Trials, 21(1), 1-9.\n\nSupplementary Reading\n\nKirkwood, B. R., & Sterne, J. A. (2010). Essential medical statistics. John Wiley & Sons. [Part F - Chapter 34, particularly §34.2]\nKohavi, Tang, D., & Xu, Y. (2020). Trustworthy online controlled experiments: a practical guide to A/B testing. Cambridge University Press. [Chapter 17 - Statistics of Online Controlled Experiments]\nHahn S. (2012). Understanding noninferiority trials. Korean journal of pediatrics, 55(11), 403–407.\n\n\n\n\nAssessed Reading\n\nVan der Bles, A. M., et al. (2019). Communicating uncertainty about facts, numbers and science. Royal Society open science.\n\nCore Reading\n\nSchünemann, H. J., et al. (2003). Letters, numbers, symbols and words: how to communicate grades of evidence and recommendations. Canadian Medical Association Journal.\n\nSupplementary Reading\n\nKearns, M. & Roth, A. (2020). The ethical algorithm: The science of socially aware algorithm design. Oxford University Press. (Chapter 4)\n\n\n\n\nAssessed Reading\n\nIn this exercise you may write a rhetorical precis for any of the supplementary readings from weeks 1 - 5.\n\n\n\n\n\n\n\n\n\n\n\nAssessed Reading\n\nSculley, D. et al. (2015). Hidden Technical Debt in Machine Learning Systems. Advances in neural information processing systems, 28.\n\nSupplementary Reading\n\nVideo - Machine Learning, Techincal Debt, and You.\n\n\n\n\nAssessed Reading\n\nSrivastava, N. et al. (2014). Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research.\n\nSupplementary Reading\n\nHardt, M., Recht, B. and Singer, Y. (2016). “Train faster, generalize better: Stability of stochastic gradient descent. Proceedings of Machine Learning Research.\n\n\n\n\nAssessed Reading\n\nChen et al. (2018). Logistic regression over encrypted data from fully homomorphic encryption. BMC Medical Genomics, 11:81.\n\nSupplementary Reading\n\nGraepel, T., Lauter, K. and Naehrig, M. (2012). ML Confidential: Machine Learning on Encrypted Data. International Conference Information Security and Cryptography, 15. [pdf]\nIezzi, M. (2020). Practical Privacy-Preserving Data Science With Homomorphic Encryption: An Overview. ArXiv Preprint.\n\n\n\n\nAssessed Reading\n\nLi et al. (2020). Federated Learning: Challenges, Methods, and Future Directions. IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50-60.\n\nCore Reading\n\nMcMahan et al. (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, PMLR 54:1273-1282. [pdf]\n\nSupplementary Reading\n\nIBM research blog. What is Federated Learning?\n\n\n\n\nAssessed Reading\n\nRigaki, M. & Garcia, S. (2020). “A Survey of Privacy Attacks in Machine Learning”. ArXiv preprint.\n\n\n\n\nAssessed Reading\n\nIn this exercise you may write a rhetorical precis for any of the supplementary readings from weeks 1 - 5."
  },
  {
    "objectID": "ethics-reading-list.html#part-1",
    "href": "ethics-reading-list.html#part-1",
    "title": "Ethics Reading List",
    "section": "",
    "text": "Assessed Reading\n\nFloridi, L., & Cowls, J. (2019). A Unified Framework of Five Principles for AI in Society. Harvard Data Science Review, 1(1).\n\nCore Reading\n\nKearns, M. & Roth, A. (2020). The ethical algorithm: the science of socially aware algorithm design. Oxford University Press. [Introduction chapter]\n\nLive Session Documents\n\nEuropean Union Guidelines\nRoyal Statistical Society Guidelines\nDutch Government Guidelines\nUK Government Guidelines\nAmerican Mathematical Society Guidelines\n\n\n\n\nAssessed Reading\n\nNarayanan, & Shmatikov, V. (2008). Robust De-anonymization of Large Sparse Datasets. 2008 IEEE Symposium on Security and Privacy (sp 2008), 111–125.\n\nCore Reading\n\nKearns, M. & Roth, A. (2020). The ethical algorithm: the science of socially aware algorithm design. Oxford University Press. [Chapter 1 - Algorithmic Privacy]\n\n\n\n\nAssessed Reading\n\nBuolamwini, J., & Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency (pp. 77-91). PMLR.\n\nCore Reading\n\nKearns, M. & Roth, A. (2020). The ethical algorithm: the science of socially aware algorithm design. Oxford University Press. [Chapter 2 - Algorithmic Fairness]\n\nSupplementary Reading\n\nVerma, S., & Rubin, J. (2018). Fairness definitions explained. In 2018 ieee/acm international workshop on software fairness (fairware) (pp. 1-7). IEEE.\n\n\n\n\nAssessed Reading\n\nAwad, E., et al. (2018). The moral machine experiment. Nature 563, 59-64.\n\nCore Reading\n\nGunantara, N. (2018). A review of multi-objective optimization: methods and its applications. Cogent Engineering 5:1.\n\n\n\n\nAssessed Reading\n\nRibeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why should I trust you?” Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD, International Conference on Knowledge Discovery and Data Mining (pp. 1135-1144).\n\nCore Reading\n\nMolnar, C. (2020). Interpretable machine learning.\n\nChapter 8.1 on Partial Dependence Plots\nChapter 9.2 on LIME\n\n\nSupplementary Reading\n\nRecording of LIME conference presentation (in supplementary material section)\n\n\n\n\nAssessed Reading\n\nAmeisen. (2020). Building machine learning powered applications: going from idea to product (First edition). O’Reilly. [Chapter 11 - Monitor and Update Models]\n\nCore Reading\n\nWilson, G., Bryan, J., Cranston, K., Kitzes, J., Nederbragt, L., & Teal, T. K. (2017). Good enough practices in scientific computing. PLoS computational biology, 13(6)."
  },
  {
    "objectID": "ethics-reading-list.html#part-2",
    "href": "ethics-reading-list.html#part-2",
    "title": "Ethics Reading List",
    "section": "",
    "text": "Assessed Reading\n\nGuthery, F. & Bingham, R. (2007). A Primer on Interpreting Regression Models. Journal of Wildlife Management 71(3):684–692.\n\nCore Reading\n\nMolnar, C. (2020). Interpretable machine learning.\n\nChapter 5.1 - Linear Regression\nChapter 5.2 - Logistic Regression\nChapter 8.1 - Partial Dependence Plots\n\n\nSupplementary Resources:\n\nTu, Y. K., Gunnell, D., & Gilthorpe, M. S. (2008). Simpson’s Paradox, Lord’s Paradox, and Suppression Effects are the same phenomenon–the reversal paradox. Emerging themes in epidemiology.\nHastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer. [Sections 10.13 & 10.14 - Partial Dependence Plots.]\npdp package homepage and documentation.\nsklearn documentation on partial dependence plots.\n\n\n\n\nAssessed Reading\n\nBerrington De Gonzalez, A and Cox, DR. (2007). Interpretation of interaction: a review. Annals of Applied Statistics, 371-385.\n\nCore Reading\n\nMolnar, C. (2020). Interpretable machine learning.\n\nChapter 8.2 - Accumulated Local Effect Plots\nChapter 8.3 - Feature Interaction\n\nGoldstein et al. (2015). Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation. Journal of Computational and Graphical Statistics, 24:1, 44-65.\n\nSupplementary Reading\n\nMolnar, C. (2020). Interpretable machine learning.\n\nChapter 8.5 - Permuation Feature Importance\nChapter 9.5 & 9.6 - Shapley values and SHAP\n\n\n\n\n\nAssessed Reading\n\nChesnaye, N. C., et. al. (2022). An introduction to inverse probability of treatment weighting in observational research. Clinical Kidney Journal, 15(1), 14-20.\n\nCore Reading\n\nHuntington-Klein, N.C. (2022) The Effect: An Introduction to Research Design and Causality.\n\nChapter 6 - Causal Diagrams.\nChapter 7 - Drawing Causal Diagrams.\nChapter 8 - Causal Paths and Closing Back Doors.\n\n\nSupplementary Reading\n\nCunninham, S. (2021) Causal Inference: The Mixtape.\n\nChapter 9 - Difference-in-Differences.\n\n\n\n\n\nAssessed Reading\n\nKohavi, Tang, D., & Xu, Y. (2020). Trustworthy online controlled experiments: a practical guide to A/B testing. Cambridge University Press. [Chapter 9 - Ethics of experimental studies in a business setting]\n\nCore Reading\n\nKendall J. M. (2003). Designing a research project: randomised controlled trials and their principles. Emergency medicine journal : EMJ, 20(2), 164–168.\nKohavi, R., Tang, D., Xu, Y., Hemkens, L. G., & Ioannidis, J. (2020). Online randomized controlled experiments at scale: lessons and extensions to medicine. Trials, 21(1), 1-9.\n\nSupplementary Reading\n\nKirkwood, B. R., & Sterne, J. A. (2010). Essential medical statistics. John Wiley & Sons. [Part F - Chapter 34, particularly §34.2]\nKohavi, Tang, D., & Xu, Y. (2020). Trustworthy online controlled experiments: a practical guide to A/B testing. Cambridge University Press. [Chapter 17 - Statistics of Online Controlled Experiments]\nHahn S. (2012). Understanding noninferiority trials. Korean journal of pediatrics, 55(11), 403–407.\n\n\n\n\nAssessed Reading\n\nVan der Bles, A. M., et al. (2019). Communicating uncertainty about facts, numbers and science. Royal Society open science.\n\nCore Reading\n\nSchünemann, H. J., et al. (2003). Letters, numbers, symbols and words: how to communicate grades of evidence and recommendations. Canadian Medical Association Journal.\n\nSupplementary Reading\n\nKearns, M. & Roth, A. (2020). The ethical algorithm: The science of socially aware algorithm design. Oxford University Press. (Chapter 4)\n\n\n\n\nAssessed Reading\n\nIn this exercise you may write a rhetorical precis for any of the supplementary readings from weeks 1 - 5."
  },
  {
    "objectID": "ethics-reading-list.html#part-3",
    "href": "ethics-reading-list.html#part-3",
    "title": "Ethics Reading List",
    "section": "",
    "text": "Assessed Reading\n\nSculley, D. et al. (2015). Hidden Technical Debt in Machine Learning Systems. Advances in neural information processing systems, 28.\n\nSupplementary Reading\n\nVideo - Machine Learning, Techincal Debt, and You.\n\n\n\n\nAssessed Reading\n\nSrivastava, N. et al. (2014). Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research.\n\nSupplementary Reading\n\nHardt, M., Recht, B. and Singer, Y. (2016). “Train faster, generalize better: Stability of stochastic gradient descent. Proceedings of Machine Learning Research.\n\n\n\n\nAssessed Reading\n\nChen et al. (2018). Logistic regression over encrypted data from fully homomorphic encryption. BMC Medical Genomics, 11:81.\n\nSupplementary Reading\n\nGraepel, T., Lauter, K. and Naehrig, M. (2012). ML Confidential: Machine Learning on Encrypted Data. International Conference Information Security and Cryptography, 15. [pdf]\nIezzi, M. (2020). Practical Privacy-Preserving Data Science With Homomorphic Encryption: An Overview. ArXiv Preprint.\n\n\n\n\nAssessed Reading\n\nLi et al. (2020). Federated Learning: Challenges, Methods, and Future Directions. IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50-60.\n\nCore Reading\n\nMcMahan et al. (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, PMLR 54:1273-1282. [pdf]\n\nSupplementary Reading\n\nIBM research blog. What is Federated Learning?\n\n\n\n\nAssessed Reading\n\nRigaki, M. & Garcia, S. (2020). “A Survey of Privacy Attacks in Machine Learning”. ArXiv preprint.\n\n\n\n\nAssessed Reading\n\nIn this exercise you may write a rhetorical precis for any of the supplementary readings from weeks 1 - 5."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "No upcoming talks currently scheduled.\nHave a speaking opportunity that you think I’d be great for? Let me know about it!\n\n\n\n\n\n\n\n\n\n\n\n\n\nShaking Things Up: Statistical Modelling of Earthquakes\n\n\n\n\n\nThursday 16 November 2023 (15:15 - 16:15) Register\nSlides:\nHTML   PDF   Source \nAbstract:\nEarthquakes are among the most unpredictable of natural disasters and can have a profound impact on both human society and the built environment. Understanding the processes underlying seismic events is crucial to being able to map and mitigate against the hazards that they present.\nIn this talk, we’ll start by introducing some basic seismology principles and the data that can be collected through monitoring networks. We’ll then investigate some of the statistical models commonly used to describe earthquake behaviour, from simple point processes to more complex branching process representations and extreme value statistics. Along the way, we’ll discuss the challenges of applying these models to real earthquake data and highlight the growing role of machine learning in earthquake research.\nWe’ll explore recent approaches that address the limitations and uncertainties in earthquake modelling, so that by the end of this talk you’ll have both a solid understanding of the fundamental models in statistical seismology and an insight into their potential future developments.\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Your Work to Work in Academia (and Beyond)\n\n\n\n\n\nSlides:  HTML   PDF   Source \nAbstract:\nYou can’t solve a hard problem all in one go. One example of this is the ongoing replication crisis faced by the academic world, where published results can’t be verified by independent repetition of the original study. So, what do we do when we can’t solve a hard problem straight away? We find a sub-problem that we can solve and use that as a steppingstone towards our final goal. In the case of the replication crisis, we can narrow our scope to reproducibility: given the same experimental or observational data can the original results be recovered?\nThis talk will focus on the ways that reproducibility interacts with three aspects of academic (and non-academic) work: research, teaching, and the training our future colleagues. I’ll share my experiences of moving toward a more reproducible way of working and argue that the individual and collective benefits of working reproducibility more than justify the extra effort this requires.\n\n\n\n\n\n\n\n\n\n\n\n\nGo Figure!\n\n\n\n\n\nSlides:  HTML   PDF   Source \nAbstract:\nAs the saying goes, a picture is worth a thousand words. If that’s true then the importance of high quality figures should not be underestimated. In this session, we’ll explore how you can effectively support and explain your research using visualisations.\nWe’ll begin with some design principles to help your figures to clearly convey your intended meaning. We’ll then develop strategies to adapt your figures for different audiences and introduce tools to make them accessible to a broader audience. To put your new-found knowledge to the test, we’ll conclude the session with a game of broken picture telephone (including prizes for the winners!).\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations in Data Science\n\n\n\n\n\nAbstract:\nData ethics is a rapidly developing area within data science, covering a wide range of topics and every stage in the life cycle of a data science project. It aims to answer questions such as:\n\nWhat data are we reasonably allowed to gather, for how long can we keep it and how do we store it securely?\nFor personal information, which attributes should we obscuring or avoiding entirely in our modelling process?\nHow can we evaluate the models that we do fit to ensure that they protect the privacy of the individuals within the training data? How can we quantify the fairness of the decisions and predictions that those models make, and can we test whether are fair?\nOnce models are in production and impacting people’s lives, how do we ensure those models remain relevant and how do we prevent our models from perpetuating historical biases?\n\nWe will take loan applications as an example to explore some of the challenges you might face when you have to answer these questions about your own work.\n\n\n\n\n\n\n\n\n\n\n\n\nPeer-graded assessments as part of an online degree\n\n\n\n\n\nSlides   Video \nAbstract:\nWho actually does the required reading before class? Honestly, very few people. Solving this problem can be relatively straightforward if we appropriately reward students for doing that reading. The best reward will differ between students: for one it might be course credit, for another detailed written feedback or for another it might be feeling well prepare to participate in class discussions.\nIn this short talk, I will discuss how introducing peer-marked reading summaries improved the teaching and learning experiences in an online course on ethical data science. I’ll give some context about the course and describe how we introduced the peer-marked assignments as well as the impact this has had on teaching and learning (both positive and negative).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract\n\n\n\n\n\n\nSlides \nAbstract:\nData-driven decision making is now pervasive and impacts us all. Your data is used by others to make decisions about who you are, how you will behave, and what options should be made available to you. Predictive models are used to decide anything from the promotion that is offered to you by a retailer through to whether your loan application is granted by a bank.\nThe ways in which these predictive models can fail mathematically form a core part of the training for an aspiring statistician or data scientist. In contrast, the potential for ethical failures in these same models is rarely covered in-depth during as part of this initial training. As a result, these modes of failure are often not considered until those predictive models have been put into production and are actively causing harm. We argue that to prevent this harm, the ethical impacts of using data to make decisions must be made core to the curriculum of both statistics and data science.\nThis talk will describe how this may be done in a way that is appealing to an audience with a strong mathematical focus and that does not require the authoring of extended essays or moral treaties. The discussion is structured around the development of a post-graduate course in the Ethics of Data Science, but the core ideas are salient to all statistics training. Throughout, we give actionable ways in which these topics may be integrated into statistical training at all levels.\n\n\n\n\nOlder talks will be added here as time allows."
  },
  {
    "objectID": "talks.html#upcoming",
    "href": "talks.html#upcoming",
    "title": "Talks",
    "section": "",
    "text": "No upcoming talks currently scheduled.\nHave a speaking opportunity that you think I’d be great for? Let me know about it!"
  },
  {
    "objectID": "talks.html#archive",
    "href": "talks.html#archive",
    "title": "Talks",
    "section": "",
    "text": "Shaking Things Up: Statistical Modelling of Earthquakes\n\n\n\n\n\nThursday 16 November 2023 (15:15 - 16:15) Register\nSlides:\nHTML   PDF   Source \nAbstract:\nEarthquakes are among the most unpredictable of natural disasters and can have a profound impact on both human society and the built environment. Understanding the processes underlying seismic events is crucial to being able to map and mitigate against the hazards that they present.\nIn this talk, we’ll start by introducing some basic seismology principles and the data that can be collected through monitoring networks. We’ll then investigate some of the statistical models commonly used to describe earthquake behaviour, from simple point processes to more complex branching process representations and extreme value statistics. Along the way, we’ll discuss the challenges of applying these models to real earthquake data and highlight the growing role of machine learning in earthquake research.\nWe’ll explore recent approaches that address the limitations and uncertainties in earthquake modelling, so that by the end of this talk you’ll have both a solid understanding of the fundamental models in statistical seismology and an insight into their potential future developments.\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Your Work to Work in Academia (and Beyond)\n\n\n\n\n\nSlides:  HTML   PDF   Source \nAbstract:\nYou can’t solve a hard problem all in one go. One example of this is the ongoing replication crisis faced by the academic world, where published results can’t be verified by independent repetition of the original study. So, what do we do when we can’t solve a hard problem straight away? We find a sub-problem that we can solve and use that as a steppingstone towards our final goal. In the case of the replication crisis, we can narrow our scope to reproducibility: given the same experimental or observational data can the original results be recovered?\nThis talk will focus on the ways that reproducibility interacts with three aspects of academic (and non-academic) work: research, teaching, and the training our future colleagues. I’ll share my experiences of moving toward a more reproducible way of working and argue that the individual and collective benefits of working reproducibility more than justify the extra effort this requires.\n\n\n\n\n\n\n\n\n\n\n\n\nGo Figure!\n\n\n\n\n\nSlides:  HTML   PDF   Source \nAbstract:\nAs the saying goes, a picture is worth a thousand words. If that’s true then the importance of high quality figures should not be underestimated. In this session, we’ll explore how you can effectively support and explain your research using visualisations.\nWe’ll begin with some design principles to help your figures to clearly convey your intended meaning. We’ll then develop strategies to adapt your figures for different audiences and introduce tools to make them accessible to a broader audience. To put your new-found knowledge to the test, we’ll conclude the session with a game of broken picture telephone (including prizes for the winners!).\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations in Data Science\n\n\n\n\n\nAbstract:\nData ethics is a rapidly developing area within data science, covering a wide range of topics and every stage in the life cycle of a data science project. It aims to answer questions such as:\n\nWhat data are we reasonably allowed to gather, for how long can we keep it and how do we store it securely?\nFor personal information, which attributes should we obscuring or avoiding entirely in our modelling process?\nHow can we evaluate the models that we do fit to ensure that they protect the privacy of the individuals within the training data? How can we quantify the fairness of the decisions and predictions that those models make, and can we test whether are fair?\nOnce models are in production and impacting people’s lives, how do we ensure those models remain relevant and how do we prevent our models from perpetuating historical biases?\n\nWe will take loan applications as an example to explore some of the challenges you might face when you have to answer these questions about your own work.\n\n\n\n\n\n\n\n\n\n\n\n\nPeer-graded assessments as part of an online degree\n\n\n\n\n\nSlides   Video \nAbstract:\nWho actually does the required reading before class? Honestly, very few people. Solving this problem can be relatively straightforward if we appropriately reward students for doing that reading. The best reward will differ between students: for one it might be course credit, for another detailed written feedback or for another it might be feeling well prepare to participate in class discussions.\nIn this short talk, I will discuss how introducing peer-marked reading summaries improved the teaching and learning experiences in an online course on ethical data science. I’ll give some context about the course and describe how we introduced the peer-marked assignments as well as the impact this has had on teaching and learning (both positive and negative).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract\n\n\n\n\n\n\nSlides \nAbstract:\nData-driven decision making is now pervasive and impacts us all. Your data is used by others to make decisions about who you are, how you will behave, and what options should be made available to you. Predictive models are used to decide anything from the promotion that is offered to you by a retailer through to whether your loan application is granted by a bank.\nThe ways in which these predictive models can fail mathematically form a core part of the training for an aspiring statistician or data scientist. In contrast, the potential for ethical failures in these same models is rarely covered in-depth during as part of this initial training. As a result, these modes of failure are often not considered until those predictive models have been put into production and are actively causing harm. We argue that to prevent this harm, the ethical impacts of using data to make decisions must be made core to the curriculum of both statistics and data science.\nThis talk will describe how this may be done in a way that is appealing to an audience with a strong mathematical focus and that does not require the authoring of extended essays or moral treaties. The discussion is structured around the development of a post-graduate course in the Ethics of Data Science, but the core ideas are salient to all statistics training. Throughout, we give actionable ways in which these topics may be integrated into statistical training at all levels.\n\n\n\n\nOlder talks will be added here as time allows."
  },
  {
    "objectID": "talks.html#incorporating-ethics-into-the-data-science-curriculum",
    "href": "talks.html#incorporating-ethics-into-the-data-science-curriculum",
    "title": "Talks",
    "section": "",
    "text": "Slides \nAbstract:\nData-driven decision making is now pervasive and impacts us all. Your data is used by others to make decisions about who you are, how you will behave, and what options should be made available to you. Predictive models are used to decide anything from the promotion that is offered to you by a retailer through to whether your loan application is granted by a bank.\nThe ways in which these predictive models can fail mathematically form a core part of the training for an aspiring statistician or data scientist. In contrast, the potential for ethical failures in these same models is rarely covered in-depth during as part of this initial training. As a result, these modes of failure are often not considered until those predictive models have been put into production and are actively causing harm. We argue that to prevent this harm, the ethical impacts of using data to make decisions must be made core to the curriculum of both statistics and data science.\nThis talk will describe how this may be done in a way that is appealing to an audience with a strong mathematical focus and that does not require the authoring of extended essays or moral treaties. The discussion is structured around the development of a post-graduate course in the Ethics of Data Science, but the core ideas are salient to all statistics training. Throughout, we give actionable ways in which these topics may be integrated into statistical training at all levels."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zak Varty",
    "section": "",
    "text": "I am a statistician, data scientist and educator.\nI work as a Teaching Fellow at Imperial College London where I teach courses and supervise research projects in statistics, data science and data ethics.\nMy research interests are at the intersection of applied and methodological statistics. I particularly enjoy developing efficient inference methods for non-standard data generating mechanisms and data science projects with environmental or industrial applications.\n\nWhen I’m not in front of my computer, I enjoy distance running and I love to read 🏃 📖. Ideally, these activities are accompanied by a good cup of coffee and a large slice of cake!"
  },
  {
    "objectID": "blog/2023-09-13-bib-but-better/index.html",
    "href": "blog/2023-09-13-bib-but-better/index.html",
    "title": "Bib but Better",
    "section": "",
    "text": "Bib files and BibTeX are awesome tools for managing your references when writing in LaTeX or Quarto. The problem is, I can never remember the all the required fields in a bibtex entry for anything more complicated than an article.\nHonestly, even for an article I struggle to list these in a consistent order. I’m saying “no more!” to this nonsense; I will have beautiful bib files if it kills me. To make that happen, I’ve put together this list of just about any entry I might ever need and listed them as easily copied code snippets.\nI know that reference managers exist to hide some of this under the rug of a nice GUI, but I have trust issues. I can’t convince myself that whichever one I pick won’t die or move to a subscription model in the medium-term future. Bib files are pain text so they have the benefit of longevity, portability and working wonderfully with version control.\nThis will definitely be helpful for future me, and I hope it is useful for you too."
  },
  {
    "objectID": "blog/2023-09-13-bib-but-better/index.html#papers",
    "href": "blog/2023-09-13-bib-but-better/index.html#papers",
    "title": "Bib but Better",
    "section": "Papers",
    "text": "Papers\n\n@article\nAn article entry is the bread and butter of academic referencing, used for published journal articles.\nRequired fields: author, title, journal, year.\n@article{chavez2005estimating,\n    author = {Valerie Chavez-Demoulin and Anthony C Davison and Alexander J McNeil},\n    title = {Estimating value-at-risk: a point process approach},\n    journal = {Quantitative Finance},\n    year = {2005},\n    % optional below here\n    volume = {5},\n    number = {2},       \n    pages = {227--234},\n    publisher = {Taylor \\& Francis}\n}\n\nNote: BibTeX accepts both forename surname and surname, forename formats for names. I find the former more readable but this does cause issues with multi-word surnames. Chevez-Demoulin is handled correctly because it is hypenated. However, John {von Neumann} or Amy {Berrington de Gonzalez} require additional braces that clearly identify which words belong to the surname, as opposed to being middle names or initials.\n\n\n\n@inproceedings\nFor a paper that has been published in conference proceedings, rather than in a journal, use @inproceedings.\n@inproceedings{morgan2019spline,\n  author = {Lucy E Morgan and Barry L Nelson and Andrew C Titman and David J Worthington},\n  title  = {A spline-based method for modelling and generating a nonhomogeneous {P}oisson process},\n  booktitle = {2019 {W}inter simulation conference},\n  year  = {2019},\n  pages = {356--367},\n  doi = {10.1109/WSC40007.2019.9004867}\n}"
  },
  {
    "objectID": "blog/2023-09-13-bib-but-better/index.html#books",
    "href": "blog/2023-09-13-bib-but-better/index.html#books",
    "title": "Bib but Better",
    "section": "Books",
    "text": "Books\n\n@book\nThe other staple of an academic diet are books.\nRequired Fields: author, title, publisher, year.\nA stand-alone book, where you want to reference the entire work gets a @book entry.\n@book{james2021introduction,\n    author = {Gareth James and Daniela Witten and Trevor Hastie and Robert Tibshirani},\n    title = {An introduction to statistical learning},\n    publisher = {Springer},\n    year = {2021},\n    % optional below here\n    subtitle = {with applications in {R}},\n    edition = {2}\n}\n\n\n@inbook\nIf you just want to reference a particular section, chapter or page of a book, then using an @inbook entry with the additional chapter and/or pages arguments is more appropriate.\n@inbook{james2021resampling,\n    author = {Gareth James and Daniela Witten and Trevor Hastie and Robert Tibshirani},\n    title = {An introduction to statistical learning},\n    chapter = {Resampling methods},\n    publisher = {Springer},\n    year = {2021},\n    % optional below here\n    subtitle = {with applications in {R}},\n    edition = {2}\n    pages = {197--223}\n}\n\n\n@incollection\nA collection is like the academic version of a book of short stories. All chapters focus on the same over-arching theme but each chapter is a titled, stand-alone paper where each paper will typically have different authors . To cite a single paper/chapter from within such a collection, use an @incollection entry.\nRequired fields: author, title, publisher, year, booktitle, editor\n(Note: editor not required if also the author)\n@incollection{isham2010spatial,\n    author = {Valerie Isham},\n    title = {Spatial point process models},\n    publisher = {CRC Press},\n    year = {2010},\n    booktitle = {Handbook of spatial statistics},\n    editor = {Alan E Gelfand and Peter J Diggle and Montserrat Fuentes and Peter Guttorp},\n    pages = {283--298}\n}\n\n\n@series\nFinally, if the book is part of a larger series then this can be acknowledged too. One nice example of these is the SUMS collection by Springer.\n@book{capinski2004measure,\n    author = {Marek Capi{\\'n}ski and Ekkehard Kopp},\n    title = {Measure, integral and probability},\n    publisher = {Springer},\n    year = {2004},\n    series = {Springer undergraduate mathematics series},\n    volume = {14},\n    edition = {2}\n}"
  },
  {
    "objectID": "blog/2023-09-13-bib-but-better/index.html#reports",
    "href": "blog/2023-09-13-bib-but-better/index.html#reports",
    "title": "Bib but Better",
    "section": "Reports",
    "text": "Reports\n\n@techreport\nPublished documents appear in many other places besides academic journals. These might be technical reports written buy businesses, official guidelines written by governing bodies or annual reports of charities. All of these documents are best suited to a techreport entry.\nRequired fields: author, title, publisher, year.\n@techreport{nam2016addendum,\n    author = {NAM},\n    title = {Technical addendum to the {W}inningsplan {G}rongingen 2016},\n    institution = {Nederlandse Aardolie Maatschappij},\n    year = {2016},\n    % optional below here\n    subtitle = {Summary and production},\n    volume = {1}\n}\n\n\n@phdthesis\nThesis and dissertation documents can be valuable sources of information that never makes it into another form of publication, whether that is due to page limits or the researcher graduating and having better things to do with their time. But that is no excuse not to give proper attribution to their work, which can be done using a phdthesis or mastersthesis entry.\n\nNote the “s”: it is a “masters thesis” not a “master thesis”.\n\nRequired fields: author, title, publisher, year.\n@phdthesis{varty2021statistical,\n    author = {Zak Varty},\n    title = {Statistical Modelling of Induced Earthquakes},\n    publisher = {Lancaster University},\n    year = {2021},\n    % optional below here\n    address = {United Kingdom},\n    doi = {10.17635/lancaster/thesis/1436}\n}\n@mastersthesis{varty2016computer,\n    author = {Zak Varty},\n    title = {Computer intensive methods for modelling household epidemics},\n    publisher = {Lancaster University},\n    year = {2016},\n    % optional below here\n    address = {United Kingdom},\n}"
  },
  {
    "objectID": "blog/2023-09-13-bib-but-better/index.html#other-entries",
    "href": "blog/2023-09-13-bib-but-better/index.html#other-entries",
    "title": "Bib but Better",
    "section": "Other Entries",
    "text": "Other Entries\n\n@misc\nWebsites and lecture notes tend to be secondary sources of information and we should seek out the primary sources where possible. Sometimes though, these are the best we can do. I those cases we should still reference them using the highly flexible misc entry.\n(misc is a contraction of miscellaneous, which is difficult to spell correctly.)\nRequired fields: none.\n@misc{nam2017infosite,\n    author = {NAM},\n    title = {English information site},\n    institution = {Nederlandse Aardolie Maatschappij},\n    year = {2017},\n    howpublished = {Available at \\url{https://www.nam.nl/english-information.html}. (Accessed: 2017-07-26.)},\n}\n\nNote: For websites and html lecture notes, if you are using BibLaTeX a similar @online entry can also be used.\n@online{nam2017infosite,\n author = {NAM},\n title = {English information site},\n year = {2017},\n url = {https://www.nam.nl/english-information.html},\n urldate = {2017-07-26}\n}\n\n@misc{lee2021extreme,\n    author = {Clement Lee and Zak Varty},\n    title = {Extreme Value Theory},\n    organization = {Lancaster University},\n    year = {2021},\n    howpublished = {Lecutre notes, 2020/21}\n}\nA primary source that also uses the misc entry is personal communications, whether that is in person conversation, a recorded interview or an online exchange.\n@misc{varty2023email,\n    author = {Zak Varty},\n    month = {September},\n    year = {2023},\n    howpublished = {Personal communication}\n}\n\n\n@manual\nThe documentation and manuals that come with software and hardware can be valuable sources of information.\nCiting the open source projects (languages, packages and software) that you use is a great way of showing your support for these projects. This allows the developers to demonstrate the impact their work is having and to justify continued work on that project. Of course, contributing your own time / money is a better form of support, but realistically people are miserly by nature and don’t like to pay for things.\nIn R, you can get citations for most packages using citation(packagename).\n\ncitation(\"PrettyCols\")\n\nTo cite package 'PrettyCols' in publications use:\n\n  Rennie N (2023). _PrettyCols: Pretty Colour Palettes_. R package\n  version 1.0.1, &lt;https://CRAN.R-project.org/package=PrettyCols&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {PrettyCols: Pretty Colour Palettes},\n    author = {Nicola Rennie},\n    year = {2023},\n    note = {R package version 1.0.1},\n    url = {https://CRAN.R-project.org/package=PrettyCols},\n  }\n\n\n@manual{r2023language,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2023},\n    url = {https://www.R-project.org/},\n  }\n\n\n@unpublished\nYou can also reference unpublished work, by replacing the publisher field with a note.\n\nfor work that has not be submitted to a journal for review this should read Unpublished manuscript.\nFor work that has been submitted but not yet accepted, this should read Manuscript submitted for publication.\nFor work that has been accepted but not yet published, this should read (in press) and the journal field should be included too.\n\n@unpublished{varty2023working,\n    author = {Zak Varty},\n    title = {Working title of paper},\n    note = {Unpublished manuscript}, \n    year = {2023}\n}"
  },
  {
    "objectID": "blog/2023-09-13-bib-but-better/index.html#bibtex-tips-and-tricks",
    "href": "blog/2023-09-13-bib-but-better/index.html#bibtex-tips-and-tricks",
    "title": "Bib but Better",
    "section": "BibTeX Tips and Tricks",
    "text": "BibTeX Tips and Tricks\n\nAll author names are separated by and, not just the last one.\nSpecial characters must be “escaped” using a backslash, for example an ampersand \\&.\nUse additional curly braces to ensure that capitalisation of proper nouns and acronyms are maintained. \nFor specific advice on bibliography management, see the corresponding section of the LaTeX wiki or the Quarto docs on citations and footnotes."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html",
    "href": "blog/2022-12-01-rvest/index.html",
    "title": "Web Scraping with {rvest}",
    "section": "",
    "text": "You can’t always rely on tidy, tabular data to land on your desk. Sometimes you are going to have to go out and gather data for yourself.\nI’m not suggesting you will need to do this manually, but you will likely need to get data from the internet that’s been made publicly or privately available to you.\nThis might be information from a webpage that you gather yourself, or data shared with you by a collaborator using an API.\nIn this first blog post we will cover the basics of scraping webpages, following the vignette for the {rvest} package."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#aquiring-data-part-1---scraping-webpage-data-using-rvest",
    "href": "blog/2022-12-01-rvest/index.html#aquiring-data-part-1---scraping-webpage-data-using-rvest",
    "title": "Web Scraping with {rvest}",
    "section": "",
    "text": "You can’t always rely on tidy, tabular data to land on your desk. Sometimes you are going to have to go out and gather data for yourself.\nI’m not suggesting you will need to do this manually, but you will likely need to get data from the internet that’s been made publicly or privately available to you.\nThis might be information from a webpage that you gather yourself, or data shared with you by a collaborator using an API.\nIn this first blog post we will cover the basics of scraping webpages, following the vignette for the {rvest} package."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#what-is-a-webpage",
    "href": "blog/2022-12-01-rvest/index.html#what-is-a-webpage",
    "title": "Web Scraping with {rvest}",
    "section": "What is a webpage?",
    "text": "What is a webpage?\nBefore we can even hope to get data from a webpage, we first need to understand what a webpage is.\nWebpages are written in a similar way to LaTeX: the content and styling of webpages are handled separately and are coded using plain text files.\nIn fact, websites go one step further than LaTeX. The content and styling of websites are written in different files and in different languages. HTML (HyperText Markup Language) is used to write the content and then CSS (Cascading Style Sheets) are used to control the appearance of that content when it’s displayed to the user."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#html",
    "href": "blog/2022-12-01-rvest/index.html#html",
    "title": "Web Scraping with {rvest}",
    "section": "HTML",
    "text": "HTML\nA basic HTML page with no styling applied might look something like this:\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Page title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1 id='first'&gt;A level 1 heading&lt;/h1&gt;\n  &lt;p&gt;Hello World!&lt;/p&gt;\n  &lt;p&gt;Here is some plain text &amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt;\n  &lt;img src='myimg.png' width='100' height='100'&gt;\n&lt;/body&gt;\n\nHTML elements\n\nJust like XML data files, HTML has a hierarchical structure. This structure is crafted using HTML elements. Each HTML element is made up of of a start tag, optional attributes, an end tag.\nWe can see each of these in the first level header, where &lt;h1&gt; is the opening tag, id='first' is an additional attribute and &lt;/h1&gt; is the closing tag. Everything between the opening and closing tag are the contents of that element. There are also some special elements that consist of only a single tag and its optional attributes. An example of this is the &lt;img&gt; tag.\nSince &lt; and &gt; are used for start and end tags, you can’t write them directly in an HTML document. Instead, you have to use escape characters. This sounds fancy, but it’s just an alternative way to write characters that serve some special function within a language.\nYou can write greater than &gt; and less than as &lt;. You might notice that those escapes use an ampersand (&). This means that if you want a literal ampersand on your webpage, you have to escape too using &amp;.\nThere are a wide range of possible HTML tags and escapes. We’ll cover the most common tags in this lecture and you don’t need to worry about escapes too much because {rvest} will automatically handle them for you.\n\n\nImportant HTML Elements\nIn all, there are in excess of 100 HTML elements. The most important ones for you to know about are:\n\nThe &lt;html&gt; element, that must enclose every HTML page. The &lt;html&gt; element must have two child elements within it. The &lt;head&gt; element contains metadata about the document, like the page title that is shown in the browser tab and the CSS style sheet that should be applied. The &lt;body&gt; element then contains all of the content that you see in the browser.\nBlock elements are used to give structure to the page. These are elements like headings, sub-headings and so on from &lt;h1&gt; all the way down to &lt;h6&gt;. This category also contains paragraph elements &lt;p&gt;, ordered lists &lt;ol&gt; unordered lists &lt;ul&gt;.\nFinally, inline tags like &lt;b&gt; for bold, &lt;i&gt; for italics, and &lt;a&gt; for hyperlinks are used to format text inside block elements.\n\nWhen you come across a tag that you’ve never seen before, you can find out what it does with just a little bit of googling. A good resource here is the MDN Web Docs which are produced by Mozilla, the company that makes the Firefox web browser. The W3schools website is another great resource for web development and coding resources more generally."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#html-attributes",
    "href": "blog/2022-12-01-rvest/index.html#html-attributes",
    "title": "Web Scraping with {rvest}",
    "section": "HTML Attributes",
    "text": "HTML Attributes\nWe’ve seen one example of a header with an additional attribute. More generally, all tags can have named attributes. These attributes are contained within the opening tag and look something like:\n&lt;tag attribute1='value1' attribute2='value2'&gt;element contents&lt;/tag&gt;\nTwo of the most important attributes are id and class. These attributes are used in conjunction with the CSS file to control the visual appearance of the page. These are often very useful to identify the elements that you are interested in when scraping data off a page."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#css-selectors",
    "href": "blog/2022-12-01-rvest/index.html#css-selectors",
    "title": "Web Scraping with {rvest}",
    "section": "CSS Selectors",
    "text": "CSS Selectors\nThe Cascading Style Sheet is used to describe how your HTML content will be displayed. To do this, CSS has it’s own system for selecting elements of a webpage, called CSS selectors.\nCSS selectors define patterns for locating the HTML elements that a particular style should be applied to. A happy side-effect of this is that they can sometimes be very useful for scraping, because they provide a concise way of describing which elements you want to extract.\nCSS Selectors can work on the level of an element type, a class, or a tag and these can be used in a nested (or cascading) way.\n\nThe p selector will select all paragraph &lt;p&gt; elements.\nThe .title selector will select all elements with class “title”.\nThe p.special selector will select all&lt;p&gt; elements with class “special”.\nThe #title selector will select the element with the id attribute “title”.\n\nWhen you want to select a single element id attributes are particularly useful because that must be unique within a html document. Unfortunately, this is only helpful if the developer added an id attribute to the element(s) you want to scrape!\nIf you want to learn more CSS selectors I recommend starting with the fun CSS dinner tutorial to build a base of knowledge and then using the W3schools resources as a reference to explore more webpages in the wild."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#which-attributes-and-selectors-do-you-need",
    "href": "blog/2022-12-01-rvest/index.html#which-attributes-and-selectors-do-you-need",
    "title": "Web Scraping with {rvest}",
    "section": "Which Attributes and Selectors Do You Need?",
    "text": "Which Attributes and Selectors Do You Need?\nTo scrape data from a webpage, you first have to identify the tag and attribute combinations that you are interested in gathering.\nTo find your elements of interest, you have three options. These go from hardest to easiest but also from most to least robust.\n\nright click + “inspect page source” (F12)\nright click + “inspect”\nRvest Selector Gadget (very useful but fallible)\n\nInspecting the source of some familiar websites can be a useful way to get your head around these concepts. Beware though that sophisticated webpages can be quite intimidating. A good place to start is with simpler, static websites such as personal websites, rather than the dynamic webpages of online retailers or social media platforms."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#reading-html-with-rvest",
    "href": "blog/2022-12-01-rvest/index.html#reading-html-with-rvest",
    "title": "Web Scraping with {rvest}",
    "section": "Reading HTML with {rvest}",
    "text": "Reading HTML with {rvest}\nWith {rvest}, reading a html page can be as simple as loading in tabular data.\n\nhtml &lt;- rvest::read_html(\"https://www.zakvarty.com/professional/teaching.html\")\n\nThe class of the resulting object is an xml_document. This type of object is from the low-level package {xml2}, which allows you to read xml files into R.\n\nclass(html)\n\n[1] \"xml_document\" \"xml_node\"    \n\n\nWe can see that this object is split into several components: first is some metadata on the type of document we have scraped, followed by the head and then the body of that html document.\n\nhtml\n\n{html_document}\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body class=\"nav-fixed\"&gt;\\n\\n&lt;div id=\"quarto-search-results\"&gt;&lt;/div&gt;\\n  &lt;he ...\n\n\nWe have several possible approaches to extracting information from this document."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#extracting-html-elements",
    "href": "blog/2022-12-01-rvest/index.html#extracting-html-elements",
    "title": "Web Scraping with {rvest}",
    "section": "Extracting HTML elements",
    "text": "Extracting HTML elements\nIn {rvest} you can extract a single element with html_element(), or all matching elements with html_elements(). Both functions take a document object and one or more CSS selectors as inputs.\n\nlibrary(rvest)\nhtml %&gt;% html_elements(\"h1\")\n## {xml_nodeset (1)}\n## [1] &lt;h1&gt;Teaching&lt;/h1&gt;\nhtml %&gt;% html_elements(\"h2\")\n## {xml_nodeset (2)}\n## [1] &lt;h2 id=\"toc-title\"&gt;On this page&lt;/h2&gt;\n## [2] &lt;h2 class=\"anchored\" data-anchor-id=\"course-history\"&gt;Course History&lt;/h2&gt;\nhtml %&gt;% html_elements(\"p\")\n## {xml_nodeset (2)}\n## [1] &lt;p&gt;I am fortunate to have had the opportunity to teach in a variety of ro ...\n## [2] &lt;p&gt;I am an associate fellow of the Higher Education Academy, which you ca ...\n\nYou can also combine and nest these selectors. For example you might want to extract all links that are within paragraphs and all second level headers.\n\nhtml %&gt;% html_elements(\"p a,h2\")\n\n{xml_nodeset (3)}\n[1] &lt;h2 id=\"toc-title\"&gt;On this page&lt;/h2&gt;\n[2] &lt;a href=\"https://www.advance-he.ac.uk/fellowship/associate-fellowship\"&gt;he ...\n[3] &lt;h2 class=\"anchored\" data-anchor-id=\"course-history\"&gt;Course History&lt;/h2&gt;"
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#extracting-data-from-html-elements",
    "href": "blog/2022-12-01-rvest/index.html#extracting-data-from-html-elements",
    "title": "Web Scraping with {rvest}",
    "section": "Extracting Data From HTML Elements",
    "text": "Extracting Data From HTML Elements\nNow that we’ve got the elements we care about extracted from the complete document. But how do we get the data we need out of those elements?\nYou’ll usually get the data from either the contents of the HTML element or else from one of it’s attributes. If you’re really lucky, the data you need will already be formatted for you as a HTML table or list.\n\nExtracting text\nThe functions rvest::html_text() and rvest::html_text2() can be used to extract the plain text contents of an HTML element.\n\nhtml %&gt;% \n  html_elements(\"#teaching li\") %&gt;% \n  html_text2()\n\n[1] \"one-to-one tuition for high school students;\"                                   \n[2] \"running workshops and computer labs for undergraduate and postgraduate modules;\"\n[3] \"delivering short courses on scientific communication and LaTeX;\"                \n[4] \"supervising an undergraduate research project;\"                                 \n[5] \"developing and lecturing postgraduate modules in statistics and data science.\"  \n\n\nThe difference between html_text() and html_text2() is in how they handle whitespace. In HTML whitespace and line breaks have very little influence over how the code is interpreted by the computer (this is similar to R but very different from Python). html_text() will extract the text as it is in the raw html, while html_text2() will do its best to extract the text in a way that gives you something similar to what you’d see in the browser.\n\n\nExtracting Attributes\nAttributes are also used to record information that you might like to collect. For example, the destination of links are stored in the href attribute and the source of images is stored in the src attribute.\nAs an example of this, consider trying to extract the twitter link from the icon in the page footer. This is quite tricky to locate in the html source, so I used the Selector Gadget to help find the correct combination of elements.\n\nhtml %&gt;% html_element(\".compact:nth-child(1) .nav-link\")\n\n{html_node}\n&lt;a class=\"nav-link\" href=\"https://www.twitter.com/zakvarty\"&gt;\n[1] &lt;i class=\"bi bi-twitter\" role=\"img\"&gt;\\n&lt;/i&gt;\n\n\nTo extract the href attribute from the scraped element, we use the rvest::html_attr() function.\n\nhtml %&gt;% \n  html_elements(\".compact:nth-child(1) .nav-link\") %&gt;% \n  html_attr(\"href\")\n\n[1] \"https://www.twitter.com/zakvarty\"\n\n\nNote: rvest::html_attr() will always return a character string (or list of character strings). If you are extracting an attribute that describes a quantity, such as the width of an image, you’ll need to convert this from a string to your required data type. For example, of the width is measures in pixels you might use as.integer().\n\n\nExtracting tables\nHTML tables are composed in a similar, nested manner to LaTeX tables.\nThere are four main elements to know about that make up an HTML table:\n\n&lt;table&gt;,\n&lt;tr&gt; (table row),\n&lt;th&gt; (table heading),\n&lt;td&gt; (table data).\n\nHere’s our simple example data, formatted as an HTML table:\n\nhtml_2 &lt;- minimal_html(\"\n  &lt;table&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Name&lt;/th&gt;\n      &lt;th&gt;Number&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;A&lt;/td&gt;\n      &lt;td&gt;1&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;B&lt;/td&gt;\n      &lt;td&gt;2&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;C&lt;/td&gt;\n      &lt;td&gt;3&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/table&gt;\n  \")\n\nSince tables are a common way to store data, {rvest} includes a useful function html_table() that converts directly from an HTML table into a tibble.\n\nhtml_2 %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table()\n\n# A tibble: 3 × 2\n  Name  Number\n  &lt;chr&gt;  &lt;int&gt;\n1 A          1\n2 B          2\n3 C          3\n\n\nApplying this to our real scraped data we can easily extract the table of taught courses.\n\nhtml %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table()\n\n# A tibble: 25 × 3\n   Year      Course                                   Role                      \n   &lt;chr&gt;     &lt;chr&gt;                                    &lt;chr&gt;                     \n 1 \"2021-22\" Supervised Learning                      Lecturer                  \n 2 \"\"        Ethics in Data Science I                 Lecturer                  \n 3 \"\"        Ethics in Data Science II                Lecturer                  \n 4 \"—\"       —                                        —                         \n 5 \"2020-21\" MATH562/582: Extreme Value Theory        Lecturer                  \n 6 \"\"        MATH331: Bayesian Inference              Graduate teaching assista…\n 7 \"\"        MATH330: Likelihood Inference            Graduate teaching assista…\n 8 \"2019-20\" DSCI485: Introduction to LaTeX           Co-leading short course   \n 9 \"\"        MATH566: Longitudinal Data Analysis      Graduate teaching assista…\n10 \"2018-19\" STOR-i Internship: Introduction to LaTeX Co-leading short course   \n# … with 15 more rows"
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#tip-for-building-tibbles",
    "href": "blog/2022-12-01-rvest/index.html#tip-for-building-tibbles",
    "title": "Web Scraping with {rvest}",
    "section": "Tip for Building Tibbles",
    "text": "Tip for Building Tibbles\nWhen scraping data from a webpage, your end-goal is typically going to be constructing a data.frame or a tibble.\nIf you are following our description of tidy data, you’ll want each row to correspond some repeated unit on the HTML page. In this case, you should\n\nUse html_elements() to select the elements that contain each observation unit;\nUse html_element() to extract the variables from each of those observations.\n\nTaking this approach guarantees that you’ll get the same number of values for each variable, because html_element() always returns the same number of outputs as inputs. This is vital when you have missing data - when not every observation unit has a value for every variable of interest.\nAs an example, consider this extract of text about the starwars dataset.\n\nstarwars_html &lt;- minimal_html(\"\n  &lt;ul&gt;\n    &lt;li&gt;&lt;b&gt;C-3PO&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class='weight'&gt;167 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;R2-D2&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class='weight'&gt;96 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;Yoda&lt;/b&gt; weighs &lt;span class='weight'&gt;66 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;R4-P17&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  \")\n\nThis is an unordered list where each list item corresponds to one observational unit (one character from the starwars universe). The name of the character is given in bold, the character species is specified in italics and the weight of the character is denoted by the .weight class. However, some characters have only a subset of these variables defined: for example Yoda has no species entry.\nIf we try to extract each element directly, our vectors of variable values are of different lengths. We don’t know where the missing values should be, so we can’t line them back up to make a tibble.\n\nstarwars_html %&gt;% html_elements(\"b\") %&gt;% html_text2()\n## [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_html %&gt;% html_elements(\"i\") %&gt;% html_text2()\n## [1] \"droid\" \"droid\" \"droid\"\nstarwars_html %&gt;% html_elements(\".weight\") %&gt;% html_text2()\n## [1] \"167 kg\" \"96 kg\"  \"66 kg\"\n\nWhat we should do instead is start by extracting all of the list item elements using html_elements(). Once we have done this, we can then use html_element() to extract each variable for all characters. This will pad with NAs, so that we can collate them into a tibble.\n\nstarwars_characters &lt;- starwars_html %&gt;% html_elements(\"li\")\n\nstarwars_characters %&gt;% html_element(\"b\") %&gt;% html_text2()\n## [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_characters %&gt;% html_element(\"i\") %&gt;% html_text2()\n## [1] \"droid\" \"droid\" NA      \"droid\"\nstarwars_characters %&gt;% html_element(\".weight\") %&gt;% html_text2()\n## [1] \"167 kg\" \"96 kg\"  \"66 kg\"  NA\n\n\ntibble::tibble(\n  name = starwars_characters %&gt;% html_element(\"b\") %&gt;% html_text2(),\n  species = starwars_characters %&gt;% html_element(\"i\") %&gt;% html_text2(),\n  weight = starwars_characters %&gt;% html_element(\".weight\") %&gt;% html_text2()\n)\n\n# A tibble: 4 × 3\n  name   species weight\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; \n1 C-3PO  droid   167 kg\n2 R2-D2  droid   96 kg \n3 Yoda   &lt;NA&gt;    66 kg \n4 R4-P17 droid   &lt;NA&gt;"
  },
  {
    "objectID": "blog/2022-10-11-ravelry-tidy-tuesday/index.html",
    "href": "blog/2022-10-11-ravelry-tidy-tuesday/index.html",
    "title": "Tidy Tuesday: Ravelry Yarn",
    "section": "",
    "text": "For my first Tidy Tuesday I kept things simple with a stacked bar chart.\nI have made several plots using {ggplot2} before, but this was my first attempt at making one aesthetically pleasing (forgive the pun).\nWhen making this plot I learned about using custom font, colours, annotations and arrows from a lot of @nrennie’s past examples.\nCode and figure down below ↓\n\n\nCode\n# Load Packages ----\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(showtext)\n\n# Load Fonts ----\nfont_add_google(name = \"Indie Flower\", family = \"indie-flower\")\nfont_add_google(name = \"Permanent Marker\", family = \"marker\")\nshowtext_auto()\n\n# Load Data ----\nurl &lt;- \"https://github.com/rfordatascience/tidytuesday/raw/master/data/2022/2022-10-11/yarn.csv\"\nyarn &lt;- readr::read_csv(file = url)\n\n# Data Handling ----\n\nother_weight_names &lt;- c(\n  \"Thread\",\n  \"Cobweb\",\n  \"Jumbo\",\n  \"DK / Sport\",\n  \"Aran / Worsted\",\n  \"No weight specified\")\n\nyarn_data &lt;- yarn %&gt;%\n  select(yarn_weight_name) %&gt;%\n  mutate(yarn_weight_name = as.character(yarn_weight_name)) %&gt;%\n  mutate_at(c(\"yarn_weight_name\"), ~replace_na(.,\"Missing\")) %&gt;%\n  mutate(name = fct_collapse(yarn_weight_name, Other = other_weight_names)) %&gt;%\n  mutate(name = fct_collapse(name, \"Double Knit\" = c(\"DK\"))) %&gt;%\n  group_by(name) %&gt;%\n  summarise(value = n())\n\n# Helper data frames for adding arrows to plot\narrow_df_1 &lt;- data.frame(x1 = 27000, x2 = 27000, y1 = 7.5, y2 = 10.4)\narrow_df_2 &lt;- data.frame(x1 = 27000, x2 = 19000, y1 = 7.5, y2 = 10)\n\n# Making Plot ----\n\nbar_colour &lt;- \"#483248\"\nbg_colour &lt;- \"#FEFBEA\"\ntitle_font &lt;- \"marker\"\nmain_font &lt;- \"indie-flower\"\n\nyarn_plot &lt;- yarn_data %&gt;%\n  ggplot(aes(y = reorder(name, value), x = value)) +\n  geom_bar(stat = \"identity\", fill = bar_colour) +\n  theme_void() +\n  ggtitle(\" \\n Yarn weights on Ravelry, ordered by frequency\",subtitle = \" \") +\n  theme(axis.title = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        text = element_text(family = main_font),\n        plot.background = element_rect(fill = bg_colour, colour = bg_colour),\n        panel.background = element_rect(fill = bg_colour, colour = bg_colour),\n        plot.title = element_text(family = title_font, size = 22, hjust = 0.5)\n  ) +\n  lims(x = c(0,28000)) +\n  geom_text(aes(label = name, x = 200),\n            color = bg_colour,\n            hjust = 0,\n            family = main_font,\n            size = 5) +\n  geom_text(aes(label = value),\n            hjust = 0,\n            nudge_x = 200,\n            color = bar_colour,\n            family = main_font,\n            size = 6) +\n  geom_text(aes(label = \"The most popular yarn weights \\n  are 'Fingering' and 'Double Knit'\",\n                x = 20000,\n                y = 6.7),\n            family = main_font,\n            size = 7) +\n  geom_text(aes(label = \"There were more missing yarn weights \\n than in all remaining categories combined\",\n                x = 18000,\n                y = 1.6),\n            family = main_font,\n            size  = 7) +\n  geom_text(aes(label = \"}\"),\n            x = 7000,\n            y = 1.5,\n            size = 19,\n            family = main_font) +\n  geom_text(aes(label = \"Tidy Tuesday 11 Oct 2022 | Data: Ravelry |  @zakvarty\"),\n            x = 29000,\n            y = 4.5,\n            size = 5,\n            family = main_font,\n            angle = 270) +\n  geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2),\n             data = arrow_df_1,\n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2),\n             data = arrow_df_2,\n             arrow = arrow(length = unit(0.03, \"npc\")))\n\nyarn_plot\n\n\n\n\n\n\n\n\n\nCode\n# Exported as 8x8 inch pdf and 800x700 png\n# (next time start by setting canvas size!)"
  },
  {
    "objectID": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html",
    "href": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html",
    "title": "Method of Moments Estimation for Randomised Response Surveys",
    "section": "",
    "text": "For a random variable \\(X\\), the \\(r^{th}\\) moment \\(\\mu_r\\) is given by \\(\\mathbb{E}[X^r]\\).\nFor a discrete random variable with sample space \\(\\mathcal{X}\\), we may calculate this as:\n\\[ \\mu_r = \\mathbb{E}[X^r] = \\sum_{x \\in \\mathcal{X}} x^r \\Pr(X = x).\\]\nFor a continuous random variable with probability density function \\(f_X(x)\\), we may calculate this as:\n\\[\\mu_r = \\mathbb{E}[X^r] = \\int_{x \\in \\mathcal{X}} x^r f_X(x) \\mathrm{d}x.\\]"
  },
  {
    "objectID": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#what-is-a-moment",
    "href": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#what-is-a-moment",
    "title": "Method of Moments Estimation for Randomised Response Surveys",
    "section": "",
    "text": "For a random variable \\(X\\), the \\(r^{th}\\) moment \\(\\mu_r\\) is given by \\(\\mathbb{E}[X^r]\\).\nFor a discrete random variable with sample space \\(\\mathcal{X}\\), we may calculate this as:\n\\[ \\mu_r = \\mathbb{E}[X^r] = \\sum_{x \\in \\mathcal{X}} x^r \\Pr(X = x).\\]\nFor a continuous random variable with probability density function \\(f_X(x)\\), we may calculate this as:\n\\[\\mu_r = \\mathbb{E}[X^r] = \\int_{x \\in \\mathcal{X}} x^r f_X(x) \\mathrm{d}x.\\]"
  },
  {
    "objectID": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#what-is-the-method-of-moments",
    "href": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#what-is-the-method-of-moments",
    "title": "Method of Moments Estimation for Randomised Response Surveys",
    "section": "What is the method of moments?",
    "text": "What is the method of moments?\nSuppose we have observed values \\(x_1, \\ldots, x_n\\), which we wish to model as realisations from a collection of \\(n\\) independent and identically distributed (i.i.d) random variables \\(X_1, \\ldots, X_n\\).\nWe might assume a parametric model for the probability density function \\(f_X(x;\\theta)\\), so that the distribution of the \\(X_i\\)’s is entirely described by one or more unknown parameter(s) \\(\\theta\\).\nThe method of moments gives us a way to estimate the parameters \\(\\theta\\) based on the observed values \\(x_1, \\ldots, x_n\\). We do this by equating the moments of the sample and of the assumed parametric model."
  },
  {
    "objectID": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#example-1-exponential-distribution",
    "href": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#example-1-exponential-distribution",
    "title": "Method of Moments Estimation for Randomised Response Surveys",
    "section": "Example 1: Exponential distribution",
    "text": "Example 1: Exponential distribution\nSuppose that \\(X_1, \\ldots, X_n \\ \\overset{\\text{i.i.d}}{\\sim} \\  \\text{Exp}(\\lambda)\\) with probability density function\n\\[f_X(x) = \\lambda \\exp\\{-\\lambda x\\} \\mathbb{I}[x&gt;0],\\]\nand that we observe a sample of values \\(x_1, \\ldots ,x_n\\) with mean \\(\\bar x = 3.25  = \\frac{13}{4}\\).\nTo estimate the single parameter \\(\\lambda\\) for this model, we can equate the first moment of the Exponential model with the first moment of our sample.\nYou might already know the expectation of an exponential variable is \\(\\lambda^{-1}\\). If not, you might derive it for yourself:\n\\[\\begin{align*}\n    \\mu_1 = \\mathbb{E}[X] &= \\int_{-\\infty}^{\\infty} x f_X(x) \\mathrm{d}x \\\\\n    &= \\int_{-\\infty}^{\\infty} x \\lambda \\exp\\{-\\lambda x\\} \\mathbb{I}[x&gt;0] \\mathrm{d}x \\\\\n    &= \\int_{0}^{\\infty} x \\lambda \\exp\\{-\\lambda x\\} \\mathrm{d}x \\\\\n    &= \\left[-x \\exp\\{-\\lambda x\\}\\right]_{0}^{\\infty} - \\int_0^{\\infty} - \\exp\\{-\\lambda x\\} \\mathrm{d}x \\\\\n    &= (0 - 0) + \\frac{1}{\\lambda} \\int_0^{\\infty} \\lambda \\exp\\{-\\lambda x\\} \\mathrm{d}x \\\\\n    &= \\frac{1}{\\lambda}.\n\\end{align*}\\]\nEquating this first moment of the probabiltiy model to the first sample moment, \\(\\frac{1}{n}\\sum_{i=1}^{n} x_i = \\bar x = \\frac{13}{4},\\) we can find the method of moments estimate \\(\\hat \\lambda\\) for \\(\\lambda\\).\nThis gives us \\[ \\frac{1}{\\hat\\lambda} = \\bar x \\quad \\Rightarrow \\quad \\hat \\lambda = \\bar x ^{-1} = \\frac{4}{13}.\\]"
  },
  {
    "objectID": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#example-2---gamma-distribution.",
    "href": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#example-2---gamma-distribution.",
    "title": "Method of Moments Estimation for Randomised Response Surveys",
    "section": "Example 2 - Gamma Distribution.",
    "text": "Example 2 - Gamma Distribution.\nSuppose instead that we had a two parameter model for our data generating process, such as the gamma distribution. In this case, we proceed in much the same way, but to estimate the two parameters of this model we must equate the first two moments of the sample and the model, and then solve those equations simultaneously.\nIf \\(Y_1, \\ldots, Y_n \\overset{\\text{i.i.d.}}{\\sim} \\text{Gamma}(\\alpha, \\beta)\\), then\n\\[f_X(x) = \\frac{\\beta ^ \\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1} \\exp{-\\beta x} \\mathbb{I}[x &gt; 0].\\]\nWe can calculate the first two moments of the gamma distribution directly using this probability density function. Alternatively, we might obtain these using the known closed forms for the mean and variance of a Gamma random variable:\n\\[\\mu_1 = \\mathbb{E}[Y] = \\frac{\\alpha}{\\beta};\\]\n\\[\\begin{align*}\n\\mu_2 = \\mathbb{E}[Y^2] &= \\text{Var}(Y) + \\mathbb{E}[Y]^2 \\\\\n&= \\frac{\\alpha}{\\beta^2} + \\left(\\frac{\\alpha}{\\beta}\\right)^2 \\\\\n&= \\frac{\\alpha + \\alpha^2}{\\beta^2}.\n\\end{align*}\\]\nEquating these to the sample moments we get the system of simultaneous equations that we need to solve:\n\\[\\begin{equation}\n\\bar x = \\frac{\\alpha}{\\beta} \\quad \\quad \\frac{1}{n}\\sum_{i=1}^{n} x_i^2 = \\frac{\\alpha + \\alpha^2}{\\beta^2}.\n\\end{equation}\\]\nSuppose \\(\\hat \\alpha\\) and \\(\\hat \\beta\\) satisfy both of these equations. By rearrangement of the first equation we can express \\(\\hat \\alpha = \\hat \\beta \\bar x\\). Substituting this into the second equation we find that:\n\\[\\begin{align*}\n\\frac{1}{n} \\sum_{i=1}^{n} x_i ^2 = \\frac{\\hat \\beta \\bar x + (\\hat \\beta \\bar x)^2}{\\hat \\beta^2}\n\\quad\n\\Rightarrow\n\\quad\n\\hat\\beta = \\frac{\\bar x}{\\sum_{i=1}^{n} x_i^2 - \\bar x^2}.\n\\end{align*}\\]\nThis can then be substituted back into the fist equation to find that\n\\[\\hat \\alpha = \\frac{\\bar x^2}{\\sum_{i=1}^{n} x_i^2 - \\bar x^2}.\\]\nAnd so, we have found the method of moments estimators \\(\\hat \\alpha\\) and \\(\\hat \\beta\\) for our two model parameters \\(\\alpha\\) and \\(\\beta\\)."
  },
  {
    "objectID": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#example-3---beta-distribution",
    "href": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#example-3---beta-distribution",
    "title": "Method of Moments Estimation for Randomised Response Surveys",
    "section": "Example 3 - Beta Distribution",
    "text": "Example 3 - Beta Distribution\nSuppose instead that we had observations \\(z_1, \\ldots, z_n\\) which we model as realisations of \\(Z_1, \\ldots, Z_n\\) which are i.i.d. random variables from a beta distribution. This is another two parameter family of probability distributions, characterised by probability density functions of the form\n\\[ f_Z(z; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha - 1} (1- x)^{\\beta - 1}.\\]\nWe can show that\n\\[ \\mathbb{E}[Z] = \\frac{\\alpha}{\\alpha + \\beta}\\]\nand\n\\[ \\mathbb{E}[Z^2] = \\text{Var}(Z) + \\mathbb{E}[Z]^2  \n        = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n            + \\left[\\frac{\\alpha}{\\alpha + \\beta}\\right]^2 \\\\\n        = \\frac{\\alpha \\beta + \\alpha^2(\\alpha + \\beta + 1)}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}.\\]\nSo the system of equations that we wish to solve simultaneously for \\(\\hat \\alpha\\) and \\(\\hat\\beta\\) are\n\\[\\sum_{i=1}^n z_i = \\frac{\\hat\\alpha}{\\hat\\alpha + \\hat\\beta}\\]\nand\n\\[\\sum_{i=1}^n z_i^2 = \\frac{\\hat\\alpha \\hat\\beta + \\hat\\alpha^2(\\hat\\alpha + \\hat\\beta + 1)}{(\\hat\\alpha + \\hat\\beta)^2(\\hat\\alpha + \\hat\\beta + 1)}.\\]\nThis system of equations has no closed-form solution but could be solved using numerically to optimisation to produce the method of moments estimates for a particular data set.\nThis points out that while the method of moments is conceptually simple, actually obtaining the estimates for a given data generating model is not always straightforward."
  },
  {
    "objectID": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#example-4---binomial-distribution-direct-response",
    "href": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#example-4---binomial-distribution-direct-response",
    "title": "Method of Moments Estimation for Randomised Response Surveys",
    "section": "Example 4 - Binomial Distribution (Direct Response)",
    "text": "Example 4 - Binomial Distribution (Direct Response)\nSuppose we are estimating some population proportion \\(p\\) of people with a postgraduate degree. Assuming that we can take a representative sample of size \\(m\\) from the population, we might model the number of survey respondents who have a postgraduate degree as \\(N \\sim \\text{Bin}(m, p)\\).\nIn that case \\(\\mathbb{E}[N] = mp\\) and since we only run the survey once, our sample mean is simply the one observed count.\nEquating the fist moments of the sample and the model, we have \\(n = m \\hat p\\) and so \\(\\hat p = \\frac{n}{m}\\).\nTherefore, under this direct-response survey design, the method of moments estimate for the population proportion of people with a postgraduate degree is simply the sample proportion."
  },
  {
    "objectID": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#example-5---binomial-distribution-random-response",
    "href": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#example-5---binomial-distribution-random-response",
    "title": "Method of Moments Estimation for Randomised Response Surveys",
    "section": "Example 5 - Binomial Distribution (Random Response)",
    "text": "Example 5 - Binomial Distribution (Random Response)\nSuppose instead that we were interested in the proportion of the population who have illegally downloaded content from the internet. We will denote the true population proportion that we wish to estimate as \\(p\\).\nIf we use a direct response surey, some respondents might be reluctant to admit thier bad behaviour, this might cause non-response or dishonest responses from some participants. To alleviate this issue, we can use a randomised response survey to provide respondents with plausible deniability. We will add some element of randomisation to the survey design, so that for an individual respondent who replies in the affirmative we will no longer be certain that they have illegal downloads.\nIn particular, if a participant has not downloaded content from the internet, they will declare as such with probability 0.75 and with probability 0.25 they will declare that they do indeed have illegal downloads. Therefore, a respondent might reply in the affirmative to this survey either if they have truly downloaded illegal content or if they have not and happen to do so because of their random allocation. The probability of an affirmative response is given by \\(\\theta = p + (1-p)/4\\).\nThe expectation of our total number of affirmative responses \\(N\\) could, as in the direct response design, be modelled using a binomial distribution with \\(m\\) trials. However, for this randomised response design the “success” probability is \\(\\theta\\) rather than \\(p\\).\nTherefore the first moment of our affirmative response count \\(N\\) is\n\\[\\mathbb{E}[N] =  m \\theta = m \\left[ p + (1 - p)/4 \\right],\\]\nwhich we can equate to the sample mean (i.e. the observed count)\n\\[m \\left[ \\hat p + (1 - \\hat p)/4 \\right] = n.\\]\nRearranging this equality results in the following method of moments estimate for \\(p\\):\n\\[\\hat p = \\frac{4}{3}\\left(\\frac{n}{m} - \\frac{1}{4}\\right) = \\frac{4n}{3m} - \\frac{1}{3}.\\]\nWe can use this expression to get an unbiased estimate of the population proportion of people who illegally download content, based on our deliberately corrupted responses that provide individuals with plausible deniability."
  },
  {
    "objectID": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#session-information",
    "href": "blog/2023-11-09-method-of-moments-estimation-for-randomised-response-surveys/index.html#session-information",
    "title": "Method of Moments Estimation for Randomised Response Surveys",
    "section": "Session Information",
    "text": "Session Information\n\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin20 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, utils, datasets, methods and base\nloaded via a namespace (and not attached): htmlwidgets(v.1.6.4), compiler(v.4.3.3), fastmap(v.1.1.1), cli(v.3.6.2), tools(v.4.3.3), htmltools(v.0.5.8.1), rstudioapi(v.0.16.0), yaml(v.2.3.8), Rcpp(v.1.0.12), pander(v.0.6.5), rmarkdown(v.2.26), knitr(v.1.45), jsonlite(v.1.8.8), xfun(v.0.43), digest(v.0.6.35), rlang(v.1.1.4) and evaluate(v.0.23)"
  },
  {
    "objectID": "blog/2023-07-26-mundanity-of-excellence/index.html",
    "href": "blog/2023-07-26-mundanity-of-excellence/index.html",
    "title": "The Mundanity of Excellence",
    "section": "",
    "text": "Image credit: Serena Repice Lentini.\n\n\n\nchambliss1989mundanity\nChambliss (1989)\nTitle: The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers. {1989}.\nAuthors: Daniel F. Chambliss\nKey words: Workflows, Data Science.\nDaniel Chambliss, a sociologist writing in the 1980s, asserts three main points in this essay:\n\nexcellence in any field is the result of qualitative not quantitative changes in behaviour;\ntalent is a retrospective explanation for success, used to account for unexplained variability in performance when only final outcomes are observed;\nachieving excellence is the combination of initial privilege and mundane actions.\n\nThese claims are expanded upon based on a qualitative, longitudinal study of competitive swimmers at a range of “levels” from casual competitors to Olympic athletes, with the relevance of the findings to other disciplines signposted throughout the text.\nThe text highlights more nuanced alternative to the common view that everyone should progress linearly through progressive “levels” of an activity, and do so simply by increasing the quantity of their current activities. The essay is delivered in conversational tone and is exceptionally easy to read (particularly in comparison to statistics papers); it falls into the happy middle ground between academic rigour and self-help readability.\n\n\n\n\nReferences\n\nChambliss, Daniel F. 1989. “The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers.” Sociological Theory 7 (1): 70–86.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html",
    "href": "blog/2023-01-06-data-wrangling/index.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Okay, so you’ve got some data. Great start!\nYou might have had it handed to you by a collaborator, requested it via an API or scraped it from the raw html of a webpage. In the worst case scenario, you’re an actual scientist (not just a data one) and you spent the last several months of your life painstakingly measuring flower petals or car parts. Now we really want to do something useful with that data.\nWe’ve seen already how you can load the data into R and pivot between wider and longer formats, but that probably isn’t enough to satisfy your curisity. You want to be able to view your data, manipulate and subset it, create new variables from exisiting ones and cross-reference your dataset with others. All of these are things possible in R and are known under various collective names including data manipulation, data munging and data wrangling.\nI’ve decided to use the term data wranging here. That’s because data manipulation sounds boring a.f. and data munging is both unpleasant to say and makes me imagine we are squelching through some sort of information swamp.\nIn what follows I’ll give a fly-by tour of tools for data wrangling in R, showing some examples along the way. I’ll focus on some of the most common and useful operations and link out to some more extensive guides for wrangling your data in R, that you can refer back to as you need them."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#what-is-data-wrangling",
    "href": "blog/2023-01-06-data-wrangling/index.html#what-is-data-wrangling",
    "title": "Data Wrangling",
    "section": "",
    "text": "Okay, so you’ve got some data. Great start!\nYou might have had it handed to you by a collaborator, requested it via an API or scraped it from the raw html of a webpage. In the worst case scenario, you’re an actual scientist (not just a data one) and you spent the last several months of your life painstakingly measuring flower petals or car parts. Now we really want to do something useful with that data.\nWe’ve seen already how you can load the data into R and pivot between wider and longer formats, but that probably isn’t enough to satisfy your curisity. You want to be able to view your data, manipulate and subset it, create new variables from exisiting ones and cross-reference your dataset with others. All of these are things possible in R and are known under various collective names including data manipulation, data munging and data wrangling.\nI’ve decided to use the term data wranging here. That’s because data manipulation sounds boring a.f. and data munging is both unpleasant to say and makes me imagine we are squelching through some sort of information swamp.\nIn what follows I’ll give a fly-by tour of tools for data wrangling in R, showing some examples along the way. I’ll focus on some of the most common and useful operations and link out to some more extensive guides for wrangling your data in R, that you can refer back to as you need them."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#example-data-sets",
    "href": "blog/2023-01-06-data-wrangling/index.html#example-data-sets",
    "title": "Data Wrangling",
    "section": "Example Data Sets",
    "text": "Example Data Sets\nTo demonstrate some standard skills we will use some standard datasets that come built into any R installation. These are the penguins data set from {palmerpenguins} and the mtcars data set.\n\nlibrary(palmerpenguins)\npengins &lt;- palmerpenguins::penguins\ncars &lt;- datasets::mtcars"
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#viewing-your-data",
    "href": "blog/2023-01-06-data-wrangling/index.html#viewing-your-data",
    "title": "Data Wrangling",
    "section": "Viewing Your Data",
    "text": "Viewing Your Data\n\nView()\nThe View() function can be used to crease a spreadsheet-like view of your data. In RStudio this will open as a new tab.\nView() will work for any “matrix-like” R object, such as a tibble, data frame, vector or matrix. Note the capital letter - the function is called View(), not view().\n\nView(penguins)\n\n\n\n\nScreenshot of RStduio files pane, containg a spreadsheet view of the palmer penguins data set.\n\n\n\n\nhead()\nFor large data sets, you might not want (or be able to) view it all at once. You can then use head() to view the first few rows. The integer argument n specifies the number of rows you would like to return.\n\nhead(x = pengins, n = 3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\n\nstr()\nAn alternative way to view the a large data set, or one with a complicated format is to examine its structure with str(). This is a useful way to inspect the structure of list-like objects, particularly when they’ve got a nested structure.\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\n\n\nnames()\nFinally, if you just want to access the variable names you can do so with the names() function from base R.\n\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\nSimilarly, you can explicitly access the row and column names of a data frame or tibble using colnames() or rownames().\n\ncolnames(cars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\nrownames(cars)\n\n [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n[10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n[13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n[16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n[19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n[22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n[25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n[28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n[31] \"Maserati Bora\"       \"Volvo 142E\"         \n\n\nIn the cars data, the car model are stored as the row names. This doesn’t really jive with our idea of tidy data - we’ll see how to fix that shortly."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#renaming-variables",
    "href": "blog/2023-01-06-data-wrangling/index.html#renaming-variables",
    "title": "Data Wrangling",
    "section": "Renaming Variables",
    "text": "Renaming Variables\n\ncolnames()\nThe function colnames() can be used to set as well as to retrieve column names.\n\ncars_renamed &lt;- cars \ncolnames(cars_renamed)[1] &lt;- \"miles_per_gallon\"\ncolnames(cars_renamed)\n\n [1] \"miles_per_gallon\" \"cyl\"              \"disp\"             \"hp\"              \n [5] \"drat\"             \"wt\"               \"qsec\"             \"vs\"              \n [9] \"am\"               \"gear\"             \"carb\"            \n\n\n\n\ndplyr::rename()\nWe can also use functions from {dplyr} to rename columns. Let’s alter the second column name.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ncars_renamed &lt;- rename(.data = cars_renamed, cylinders = cyl)\ncolnames(cars_renamed)\n\n [1] \"miles_per_gallon\" \"cylinders\"        \"disp\"             \"hp\"              \n [5] \"drat\"             \"wt\"               \"qsec\"             \"vs\"              \n [9] \"am\"               \"gear\"             \"carb\"            \n\n\nThis could be done as part of a pipe, if we were making many alterations.\n\ncars_renamed &lt;- cars_renamed %&gt;% \n  rename(displacement = disp) %&gt;% \n  rename(horse_power = hp) %&gt;% \n  rename(rear_axel_ratio = drat)\n\ncolnames(cars_renamed)\n\n [1] \"miles_per_gallon\" \"cylinders\"        \"displacement\"     \"horse_power\"     \n [5] \"rear_axel_ratio\"  \"wt\"               \"qsec\"             \"vs\"              \n [9] \"am\"               \"gear\"             \"carb\"            \n\n\nWhen using the dplyr function you have to remember the format new_name = old_name. This matches the format used to create a data frame or tibble, but is the opposite order to the python function of the same name and often catches people out.\nIn the section (#creating-new-variables) on creating new variables, we will see an alternative way of doing this by copying the column and then deleting the original."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#subsetting",
    "href": "blog/2023-01-06-data-wrangling/index.html#subsetting",
    "title": "Data Wrangling",
    "section": "Subsetting",
    "text": "Subsetting\n\nBase R\nIn base R you can extract rows, columns and combinations thereof using index notation.\n\n# First row\npenguins[1, ]\n\n# A tibble: 1 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n# First Column \npenguins[ , 1]\n\n# A tibble: 344 × 1\n   species\n   &lt;fct&gt;  \n 1 Adelie \n 2 Adelie \n 3 Adelie \n 4 Adelie \n 5 Adelie \n 6 Adelie \n 7 Adelie \n 8 Adelie \n 9 Adelie \n10 Adelie \n# … with 334 more rows\n\n# Rows 2-3 of columns 1, 2 and 4\npenguins[2:3, c(1, 2, 4)]\n\n# A tibble: 2 × 3\n  species island    bill_depth_mm\n  &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;\n1 Adelie  Torgersen          17.4\n2 Adelie  Torgersen          18  \n\n\nUsing negative indexing you can remove rows or columns\n\n# Drop all but first row\npenguins[-(2:344), ]\n\n# A tibble: 1 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\n# Drop all but first column \npenguins[ , -(2:8)]\n\n# A tibble: 344 × 1\n   species\n   &lt;fct&gt;  \n 1 Adelie \n 2 Adelie \n 3 Adelie \n 4 Adelie \n 5 Adelie \n 6 Adelie \n 7 Adelie \n 8 Adelie \n 9 Adelie \n10 Adelie \n# … with 334 more rows\n\n\nYou can also select rows or columns by their names. This can be done using the bracket syntax ([ ]) or the dollar syntax ($).\n\npengins[ , \"species\"]\n\n# A tibble: 344 × 1\n   species\n   &lt;fct&gt;  \n 1 Adelie \n 2 Adelie \n 3 Adelie \n 4 Adelie \n 5 Adelie \n 6 Adelie \n 7 Adelie \n 8 Adelie \n 9 Adelie \n10 Adelie \n# … with 334 more rows\n\npenguins$species\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo\n\n\nSince penguins is a tibble, these return different types of object. Sub-setting a tibble with bracket syntax will return a tibble, while extracting a column using the dollar syntax returns a vector of values.\n\n\nfilter() and select()\n{dplyr} has two functions for subsetting, filter() subsets by rows and select() subsets by column.\nIn both functions you list what you would like to retain. Filter and select calls can be piped together to subset based on row and column values.\n\npenguins %&gt;% \n  select(species, island,body_mass_g)\n\n# A tibble: 344 × 3\n   species island    body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n 1 Adelie  Torgersen        3750\n 2 Adelie  Torgersen        3800\n 3 Adelie  Torgersen        3250\n 4 Adelie  Torgersen          NA\n 5 Adelie  Torgersen        3450\n 6 Adelie  Torgersen        3650\n 7 Adelie  Torgersen        3625\n 8 Adelie  Torgersen        4675\n 9 Adelie  Torgersen        3475\n10 Adelie  Torgersen        4250\n# … with 334 more rows\n\n\n\npenguins %&gt;% \n  select(species, island, body_mass_g) %&gt;% \n  filter(body_mass_g &gt; 6000)\n\n# A tibble: 2 × 3\n  species island body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;        &lt;int&gt;\n1 Gentoo  Biscoe        6300\n2 Gentoo  Biscoe        6050\n\n\nSubsetting rows can be inverted by negating the filter() statement.\n\npenguins %&gt;% \n  select(species, island, body_mass_g) %&gt;% \n  filter(!(body_mass_g &gt; 6000))\n\n# A tibble: 340 × 3\n   species island    body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n 1 Adelie  Torgersen        3750\n 2 Adelie  Torgersen        3800\n 3 Adelie  Torgersen        3250\n 4 Adelie  Torgersen        3450\n 5 Adelie  Torgersen        3650\n 6 Adelie  Torgersen        3625\n 7 Adelie  Torgersen        4675\n 8 Adelie  Torgersen        3475\n 9 Adelie  Torgersen        4250\n10 Adelie  Torgersen        3300\n# … with 330 more rows\n\n\nand dropping columns can done by selecting all columns except the one(s) you want to drop.\n\npenguins %&gt;% \n  select(species, island, body_mass_g) %&gt;% \n  filter(!(body_mass_g &gt; 6000)) %&gt;% \n  select(!c(species, island))\n\n# A tibble: 340 × 1\n   body_mass_g\n         &lt;int&gt;\n 1        3750\n 2        3800\n 3        3250\n 4        3450\n 5        3650\n 6        3625\n 7        4675\n 8        3475\n 9        4250\n10        3300\n# … with 330 more rows"
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#creating-new-variables",
    "href": "blog/2023-01-06-data-wrangling/index.html#creating-new-variables",
    "title": "Data Wrangling",
    "section": "Creating New Variables",
    "text": "Creating New Variables\n\nBase R\nWe can create new variables in base R by assigning a vector of the correct length to a new column name.\n\ncars_renamed$weight &lt;- cars_renamed$wt\n\nIf we then drop the original column from the data frame, this gives us an alternative way of renaming columns.\n\ncars_renamed &lt;- cars_renamed[ ,-which(names(cars_renamed) == \"wt\")]\nhead(cars_renamed, n = 5)\n\n                  miles_per_gallon cylinders displacement horse_power\nMazda RX4                     21.0         6          160         110\nMazda RX4 Wag                 21.0         6          160         110\nDatsun 710                    22.8         4          108          93\nHornet 4 Drive                21.4         6          258         110\nHornet Sportabout             18.7         8          360         175\n                  rear_axel_ratio  qsec vs am gear carb weight\nMazda RX4                    3.90 16.46  0  1    4    4  2.620\nMazda RX4 Wag                3.90 17.02  0  1    4    4  2.875\nDatsun 710                   3.85 18.61  1  1    4    1  2.320\nHornet 4 Drive               3.08 19.44  1  0    3    1  3.215\nHornet Sportabout            3.15 17.02  0  0    3    2  3.440\n\n\nOne thing to be aware of is that this operation does not preserve column ordering.\nGenerally speaking, code that relies on columns being in a specific order is fragile - it breaks easily. If possible, you should try to write your code in another way that’s robust to column reordering. I’ve done that here when removing the wt column by looking up the column index as part of my code, rather than assuming it will always be the fourth column.\n\n\ndplyr::mutate()\nThe function from {dplyr} to create new columns is mutate(). Let’s create another column that has the car’s weight in kilogrammes rather than tonnes.\n\ncars_renamed &lt;- cars_renamed %&gt;% \n  mutate(weight_kg = weight * 1000)\n\ncars_renamed %&gt;% \n  select(miles_per_gallon, cylinders, displacement, weight, weight_kg) %&gt;% \n  head(n = 5)\n\n                  miles_per_gallon cylinders displacement weight weight_kg\nMazda RX4                     21.0         6          160  2.620      2620\nMazda RX4 Wag                 21.0         6          160  2.875      2875\nDatsun 710                    22.8         4          108  2.320      2320\nHornet 4 Drive                21.4         6          258  3.215      3215\nHornet Sportabout             18.7         8          360  3.440      3440\n\n\nYou can also create new columns that combine multiple other columns\n\ncars_renamed &lt;- cars_renamed %&gt;% \n  mutate(cylinder_adjusted_mpg = miles_per_gallon / cylinders)\n\n\n\nrownames_to_column()\nOne useful example of adding an additional row to a data frame is to convert its row names to a column of the data fame.\n\ncars %&gt;% \n  mutate(model = rownames(cars_renamed)) %&gt;% \n  select(mpg, cyl, model) %&gt;% \n  head(n = 5)\n\n                   mpg cyl             model\nMazda RX4         21.0   6         Mazda RX4\nMazda RX4 Wag     21.0   6     Mazda RX4 Wag\nDatsun 710        22.8   4        Datsun 710\nHornet 4 Drive    21.4   6    Hornet 4 Drive\nHornet Sportabout 18.7   8 Hornet Sportabout\n\n\nTherea neat function called rownames_to_column() in {tibble} which will add this as the first column and remove the row names all in one step.\n\ncars %&gt;% \n  tibble::rownames_to_column(var = \"model\") %&gt;% \n  head(n = 5)\n\n              model  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\nrowids_to_column()\nAnother function from {tibble} adds the row id of each observation as a new column. This is often useful when ordering or combining tables.\n\ncars %&gt;% \n  tibble::rowid_to_column(var = \"row_id\") %&gt;% \n  head(n = 5)\n\n  row_id  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1      1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2      2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3      3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4      4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5      5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#summaries",
    "href": "blog/2023-01-06-data-wrangling/index.html#summaries",
    "title": "Data Wrangling",
    "section": "Summaries",
    "text": "Summaries\nThe summarise() function allows you to collapse a data frame into a single row, which gives a summary statistic of your choosing.\nThis can be used to calculate a single summary\n\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm))\n\n# A tibble: 1 × 1\n  average_bill_length_mm\n                   &lt;dbl&gt;\n1                     NA\n\n\nSince we have missing values, we might instead want to calculate the mean of the recorded values.\n\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  average_bill_length_mm\n                   &lt;dbl&gt;\n1                   43.9\n\n\nWe can also use summarise() to gather multiple summaries in a single data frame.\n\nbill_length_mm_summary &lt;- penguins %&gt;% \n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n\nbill_length_mm_summary\n\n# A tibble: 1 × 8\n   mean median   min   q_0   q_1   q_2   q_3   q_4\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  43.9   44.4  59.6  32.1  39.2  44.4  39.2  59.6\n\n\nIn all this isn’t overly exciting. You might, rightly, wonder why you’d want to use these summarise() calls when we could just use the simpler base R calls directly.\nOne benefit is that the summarise calls ensure consistent output. However, the main advantage comes when you want to apply these summaries to distinct subgroups of the data."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#grouped-operations",
    "href": "blog/2023-01-06-data-wrangling/index.html#grouped-operations",
    "title": "Data Wrangling",
    "section": "Grouped Operations",
    "text": "Grouped Operations\nThe real benefit of summarise() comes from its combination with group_by(). This allows to you calculate the same summary statistics for each level of a factor with only one additional line of code. Here we’re re-calculating the same set of summary statistics we just found for all penguins, but for each individual species.\n\npenguins %&gt;% \n  group_by(species) %&gt;%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 9\n  species    mean median   min   q_0   q_1   q_2   q_3   q_4\n  &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     38.8   38.8  46    32.1  36.8  38.8  36.8  46  \n2 Chinstrap  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n3 Gentoo     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\n\nYou can group by multiple factors to calculate summaries for each distinct combination of levels within your data set. Here we group by combinations of species and the island to which they belong.\n\npenguin_summary_stats &lt;- penguins %&gt;% \n  group_by(species, island) %&gt;%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\npenguin_summary_stats\n\n# A tibble: 5 × 10\n# Groups:   species [3]\n  species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n  &lt;fct&gt;     &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\n\n\nUngrouping\nBy default, each call to summarise() will undo one level of grouping. This means that our previous result was still grouped by species.\n(We can see this by examining the structure of the returned data frame. The first line tells us that this this is an S3 object of class “grouped_df”, which inherits its properties from a “tbl_df”, whose properties in turn come from “tbl” and “data.frame” objects.)\n\nclass(penguin_summary_stats)\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nSince we have grouped by two variables, R expects us to use two summaries before returning a data frame (or tibble) that is not grouped. One way to satisfy this is to use apply a second summary at the species level of grouping.\n\npenguin_summary_stats %&gt;% \n  summarise_all(mean, na.rm = TRUE)\n\nWarning in mean.default(island, na.rm = TRUE): argument is not numeric or\nlogical: returning NA\n\nWarning in mean.default(island, na.rm = TRUE): argument is not numeric or\nlogical: returning NA\n\nWarning in mean.default(island, na.rm = TRUE): argument is not numeric or\nlogical: returning NA\n\n\n# A tibble: 3 × 10\n  species   island  mean median   min   q_0   q_1   q_2   q_3   q_4\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie        NA  38.8   38.7  45.2  33.4  37.0  38.7  37.0  45.2\n2 Chinstrap     NA  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n3 Gentoo        NA  47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\n\nHowever, we won’t always want to do apply another summary. In that case, we can undo the grouping using ungroup(). Remembering to ungroup is a gotcha and cause of confusion when working with multiple-group summaries.\n\nungroup(penguin_summary_stats)\n\n# A tibble: 5 × 10\n  species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n  &lt;fct&gt;     &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\n\nThere’s an alternative method to achieve the same thing in a single step when using {dplyr} versions 1.0.0 and above. This is to to set the .groups parameter of the summarise() function call, which determines the grouping of the returned data frame.\nThe .groups parameter and can take 4 possible values:\n\n“drop_last”: dropping the last level of grouping (The only option before v1.0.0);\n“drop”: All levels of grouping are dropped;\n“keep”: Same grouping structure as .data;\n“rowwise”: Each row is its own group.\n\nBy default, “drop_last” is used if all the results have 1 row and “keep” is used otherwise.\n\n\nReordering Factors\nR stored factors as integer values, which it then maps to a set of labels. Only factor levels that appear in your data will be assigned a coded integer value and the mapping between factor levels and integers will depend on the order that the labels appear in your data.\nThis can be annoying, particularly when your factor levels relate to properties that aren’t numerical but do have an inherent ordering to them. In the example below, we have the t-shirt size of twelve people.\n\ntshirts &lt;- tibble::tibble(\n  id = 1:12, \n  size = as.factor(c(\"L\", NA, \"M\", \"S\", \"XS\", \"M\", \"XXL\", \"L\", \"XS\", \"M\", \"L\", \"S\"))\n)\n\nlevels(tshirts$size)\n\n[1] \"L\"   \"M\"   \"S\"   \"XS\"  \"XXL\"\n\n\nAnnoyingly, the sizes aren’t in order and extra large is not included because it is not included in the sample. This leads to awkward summary tables (and plots).\n\ntshirts %&gt;% group_by(size) %&gt;% summarise(count = n())\n\n# A tibble: 6 × 2\n  size  count\n  &lt;fct&gt; &lt;int&gt;\n1 L         3\n2 M         3\n3 S         2\n4 XS        2\n5 XXL       1\n6 &lt;NA&gt;      1\n\n\nWe can fix this by creating a new variable with the factors explicitly coded in the correct order. We also need to sepecify that we should not drop empty groups as part of group_by().\n\ntidy_tshirt_levels &lt;- c(\"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\", NA)\n\ntshirts %&gt;% \n  mutate(size_tidy = factor(size, levels = tidy_tshirt_levels)) %&gt;% \n  group_by(size_tidy, .drop = FALSE ) %&gt;% \n  summarise(count = n())\n\n# A tibble: 7 × 2\n  size_tidy count\n  &lt;fct&gt;     &lt;int&gt;\n1 XS            2\n2 S             2\n3 M             3\n4 L             3\n5 XL            0\n6 XXL           1\n7 &lt;NA&gt;          1"
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#be-aware-factors",
    "href": "blog/2023-01-06-data-wrangling/index.html#be-aware-factors",
    "title": "Data Wrangling",
    "section": "Be Aware: Factors",
    "text": "Be Aware: Factors\nAs we have seen a little already, categorical variables can cause issues when wrangling and presenting data in R. All of these problems are solvable using base R techniques but the {forcats} package provides tools for the most common of these problems. This includes functions for changing the order of factor levels or the values with which they are associated.\nSome examples functions from the package include:\n\nfct_reorder(): Reordering a factor by another variable.\nfct_infreq(): Reordering a factor by the frequency of values.\nfct_relevel(): Changing the order of a factor by hand.\nfct_lump(): Collapsing the least/most frequent values of a factor into “other”.\n\nExamples of each of these can be found in the forcats vignette or the factors chapter of R for data science."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#be-aware-strings",
    "href": "blog/2023-01-06-data-wrangling/index.html#be-aware-strings",
    "title": "Data Wrangling",
    "section": "Be Aware: Strings",
    "text": "Be Aware: Strings\nWorking with and analysing text data is a skill unto itself. However, it is useful to be able to do some basic manipulation of character strings programatically.\nBecause R was developed as a statistical programming language, it is well suited to the computational and modelling aspects of working with text data but the base R string manipulation functions can be a bit unwieldy at times.\nThe {stringr} package aims to combat this by providing useful helper functions for a range of text management problems. Even when not analysing text data these can be useful, for example to remove prefixes on a lot of column names.\n\n\nWarning: `as_data_frame()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\nℹ The deprecated feature was likely used in the tibble package.\n  Please report the issue at &lt;\u001b]8;;https://github.com/tidyverse/tibble/issues\u0007https://github.com/tidyverse/tibble/issues\u001b]8;;\u0007&gt;.\n\n\nSuppose we wanted to keep only the text following an underscore in these column names. We could do that by using a regular expression to extract lower-case or upper-case letters which follow an underscore.\n\nhead(poorly_named_df)\n\n# A tibble: 6 × 11\n  observatio…¹   V1_A   V2_B   V3_C    V4_D    V5_E   V6_F   V7_G   V8_H    V9_I\n         &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1            1 -0.199 -0.729  1.14   0.0205 -0.714  -1.14  0.786  -0.926 -0.740 \n2            2 -0.956 -1.71  -0.184  1.44   -1.97   -0.470 0.0393  1.42  -2.49  \n3            3  0.585 -1.06  -1.43   0.361   0.0141  0.585 1.01   -1.90  -0.0825\n4            4  0.624  1.17   0.263  1.72   -0.0808  0.707 1.93   -0.675  0.448 \n5            5  0.425 -0.190 -0.360 -0.443   0.134  -0.339 0.0577  1.21  -1.38  \n6            6  0.169  1.40   1.82  -0.999   1.14   -0.377 0.471   0.934  0.604 \n# … with 1 more variable: V10_J &lt;dbl&gt;, and abbreviated variable name\n#   ¹​observation_id\n\n\n\nstringr::str_extract(names(poorly_named_df), pattern = \"(?&lt;=_)([a-zA-Z]+)\")\n\n [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\" \n\n\nAlternatively, can avoid using regular expressions. We can split each column name at the underscore and keep only the second part of each string.\n\n# split column names at underscores and inspect structure of resuting object\nsplit_strings &lt;- stringr::str_split(names(poorly_named_df), pattern = \"_\")\nstr(split_strings)\n\nList of 11\n $ : chr [1:2] \"observation\" \"id\"\n $ : chr [1:2] \"V1\" \"A\"\n $ : chr [1:2] \"V2\" \"B\"\n $ : chr [1:2] \"V3\" \"C\"\n $ : chr [1:2] \"V4\" \"D\"\n $ : chr [1:2] \"V5\" \"E\"\n $ : chr [1:2] \"V6\" \"F\"\n $ : chr [1:2] \"V7\" \"G\"\n $ : chr [1:2] \"V8\" \"H\"\n $ : chr [1:2] \"V9\" \"I\"\n $ : chr [1:2] \"V10\" \"J\"\n\n# keep only the second element of each character vector in the list\npurrr::map_chr(split_strings, function(x){x[2]})\n\n [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\" \n\n\nAgain, unless you plan to work extensively with text data, I would recommend that you look up such string manipulations as you need them. The strings section of R for Data Science is a useful starting point."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#be-aware-date-times",
    "href": "blog/2023-01-06-data-wrangling/index.html#be-aware-date-times",
    "title": "Data Wrangling",
    "section": "Be Aware: Date-Times",
    "text": "Be Aware: Date-Times\nRemember all the fuss we made about storing dates in the ISO standard format? That was because dates and times are complicated enough to work with before adding extra ambiguity.\n\\[ \\text{YYYY} - \\text{MM} - \\text{DD}\\] Dates, times and time intervals have to reconcile two factors: the physical orbit of the Earth around the sun and the social and geopolitical mechanisms that determine how we measure and record the passing of time. This makes the history of date and time records fascinating and can make working with this type of data complicated.\nMoving from larger to smaller time spans: leap years alter the number of days in a year, months are of variable length (with February’s length changing from year to year). If your data are measured in a place that uses daylight saving, then one day a year will be 23 hours long and another will be 25 hours long. To make things worse, the dates and the hour at which the clocks change are not uniform across countries, which might be in distinct time zones that themselves change over time.\nEven at the level of minutes and seconds we aren’t safe - since the Earth’s orbit is gradually slowing down a leap second is added approximately every 21 months. Nor are things any better when looking at longer time scales or across cultures, where we might have to account for different calendars: months are added removed and altered over time, other calendar systems still take different approaches to measuring time and using different units and origin points.\nWith all of these issues you have to be very careful when working with date and time data. Functions to help you with this can be found in the {lubridate} package, with examples in the dates and times chapter of R for data science."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#be-aware-relational-data",
    "href": "blog/2023-01-06-data-wrangling/index.html#be-aware-relational-data",
    "title": "Data Wrangling",
    "section": "Be Aware: Relational Data",
    "text": "Be Aware: Relational Data\nWhen the data you need are stored across two or more data frames you need to be able to cross-reference those and match up values for observational unit. This sort of data is know as relational data, and is used extensively in data science.\nThe variables you use to match observational units across data frames are known as keys. The primary key belongs to the first table and the foreign key belongs to the secondary table. There are various ways to join these data frames, depending on if you want to retain.\n\nJoin types\nYou might want to keep only observational units that have key variables values in both data frames, this is known as an inner join.\n\n\n\nInner join diagram. Source: R for Data Science\n\n\nYou might instead want to keep all units from the primary table but pad with NAs where there is not a corresponding foreign key in the second table. This results in an (outer) left-join.\n\n\n\nDiagram for left, right and outer joins. Source: R for Data Science\n\n\nConversely, you might keep all units from the second table but pad with NAs where there is not a corresponding foreign key in the primary table. This is imaginatively named an (outer) right-join.\nThe final common join type is an outer join, in which all observational units from either table are retained and all missing values are padded with NAs. This is known as an (outer) full join.\nThings get more complicated when keys do not uniquely identify observational units in one or both tables. I’d recommend you start exploring these ideas with the relational data chapter of R for Data Science.\n\n\nWhy and where to learn more\nWorking with relational data is essential to getting any data science up and running out in the wilds of reality. This is because businesses and companies don’t store all of their data in a huge single csv file. For one this isn’t very efficient, because most cells would be empty. Secondly, it’s not a very secure approach, since you can’t grant partial access to the data. That’s why information is usually stored in many data frames (more generically known as tables) within one or more databases.\nThese data silos are created, maintained, accessed and destroyed using a relational data base management system. These management systems use code to manage and access the stored data, just like we have seen in the dplyr commands above. You might well have heard of the SQL programming language (and its many variants), which is a popular language for data base management and is the inspiration for the dplyr package and verbs.\nIf you’d like to learn more then there are many excellent introductory SQL books and courses, I’d recommend picking one that focuses on data analysis or data science unless you really want to dig into efficient storage and querying of databases."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#wrapping-up",
    "href": "blog/2023-01-06-data-wrangling/index.html#wrapping-up",
    "title": "Data Wrangling",
    "section": "Wrapping up",
    "text": "Wrapping up\nWe have:\n\nLearned how to wrangle tabular data in R with {dplyr}\nMet the idea of relational data and {dplyr}’s relationship to SQL\nBecome aware of some tricky data types and packages that can help."
  },
  {
    "objectID": "blog/2022-09-26-hello-world/index.html",
    "href": "blog/2022-09-26-hello-world/index.html",
    "title": "Hello, World!",
    "section": "",
    "text": "Some Text\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n\n\nSome Code\n\n\nCode\npar(bg = NA)\nplot(\n    x = mtcars$mpg,\n    y = mtcars$cyl,\n    xlab = \"miles per gallon\",\n    ylab = \"cyclider count\",\n    pch = 16)"
  },
  {
    "objectID": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html",
    "href": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html",
    "title": "Good Enough Practices in Scientific Computing",
    "section": "",
    "text": "wilson2017good\nTitle: Good Enough Practices in Scientific Computing. {PLOS Computational Biology, 2017} (20 pages).\nAuthors: Greg Wilson, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt and Tracy K. Teal.\nKey words: computing, research skills, reproducibilty, guides.\nIn this paper by Wilson et al. (2017), a collection of experienced researchers and instructors give simple ways to implement good computing practices during a research project. They do this by providing a list of concrete recommendations that every researcher can adopt, regardless of their current computational skills. This is important to help the transition toward open, documented and reproducible research. The article is aimed specifically at people who are new to computational research but also contains useful guidance for more experienced researchers."
  },
  {
    "objectID": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#reading-summary",
    "href": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#reading-summary",
    "title": "Good Enough Practices in Scientific Computing",
    "section": "",
    "text": "wilson2017good\nTitle: Good Enough Practices in Scientific Computing. {PLOS Computational Biology, 2017} (20 pages).\nAuthors: Greg Wilson, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt and Tracy K. Teal.\nKey words: computing, research skills, reproducibilty, guides.\nIn this paper by Wilson et al. (2017), a collection of experienced researchers and instructors give simple ways to implement good computing practices during a research project. They do this by providing a list of concrete recommendations that every researcher can adopt, regardless of their current computational skills. This is important to help the transition toward open, documented and reproducible research. The article is aimed specifically at people who are new to computational research but also contains useful guidance for more experienced researchers."
  },
  {
    "objectID": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#notes",
    "href": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#notes",
    "title": "Good Enough Practices in Scientific Computing",
    "section": "Notes",
    "text": "Notes\nThis article describes some of the best-practices in software development and how those ideas can be implemented in a reasearch project. This focus here is on implementing these approaches without requiring reseachers to learn how to use lots of peripheral technologies (for example git and LaTeX / markdown).\nAn earlier paper “Best Practices for Scientifc Computing” (Wilson et al. 2014), is aimed at those who have or would like to develop such peripheral skills."
  },
  {
    "objectID": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#suggested-best-practices",
    "href": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#suggested-best-practices",
    "title": "Good Enough Practices in Scientific Computing",
    "section": "Suggested Best Practices",
    "text": "Suggested Best Practices\nBest practices are grouped into 6 main themes.\n\n1. Data Management\n\nCreate the data you wish to see in the world\nRaw data should be created in a format that is ammenable to analysis and where multiple tables are used, a unique identifer used to link each record across these tables.\n\n\nKeep it backed up, keep it intact\nThis raw data should be backed up in more than one location and preserved during the analysis (i.e. not directly edited). When cleaning, handling and modelling the data keep a record of all steps used.\n\n\nShare the data\nTo allow your future self (and others) to access and cite your hard won data, submit it to a reputable DOI-issuing repository.\n\n\n\n2. Software\n\nScript files\nStart each script with a brief explanatory comment of its purpose and a description of any dependencies.\nWithin scripts, ruthlessly eliminate duplication. Do this by creating functions for any repeated operations and provide simple examples of how those functions work.\nWhen making functions and variables, give them meaningful names. As rule of thumb: fuctions are verbs, variables are nouns.\nIf you need your script to perform different actions, control this behaviour programmatically rather than by commenting/uncommenting sections of code.\n\n# Uncomment for weekly reports\noutput_dir &lt;- paste0(\"weekly_reports/\",year,\"/\",week_of_year,\"/\")\n# Uncomment for annual reports\n#output_dir &lt;- paste0(\"annual_reports/\",year,\"/\")\n\n\nreport_type = \"weekly\"\nyear = 2022\nweek_of_year = 21\n\nif (report_type == \"weekly\") {\n  output_dir &lt;- paste0(\"weekly_reports/\",year,\"/\",week_of_year,\"/\")\n} else if (report_type == \"annual\") {\n  output_dir &lt;- paste0(\"annual_reports/\",year,\"/\")\n} else {\n  stop(\"report_type should be 'weekly' or 'annual'.\")\n}\n\nSubmit the final code for your research project to to a reputable DOI-issuing repository.\n\n\nExternal Code\nBefore writing your own code, check if someone else got there first. Are there well-maintained software libraries that already do what you need?\nIf so, test the code (extensively!) before relying on it. Keep a record of what you have tested and add to this as you find awkward edge cases.\n\n\n\n3. Collaboration\n\nCollaborating within your team\nCreate a single file called README giving an overview of your project. This should describe aim of the project and how to get started working with the data/code/writing. A good rule of thumb is to write this as though it were for either a new-starter on your team. Future you will thank you!\nCreate a shared to-do list for the project in a file called TODO and decide on how you will communicate during the project. For example, what channels will you use for group meetings, quick questions, assigning tasks and setting deadlines?\n\n\nOpening up to the wider world\nAdd another file called LICENSE giving the licensing information for the project. This says who can use it and for what purposes. No license implies you are keeping all rights and nobody is allowed to reuse or modify the materials. For more information on licenses see choosealicense.com or The Open Source Guide. Consult your company’s legal folks as needed.\nCreate a final file called CITATION letting other people know how they should give proper attribution to your work if they use it.\n\n\n\n4. Project Organisation\nEach project should be self-contained in its own directory (folder) and this directory should be named after the project.\nCreate subdirectories called:\n\ndocs/ for all text documents associated with the project\ndata/raw/ for all raw data and metadata\ndata/derived/ for all data files during cleanup and analysis\nsrc for all code you write as part of this project\nbin for all external code or compiled programs that you use in this project\n\nWhen adding files and subdirectories within this structure, name these to clearly reflect their content or function.\n\n\n5. Tracking Changes\nAs soon as any file is created by a human, back it up in multiple locations. If you make a huge file, then consult your IT folks about how to store and back it up.\nAdd a file called CHANGELOG to the docs subfolder. Use this to track all changes made within the project by all contributers, describing when the changes happened and why they were made.\nKeep these changes as small as possible and share among collaborators frequently to avoid getting out of sync.\nMake a Copy the entire project whenever a significant change has been made.\nBetter yet, use a dedicated version control system such as git if that is a realistic option.\n\n\n6. Manuscripts\nPick one and stick to it within each project. The former has a much lower bar to entry and has most of the benefits of the latter (other than manuscripts being stored in the same place as everything else).\n\nWrite the manuscript using online tools with rich formatting, change tracking and reference management. (e.g. Overleaf, Google Docs)\nWrite the manuscript in plain text format the permits version control (e.g. tex + git or markdown + git)"
  },
  {
    "objectID": "blog/2023-08-15-improve-your-relationship/index.html",
    "href": "blog/2023-08-15-improve-your-relationship/index.html",
    "title": "How to Improve Your Relationship with Your Future Self",
    "section": "",
    "text": "bowers2016improve (Bowers and Voors 2016)\nTitle: How to Improve Your Relationship with Your Future Self. {Revista de Ciencia Politica, 2016} (19 pages).\nAuthors: Jake Bowers and Maarten Voors. (University of Illinois, Wageningen University)\nKey words: Reproducibility, Workflows, Data Science.\n\n\n\n\n\nBowers and Voors introduce seven principles to guide the reader in their move toward reproducible research. Based around their own workflows, they justify and give examples of how each of these principles can be put into practice. Although many of these principles are widely known, it can take many iterations for an individual to implement them; the authors seek to ameliorate the reader’s journey toward current best practices. The article focuses mainly on the implementation of best practices, rather than justifying them, and so is aimed at data scientists and data analysts who are aware of the need for reproducibility but don’t know where to get started."
  },
  {
    "objectID": "blog/2023-08-15-improve-your-relationship/index.html#reading-summary",
    "href": "blog/2023-08-15-improve-your-relationship/index.html#reading-summary",
    "title": "How to Improve Your Relationship with Your Future Self",
    "section": "",
    "text": "bowers2016improve (Bowers and Voors 2016)\nTitle: How to Improve Your Relationship with Your Future Self. {Revista de Ciencia Politica, 2016} (19 pages).\nAuthors: Jake Bowers and Maarten Voors. (University of Illinois, Wageningen University)\nKey words: Reproducibility, Workflows, Data Science.\n\n\n\n\n\nBowers and Voors introduce seven principles to guide the reader in their move toward reproducible research. Based around their own workflows, they justify and give examples of how each of these principles can be put into practice. Although many of these principles are widely known, it can take many iterations for an individual to implement them; the authors seek to ameliorate the reader’s journey toward current best practices. The article focuses mainly on the implementation of best practices, rather than justifying them, and so is aimed at data scientists and data analysts who are aware of the need for reproducibility but don’t know where to get started."
  },
  {
    "objectID": "blog/2023-08-15-improve-your-relationship/index.html#notes",
    "href": "blog/2023-08-15-improve-your-relationship/index.html#notes",
    "title": "How to Improve Your Relationship with Your Future Self",
    "section": "Notes",
    "text": "Notes\nThe seven principles introduced are:\n\nData analysis is computer programming.\nNo data analyst is an island for long.\nThe territory of data analysis requires maps.\nVersion control prevents clobbering, reconciles history and helps organise work.\nTesting minimises error.\nWork can be reproducible.\nResearch ought to be credible communication.\n\nThere is a large crossover between these and Wilson et al. (2017).\n\n“In another study, 29 research teams recently collaborated on a project focusing on applied statistics to see if the same answers would emerge from re-analyses of the same dataset (Silberzahn and Uhlmann 2015). They don’t.”\nCoding scales better and reproduces better than using a GUI.\nScripting reduces the opportunity for mistakes and makes them faster to correct.\nUse human-friendly file names and directory structures.\nSplit projects into modular tasks, each gets its own script. This allows parallel working without conflicts.\nData Science is just a series of decisions. Document the options you have and why you make the choice that you do.\nWrite your code for other people, not yourself. This holds you accountable for its quality and to writing clear code.\n\n\nLet us change our traditional attitude to the construction of programs: instead of imagining that our main task is to instruct a computer what to do, let us concentrate on explaining to human beings what we want a computer to do.\nKnuth (1984)\n\n\nLiterate programming allows mixing of code and description, helping code and report to be seen as one thing. Misses the difficulties of scaling and automation with notebooks.\nWrite portable file paths from the root directory of your project.\nSection 4 opens with an excellent and concise description of why we want version control.\nLearning version control takes time, energy and lots of mistakes. Make these mistakes early and on low-stakes projects - you can burn it all to the ground if you need to!\nThe paper was written reproducibly: source code available at https://github.com/jbowers/workflow. Could be a useful example for data science course.\nA formal example of the fork and pull-request workflow is given in this gist by Chase Pettit\nSection 6 ends with a nice quote to motivate practice:\n\n\n“We all learn by doing. When we create a reproducible workflow and share reproducible materials we improve both cumulation of knowledge and our methods for doing social science.”\n(Freese 2007; King 1995)"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html",
    "href": "blog/2022-10-07-rhetorical-precis/index.html",
    "title": "Writing a rhetorical précis",
    "section": "",
    "text": "Photo by Maksym Kaharlytskyi on Unsplash"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#what-is-a-rhetorical-precis",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#what-is-a-rhetorical-precis",
    "title": "Writing a rhetorical précis",
    "section": "What is a rhetorical precis?",
    "text": "What is a rhetorical precis?\n\n\n\nA rhetorical precis is a short summary and analysis of a piece of writing, which considers both the content and the delivery of the piece.\nA rhetorical precis serves to summarise and analyse the text through:\n\nan accurate bibliographic reference to the text,\na list of keywords relating to the text,\na highly structured four-sentence paragraph providing a summary and analysis of the text."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#why-write-one",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#why-write-one",
    "title": "Writing a rhetorical précis",
    "section": "Why write one?",
    "text": "Why write one?\nKeeping a rhetorical precis for each text that you read is a fantasitc way to build the skills of active reading and succinct writing. A rhetorical precis is more informative than a bib entry and more easily reviewed (read: waded through) than a stack of annotated papers.\nTaken collectively, a set of rhetorical precis summaries provide a reading record that can be a tremendously useful when trying to recall the contents of a paper or book long after you originally read it."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#how-to-store-them",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#how-to-store-them",
    "title": "Writing a rhetorical précis",
    "section": "How to store them?",
    "text": "How to store them?\nWriting and storing these reading summaries electronically can make them even more useful. This allows you to search for topics, target audiences or keywords.\nFor this reason it can be helpful to keep them all together in one word document or plain text file. Alternatively, having a single folder with each summary as a plain text or markdown file works well if you are comfortable with searching at the command line. The same can be achieved by writing these summaries within a reference manager, if that is something you are invested in already."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#definition",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#definition",
    "title": "Writing a rhetorical précis",
    "section": "Definition",
    "text": "Definition\nJust to prove that I’m not making all this up:\n\nA rhetorical precis analyzes both the content (the what) and the delivery (the how) of a unit of spoken or written discourse. It is a highly structured four-sentence paragraph blending summary and analysis. Each of the four sentences requires specific information; students are expected to use brief quotations (to convey a sense of the author’s style and tone) and to include a terminal bibliographic reference. Practicing this sort of writing fosters precision in both reading and writing, forcing a writer to employ a variety of sentence structures and to develop a discerning eye for connotative shades of meaning.  Attribution: lumenlearning.com"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#format",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#format",
    "title": "Writing a rhetorical précis",
    "section": "Format",
    "text": "Format\nFour sentences summarising the aim of the work, how this is addressed, why it is important and a description of the target audience.\n\nName of author, [optional phrase describing author], genre and title of work, date in parentheses (additional publishing information in parentheses); a rhetorically accurate verb (such as “asserts,” “argues,” suggests,” “implies,” claims,” etc.); a THAT clause containing the major assertion or thesis statement of the work.\nAn explanation of how the author develops and/or supports the thesis, usually in chronological order.\nA statement of the author’s purpose followed by an “in order to” phrase.\nA description of the intended audience and/or the essay’s tone"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#a-self-indulgent-example",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#a-self-indulgent-example",
    "title": "Writing a rhetorical précis",
    "section": "A (self-indulgent) example",
    "text": "A (self-indulgent) example\nHere is a rather self-indulgent example of a rhetorical precis.\n\nvarty2021inference\nTitle: Inference for extreme earthquake magnitudes accounting for a time-varying measurement process. {ArXiV preprint, 2021} (20 pages).\nAuthors: Zak Varty, Jonathan Tawn, Peter Atkinson and Stijn Bierman.\nKey words: extreme value, earthquake, threshold selection, magnitude of completion, seismology, bootstrap.\nIn this paper, Varty et al (2021) propose a new threshold selection method for modelling earthquake catalogues, where the magnitude distribution is stationary but detection of small events improves over time. The paper generalises the Gutenberg-Richter law to the GPD and uses metrics based on PP and QQ plots to balance between bias and variance when selecting a time-varying threshold. This procedure more than doubles the usable catalogue size for Groningen earthquakes and gives the first emprircal evidence that the magnitude distribution in this region has a finite upper end point. The paper is targeted at applied and research statisticians with an interest in EVT but would also be accessible to a statistically-minded seismologist."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#a-template-for-new-entries",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#a-template-for-new-entries",
    "title": "Writing a rhetorical précis",
    "section": "A template for new entries",
    "text": "A template for new entries\n\nfirstauthorYYYYkeyword\nTitle: Title goes here. {Journal, YYYY} (NN pages).\nAuthors: Author One, Author Two and Author Three. (optional affiliations)\nKey words: key word 1, key word 2, key work 3.\n\nWhat is the document and what does it say?\nHow do they do / show this?\nWhy are they bothering to do this in the first place?\nWho is the intended audience for this work?\n\nIn this DOC_TYPE, AUTHOUR VERB that THESIS_STATEMENT. They DO/SHOW this by ACTIONS. This is important to PEOPLE because REASONS. This work would be useful when PEOPLE are doing ACTIVITY.\n\n`firstauthorYYYYkeyword`\n\n**Title:** _Title goes here. {Journal, YYYY} (NN pages)._\n\n**Authors:** _Author One, Author Two and Author Three. (optional affiliations)_\n\n**Key words:** _key word 1_, _key word 2_, _key work 3_. \n\n1. _What_ is the document and _what_ does it say? \n2. _How_ do they do / show this?\n3. _Why_ are they bothering to do this in the first place?\n4. _Who_ is the intended audience for this work?\n\nIn this DOC_TYPE, AUTHOUR VERB that THESIS\\_STATEMENT.\nThey DO/SHOW this by ACTIONS. \nThis is important to PEOPLE because REASONS. \nThis work would be useful when PEOPLE are doing ACTIVITY."
  }
]