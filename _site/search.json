[
  {
    "objectID": "personal/reading-record.html",
    "href": "personal/reading-record.html",
    "title": "Zak Varty",
    "section": "",
    "text": "This is where I record the books I read each year and the books I might like to read in the future.\n\n\n\nData Science Ethics - David Martens\nThe Virgin Suicides - Jeffrey Eugenides\n\n\n\n\n\nHow to do nothing - Jenny Odell\nHer Body and Other Parties - Carmen Maria Machado\nSomething wicked this way comes - Ray Bradbury\nThe Hole - Hye-Young Pyun, Sora Kim-Russell (Translator)\nGender swapped fairytales - Karrie Fransman & Jonathan Plackett\nLuster - Rven Leilani\nIn the house in the dark of the woods - Laird Hunt\nMexican Gothic - Silvia Moreno-Garcia\n\n\n\n\n\nMeanwhile in Dopamine City - DBC Pierre\nTyll - Daniel Kemlmann\nHow to win friends and influence people - Dale Carnegie\nThe Mr Porter guide to a better day\nProfessor Higgins’s Problem Collection - Peter M Higgins\nThe discomfort of Evening - Marieke Lucas Rijneveld\nBlood and guts in high school - Kathy Acker\nThe Nickel Boys - Colson Whitehead\nThe Need - Helen Phillips\nThe silence of the girls - Pat Barker\nThe ginger child - Patrick Flanery\nBonjour Tristesse - Francois Sagan\nThe Outsiders - S. E. Hinton\nMicro Avdentures - Alastair Humphreys\nDecline and Fall - Evelyn Waugh\n\n\n\n\n\nThe Secret History - Donna Tartt\nFahrenheit 451 - Ray Bradbury\nDune Messiah - Frank Herbert\nOn Earth We Are Briefly Gorgeous - Ocean Vuong\nWow, no thank you - Samantha Irby\nMath without numbers - Milo Beckman\nThe Penelopiad - Margaret Atwood\nIntroduction to Statistical Learning\nBoy Parts - Eliza Clark\nHow to take smart notes - Sonke Ahrens\nIf we were villians - M. L. Rio\nThe Windup Bird Chronicle, Book 1- Haruki Murakami\nThe Windup Bird Chronicle, Book 2 - Haruki Murakami\nThe Windup Bird Chronicle, Book 3 - Haruki Murakami\nTreacle Walker - Alan Garner\nImmune - Philipp Dettmer (DNF)\nThe Beauty of Everyday Things - Seotsu Yanagi\nBetter Data Visualizations - Jonathan Schwabish\nScientifically Speaking - Jo Filshie Browning\nThe Effect - Nick Huntington-Klein\n…\n\n\n\n\n\nThe authenticity project - Clare Pooley ♣️\nDune - Frank Herbert\nHow to make the world add up: Ten rules for thinking differently about numbers - Tim Harford\nThe Thursday murder club - Richard Osman ♣️\nThe Testaments - Margaret Atwood\nConversations with friends - Sally Rooney\nThe housekeeper and the professor - by Yōko Ogawa, Stephen Snyder (Translator)\nThe dice man - Luke Rhinehart ♣️\nIntroduction to statistical modelling of extreme values - Stuart Coles\nStories of the law and how it is broken - The Secret Barrister\nCollege Science Teaching - Terry McGlynn (2021-04-07)\nIn the dream house - Carmen Maria Machado ♣️\nRadio Silence - Alice Oseman\nYour House is on Fire, Your Children All Gone - Stefan Kiesbye\nNick and Charlie - Alice Oseman\nSolaris - Stanisław Lem ♣️\nShow your work - Austin Kleon\nCirce - Madeline Miller (2021-06-19)\nConvenience Store Woman - Sayaka Murata\nOn being different - Merle Miller\nEating Animals - Jonathan Saffran Foer\nPiranesi - Susan Clarke\nHappy git with R - Jenny Bryan\nThe inheritance of loss - Kiran Desai ♣️\nWhy I am no longer talking (to white people) about race - Reni Eddo-Lodge\nReputation - Lex Croucher ♣️ (2021-08-24)\nThe Ethical Algorithm - Michael Kearns and Aaron Roth (2021-09-02)\nEverything I never told you - Celeste Ng\nInvisible Women - Caroline Criado Perez\nDigital Minimalism - Cal Newport\nThe Best of Me - David Sedaris\nWicked - Gregory Maguire\n\n\n\n\n\nWhat I talk about when I talk about running - Haruki Murakami\nGuys knit - Sockmatician\nBrit(ish) - Afua Hirsch\nSemicolon - Cecilia Watson\nThe conscious closet - Elizabeth L. Cline\nA little book of language - David Crystal\nHardboiled wonderland and the end of the world - Haruki Murakami\nThe Topeka school - Ben Lerner\nNormal people - Sally Rooney\nShow your work - Austin Kleon\n1Q84 (Book 1) - Haruki Murakami\n1Q84 (Book 2) - Haruki Murakami\n1Q84 (Book 3) - Haruki Murakami\nThe body - Bill Bryson\nYou too can have a body like mine - Alexandra Kleeman\nI’m thinking of ending things - Iain Reed\nSupper club - Lara Williams\nGirl, woman, other - Bernadine Evaristo ♣️\nLolita - Vladimir Nabokov ♣️\nSeverance - Ling Ma\nThe school of life - Alain de Botton\n…\n\n\n\n\n\nHave you eaten Grandma? - Giles Brandreth\nAn edited life - Anna Newton\nWeight expectations - Dave Chawner\nSpirits of the season - Tanya Kirk\nKilling Comendatore - Haruki Murakami\nHappy ever after - Paul Dolan\nDress your family in corduroy and denim - David Sedaris\nHear the wind sing - Haruki Murakami\nPinball 1973 - Haruki Murakami\nCall me by your name - Andre Aciman\nThe visual display of quantitative information - Edward R. Tufte\nThe elephant vanishes - Haruki Murakami\nA brave new world - Aldous Huxley\nThe missing piece - Shell Silverstein\nThe joy of work - Bruce Daisley\nJoined up writing -Roger McGough\nThe idiot - Elif Batuman\nThe slow professor - Maggie Berg & Barbara Seeber\nThe history boys - Alan Bennett\nThe handmaid’s tale - Margaret Atwood\nClassical mechanics - Leonard Susskind & George Hrabovsky\nIndistractable - Nir Eyal\nAre you experienced - William Sutcliffe\nCalypso - David Sedaris\nNo one is too small to make a difference - Greta Thunberg\n\n\n\n\n\nBridget Jones’ diary - Helen Fielding\nThe mathematics of Christmas - Hannah Fry\nCall me by your name - Andre Aciman\nTalking to my daughter about the economy - Yanis Varoufakis\nIrresistable - Adam Alter\nSympathy - Olivia Sudjic\nMoranifesto - Caitlin Moran\niGen - Jean Twenge\nHagseed - Margaret Atwood\nNorse Mythology - Neil Gaiman\nUpper North Tynedale - Beryl Charlton\nTalk like TED - Carmine Gallo\nThe boy on the Bridge - Mike Carey\nMen without women - Haruki Murakami\nSouth of the border, West of the sun - Haruki Murakami\nQueer city - Peter Ackroyd\nLess - Andrew Sean Greer\nWhy the Dutch are different - Ben Coates\nRest - Alex Soojung-Kim Pang\nWicked - Gregory Maguire\nWomen and power - Mary Beard\nGirls will be girls - Emer O’Toole\nThe boy in the striped pajamas - John Boyne\n:heavy_multiplication_x:\n\n\n\n\n\nThe throwback - Tom Sharpe\nSeven brief lessons on physics - Carlo Rovelli\nThe Circle - Dave Eggers\nFive on brexit island - Bruno Vincent\nYou too can have a body like mine - Alexandra Kleeman\nThe wish - Bill Griffin\nHappy - Derren Brown\nA little gay history - R.B. Parkinson\nNina is not OK - Shappi Khorsandi\nThe life-changing magic of tidying - Marie Kondo\nThe vegetarian - Han Kang\nRespectable - Lyndsey Hanley\nThe great indoors - Ben Highmore\nSolitude - Michael Harris\nIcecream for breakfast - Laure Jane Williams\nAmerican Gods - Neil Gaiman\nBossy pants - Tina Fey\nQuiet - Susan Cain\nHarry Potter and the philospoher’s stone - J.K. Rowling\nHarry Potter and the chamber of secrets - J.K. Rowling\nGerald’s game - Stephen King\nThe handmaid’s tale - Margaret Atwood\nNot working - Lisa Owens\nKafka on the Shore - Haruki Murakami"
  },
  {
    "objectID": "professional/teaching.html",
    "href": "professional/teaching.html",
    "title": "Zak Varty",
    "section": "",
    "text": "I am fortunate to have had the opportunity to teach in a variety of roles. These have included:\n\none-to-one tuition for high school students;\nrunning workshops and computer labs for undergraduate and postgraduate modules;\ndelivering short courses on scientific communication and LaTeX;\nsupervising an undergraduate research project;\ndeveloping and lecturing postgraduate modules in statistics and data science.\n\nI am an associate fellow of the Higher Education Academy, which you can learn more about here. I am currently working toward full fellowship to further develop my teaching practice.\n\n\n\n\n\n\n\n\n\n\nYear\nCourse\nRole\n\n\n\n\n2021-22\nSupervised Learning\nLecturer\n\n\n\nEthics in Data Science I\nLecturer\n\n\n\nEthics in Data Science II\nLecturer\n\n\n—\n—\n—\n\n\n2020-21\nMATH562/582: Extreme Value Theory\nLecturer\n\n\n\nMATH331: Bayesian Inference\nGraduate teaching assistant\n\n\n\nMATH330: Likelihood Inference\nGraduate teaching assistant\n\n\n2019-20\nDSCI485: Introduction to LaTeX\nCo-leading short course\n\n\n\nMATH566: Longitudinal Data Analysis\nGraduate teaching assistant\n\n\n2018-19\nSTOR-i Internship: Introduction to LaTeX\nCo-leading short course\n\n\n\nDSCI485: Introduction to LaTeX\nCo-leading short course\n\n\n\nMATH562: Extreme Value Theory\nGraduate teaching assistant\n\n\n\nMATH235: Statistics II\nGraduate teaching assistant\n\n\n\nMATH240: Project Skills\nGraduate teaching assistant\n\n\n\nMATH330: Likelihood Inference\nGraduate teaching assistant\n\n\n\nMATH230: Probability II\nGraduate teaching assistant\n\n\n2017-18\nSTOR-i Internship: Research Project\nSupervisor\n\n\n\nSTOR-i Internship: Introduction to LaTeX\nCo-leading short course\n\n\n\nDSCI485: Introduction to LaTeX\nCo-leading short course\n\n\n\nMATH235: Statistics II\nGraduate teaching assistant\n\n\n\nMATH465: Bayesian Inference\nGraduate teaching assistant\n\n\n\nMATH330: Likelihood Inference\nGraduate teaching assistant\n\n\n\nMATH230: Probability II\nGraduate teaching assistant\n\n\n2015-16\nLAB100: Introduction to R\nGraduate teaching assistant\n\n\n2011-13\nGCSE & A-level Mathematics\nPrivate tutor"
  },
  {
    "objectID": "professional/research.html",
    "href": "professional/research.html",
    "title": "Zak Varty",
    "section": "",
    "text": "My research focused on developing statistical models to describe and understand earthquakes that are caused by gas extraction in the Netherlands.\nHuman-induced earthquakes are usually smaller in magnitude and fewer in number than their tectonic counterparts. This low-data setting is a challenge to statistical modelling and necessitates the inclusion of domain-expert knowledge. If this low-data setting was not challenging enough, changes in gas extraction and earthquake detection lead to further complications and inefficiencies if standard modelling approaches are used.\nMy PhD research developed statistical methodology to make most efficient use of the limited available data and extended existing earthquake models to improve understanding of these induced seismic events.\nYou can find a copy of my thesis on github.\n\n\n\n\n\nVarty, Z., Tawn J.A., Atkinson P.M. and Bierman S. (2021). Inference for extreme earthquake mangitudes accounting for a time-varying measurement process. (Submitted, preprint on arXiv)\n\n\n\n\n\n\n\n\n\n\n\nDate\nEvent\nLocation\n\n\n\n\nJun 2022\nM_max workshop\nAmsterdam, NL.\n\n\nJan 2021\nCRG Extremes workshop\nRemote.\n\n\nMay 2020\nSTOR-i time-series and spatial statistics workshop\nRemote.\n\n\nSept 2019\nInterfaces in extreme value theory workshop\nLancaster, UK.\n\n\nSept 2019\nRoyal Statistical Society conference\nBelfast, UK.\n\n\nAug 2019\nInternational statistical seismology workshop (StatSei11)\nHakone, JPN.\n\n\nJul 2019\nGRASPA (Italian Environmetics Society)\nPescara, IT.\n\n\nJan 2019\nSTOR-i annual conference\nLancaster, UK.\n\n\nJan 2018\nSTOR-i annual conference\nLancaster, UK.\n\n\n\n\n\n\n\nA review of simulated annealing techniques: Simulated annealing is a metahuristic technique mainly used for combinatorial optimisation. Applications, parallelisation and extensions of the technique were reviewed.\nInference on censored networks: Networks are censored when existing nodes or edges are not observed. Methods for inference under different types of missingness were explored. Master’s project supervised by Dr. Christopher Nemeth.\nComputionally intensive methods for modelling houshold epidemics: Approximate Bayesian Computation was utilised to allow inference on disease models with intractable likelihoods. Master’s dissertation supervised by Prof. Peter Neal."
  },
  {
    "objectID": "professional/talks.html",
    "href": "professional/talks.html",
    "title": "Zak Varty",
    "section": "",
    "text": "Title: Incorporating Ethics into the Data Science Curriculum\nSlides \nAbstract:\nData-driven decision making is now pervasive and impacts us all. Your data is used by others to make decisions about who you are, how you will behave, and what options should be made available to you. Predictive models are used to decide anything from the promotion that is offered to you by a retailer through to whether your loan application is granted by a bank.\nThe ways in which these predictive models can fail mathematically form a core part of the training for an aspiring statistician or data scientist. In contrast, the potential for ethical failures in these same models is rarely covered in-depth during as part of this initial training. As a result, these modes of failure are often not considered until those predictive models have been put into production and are actively causing harm. We argue that to prevent this harm, the ethical impacts of using data to make decisions must be made core to the curriculum of both statistics and data science.\nThis talk will describe how this may be done in a way that is appealing to an audience with a strong mathematical focus and that does not require the authoring of extended essays or moral treaties. The discussion is structured around the development of a post-graduate course in the Ethics of Data Science, but the core ideas are salient to all statistics training. Throughout, we give actionable ways in which these topics may be integrated into statistical training at all levels.\n\n\n\n\n\nSlides from talks will be added here."
  },
  {
    "objectID": "professional/bookmarks.html",
    "href": "professional/bookmarks.html",
    "title": "Zak Varty",
    "section": "",
    "text": "Thread by Sarah Sheffield (@sarahsheffield) advice for new faculty.\nCourse Free 10 week course on google drive about writing academic an academic syllabus. Course by @DLabree, recommended by Elena Aydarova (@aydarova).\nTweet by Gero Grams (@GeroGrams) on how to say no to things\nThread by Mine Dogucu (@MineDogucu) on learning/teaching (with) R. See also:\n\nCompilation of free R resources\nBayesian inference book\nTeaching webpage\nBlog on data pedagogy\n\nThread by Wes Kao (@wes_kao) on managing up\nThread by Vrinda Nair (@VnVrinda) on 22 tools for your PhD Journey\nManaging research careers tool by Edinburgh University Thread and Webpage\nTalk by Laura Albert (@lauraalbertphd) on time management: Do less, Do it faster, Do it at the right time.\n\n\n\n\n\nThread by Matt Betts on 8 questions to consider before starting a PhD.\nThread by Maram Duncan (@MCDuncanLab) on the hidden curriculum for new grad students\nThread by @LifeAfterMyPhD on 5 low-stakes steps to set yourself up for an industry job search\nThread by @mathladyhazel on the best math books for self-learners\nThread by Alex Eble (@alexeble) on adivce for thriving in a PhD. Full document here. Note: has US and economics focus, but translates well.\nWebsite Stats Notes in the British Medical Journal. Like a dictionary but for stats words and methods.\nBook Esstential Math for Data Scienceby Thomas Nield. Recommended by Vicki Boykis for those tooking for an intro/refresher on linear algebra, probability and statistics. ### Agent based modelling\nPaper review of agent based model (preprint of JEL - chunky at 90 pages!)\n\n\n\n\n\nVideo Lectures by Steven Strogatz (@stevenstrogatz) on asymptotics and pertubation methods.\n\n\n\n\n\nLecture Notes by Matt Blackwell, “Causal Inference with Applications”\nPaper The taboo against explicit causal inference in nonexperimental psychology. Suggested by Brian Nosel (@BrianNosek)\nThread by Volodymyr Kuleshov (@volokuleshov) about the ICML 2022 tutorial on Causality and Fairness\nVideo Science before statistics: Causal Inference by Richard McElreath (3 hour crash course in causal inference)\nVideo Series Statistical Rethinking (2022) by Richard McElreath on Youtube\n\n\n\n\n\nArticle on setting up a private .gitignore to keep a clean codebase\nBook / website on package development in Python. (Think Hadley & Bryan’s R packages but for Python) Recommended by @EmilyRiederer\nBook The missing readme by Chris Riccomini and Dmitriy Ryaboy\nPaper Ten Simple Rules for Taking Advantage of Git and GitHub\nSildes by Ariel Muldoon (@aosmith16) on “More git and github - collaborators, merege conflicts and pull requests”\n\n\n\n\n\nThread by Alex Gold (@alexkgold) on getting started with Docker. Free online book\n\n\n\n\n\nArticle by architecture notes (@arcnotes) on “Things you should know about databases”\n\n\n\n\n\nThread by R Ladies - Sources of Messy(ish) data\n\n\n\n\n\nThe Verge - AI Drug development maske chemical weapons\nNature - AI Drug development maske chemical weapons\nRichard McElreath recommendation of paper by Xiao-Li Meng on how data quality influences effective sample size\nThread by Santiago (@svpino) on imbalanced datasets\nTweet by Adam Kruchten (@AdamKruchten) on when we care about marginal or conditional effects.\nASA article on the 2020 work and salary survey, showing women tend to earn less in base salary and total income but that in a regression gender is not significant predictor of total income.\nNPR story Where Google find that men are underpaid\nThread by Paul Hunermund (@PHunermund) on the above google article.\nPreprint on reconstructing large portions of training data from a trained neural network\nPaper Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. Suggested by Abeba Birhane (@Abebab)\nPaper Big data loses to good data. Unrepresentative big surveys significantly overestimated US vaccine uptake.\nPaper On extending LinearSHAP, TreeSHAP and DeepSHAP to RKHS-SHAP. Found through tweet by Siu Lun Chau (@Chau9991).\nChapter 33 on interpretability of book Probabilistic Machine Learning: Advacned Topicsby Kevin Murphy (@sirbayes), Been Kim (@_beenkim) and others.\nBook by Claire McKay Bowen “Protecting your privacy in a data-driven world”\nTweet by Rasha Shrain (@rashaben) requesting reading materials on p-values and p-hacking\nCourse 12 week reading course on Ehics and Data Science by Rohan Alexander\n\n\n\n\n\nThread by Indrajeet Patil (@patilindrajeets) on the effective use of colours in data vis.\nR Package {performance} for aesthetically pleasing ggplot sytle diagnostic plots. (The qq plot even has tolerance intervals!)\nBlog Post by Thomas Mock on Creating and using custom ggplot2 themes\nBlog Posts by Ameila McNamara (@AmeliaMN) about Histograms and Kernel Density Estimation\n\n\n\n\n\nCourse 10-week reading list on “History of Statistics and Data Sciences” by Rohan Alexander\n\n\n\n\n\nTweet by Steve Bauman (@realstevebauman) pointing out that Github’s markdown now supports note and warning blockquotes\nTweet by Zev Ross on using the character ├ to get aesthetically pleasing subsections in RStudio\n\n\n\n\n\nStatus code 400 meme by @da_667\nThe binary search tree actually exists by Ahmad Awais (@MrAhmadAwais)\nData Science Dinosaur A computer science python eating a statistics elephant\n\n\n\n\n\nThread by Carl Bergstrom (@CT_Bergstrom) and Ryan McGee (@RS_McGee) telling the story of a paper using a comic strip and stick-figure Darwin.\nThread by Tessa Davis on slide design to keep your audience engaged\nThread by Dorsa Amir (@DorsaAmir) on slide design\nWebsite OpenPeeps - Open Source hand drawn individual characters\nBlog Post by Kate Jolly (@katejolly6) on designing slides in xaringan with xaringanthemer and css.\n\n\n\n\n\nBlog Post on double descent in neural network performance ### Optimisation\nVideo by Trefor Bazett (@TreforBazett) on using Lagrange multipliers to solve constrained optimisation problems\nVideo Series by @3blue1brown on constrained optimisation (hosted on khan academy)\n\n\n\n\n\nThread by Pau Labarta Bajo (@paulabartabajo_)\n\n\n\n\n\nCourse Material By Rick Schoenberg on point process models\nBlog Post by Benjamin Cretois on fitting point process models in stan.\n\n\n\n\n\nTweet by Francisco Yirá (@francisco_yira) about designing a personal learning plan.\n\n\n\n\n\nSamatha Csik - Creating Quarto Websites\n\n\n\n\n\nThread by Tom Carpenter (@tcarpenter216) on translating dplyr skills to SQL\nOnline resources for learning SQL, as recommended by Ijeoma Okereafor (@MeetIjeoma)\n\nhttp://sqlbolt.com\nhttp://w3schools.com/sql\nhttp://mode.com/sql-tutorial\nhttp://sqlteaching.com\nhttp://SQLZoo.net\nhttp://selectstarsql.com\n\nhttps://pgexercises.com\n\nSQL games recommended by Vikas Rajputin (@vikasrajputin)\n\n(SQL Island)[https://sql-island.informatik.uni-kl.de/] (In German but chrome translation is pretty good)\n(SQL Murder Mystery)[https://mystery.knightlab.com/]\n(SQL Polic Department)[https://sqlpd.com/]\n\n\n\n\n\n\nJenny Bryan - Naming Things (Slides)\nTalk by David Robinson on “The unreasonable effectiveness of public work”\n\n\n\n\n\nBooks on writing suggested by Helen Sword, author of Stylish Academic Writing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zak Varty",
    "section": "",
    "text": "I am a statistician, data scientist and educator.\nI develop statistical methodology and data science solutions to solve challenging data problems. I am particularly interested in modelling environmental and industrial processes.\nI work as a teaching fellow at Imperial College London. In my work, I lead courses and supervise research projects in statistics, data science and data ethics.\nPreviously, I completed my PhD in Statistics and Operational Research within the STOR-i CDT at Lancaster University. My PhD work combined point process models with techniques from extreme value theory and Bayesian inference to model induced earthquake activity.\nWhen I am not in front of my computer, I enjoy distance running 🏃 and I love to read 📖. Ideally, these activities are accompanied by a good cup of coffee and a large slice of cake.\nFeel free to contact me if you have any questions!"
  },
  {
    "objectID": "blog/2022-09-26-adding-a-quarto-blog/index.html",
    "href": "blog/2022-09-26-adding-a-quarto-blog/index.html",
    "title": "Setting up a quarto blog",
    "section": "",
    "text": "Steps\n\nCreate a subdirectory of the website called blog/. This has sub-folders for each blog post and will contain the files of metadata that are common to all blog posts (e.g. default settings for YAML headers information and a bibliography file). \nCreate a listing page called blog.qmd in the root directory. This will become the blog “landing page” and what we will point to from the website header. \nAdd a “Blog” header item to the _quarto.yml file for the website and set the link: for this to be blog.qmd \nAdded a simple example post to the blog/ directory. See for example my hello-world post. \nAdjust the default YAML parameters for the blog posts by making the file blog/_metadata.yml. These default values can be overwritten by specifying them again in the YAML header at the top of any individual post. For examples of what you might want to include see my file or the projects section of the quarto docs. \nAdd a simple bibliography file, called library.bib or similar to the blog/ directory. Set this as the default bibliography file for each blog post by adding bibliography: ../library.bib to blog/_metadata.yml. \n(optional) Create a post template so that you don’t have to memorise header fields. \nSet your “Hello, World!” and template posts to have draft: true in their headers. This will prevent them from showing up on your website. \nSet your “Hello, World!” and template posts to have freeze: true in their headers. This will prevent any code in them from re-running each time the website is rendered.\n\nFreezing the code within posts will improve the build speed, as well as make the website more stable and portable. See the quarto docs on freezing posts for more details. My current plan is to have this as false by default and change to true on publication of each post.\n\n\nChecking that references work\nI have set up a single bibtex file in which to store references for all posts. This lives in the blog/ directory and is set as the default bibliography parameter for each post in the file blog/_metadata.yml.\nThis is an in-line reference to Wan et al. (2020) written as @citationkey. Parenthetical references, such as (Wan et al. 2020), are written using [@citationkey]. These can be strung together by separating each citation key with a semicolon, for example (Wan et al. 2020, 2020).\nTo let people know the license your work is under and how they should cite your blog posts you can use the appendix-style argument. This can be added to the YAML header of individual blog posts or you can specify a default value in blog/_metadata.yml. There are three options for this parameter:\n\ndefault does some nice formatting and makes the text a bit smaller than the rest of the article;\nplain matches the style of the rest of your post;\nnone does not add any citation details to the end of your post.\n\nI’m currently using some pretty hacky CSS to style this website so am limited to the latter two options for now. In the process of writing this article I stumbled across some neat SCSS that I hope will fix this issue that I have made for myself! [Update: I changed to SCSS and this is now fixed!]\nNote: When adding references to your posts, make sure that the site-URL field in your website’s quarto.yml does not have a trailing slash - this will be copied into the reference and break the links.\n\n\n\n\n\nReferences\n\nWan, Phyllis, Tiandong Wang, Richard A Davis, and Sidney I Resnick. 2020. “Are Extreme Value Estimation Methods Useful for Network Data?” Extremes 23 (1): 171–95.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{varty2022,\n  author = {Zak Varty},\n  title = {Setting up a Quarto Blog},\n  date = {2022-09-26},\n  url = {https://www.zakvarty.com/blog/2022-09-26-adding-a-quarto-blog},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZak Varty. 2022. “Setting up a Quarto Blog.” September 26,\n2022. https://www.zakvarty.com/blog/2022-09-26-adding-a-quarto-blog."
  },
  {
    "objectID": "blog/2022-09-26-hello-world/index.html",
    "href": "blog/2022-09-26-hello-world/index.html",
    "title": "Hello, World!",
    "section": "",
    "text": "Some Code\n\n\nCode\npar(bg = NA)\nplot(\n    x = mtcars$mpg,\n    y = mtcars$cyl,\n    xlab = \"miles per gallon\",\n    ylab = \"cyclider count\",\n    pch = 16)"
  },
  {
    "objectID": "blog/2022-10-10-statistics-books/index.html",
    "href": "blog/2022-10-10-statistics-books/index.html",
    "title": "Recommended Statistics Books",
    "section": "",
    "text": "While teaching a course on supervised learning last year, several students asked about what books I would recommended on statistical inference and modelling.\nFor context, the students on this course are all highly numerate and studying at the postgraduate level. What makes this request challenging is the broad range of student backgrounds, some students had a maths degree but the majority are trained and work as engineers, physicists or computer scientists.\nThis variety in backgrounds and exposure to undergraduate level statistics made recommending a single book difficult. Instead, I compiled a list of books that I have enjoyed or found useful. For each book I tried to give some guidance on whether it might match with their current statistical knowledge and what they are trying to achieve. I gave a brief description of the level and target audience of each text, which I reproduce below.\nWhen evaluating whether these resources might suit your current needs, I find it helpful to skim through a section on a topic that you already know (such as linear regression). This is usually the fastest and most reliable way to assess if the book is going to be a good fit for you.\nThis list is by no means exhaustive. If you know of any gems that I have not included in this list, please do let me know!\n\n\n\n\n\n\nRice (2007) covers the basics of probability and statistics usually contained in the first couple of undergraduate statistics courses. Generally the first university courses are a bit dry, building up the required knowledge to do interesting things. This book is slightly better than the average treatment in terms of readability and is fairly comprehensive, making it well suited as a reference text. This is a book full of all the stuff you might once have known but have now forgotten, or never studied before.\n\n\n\n\n\n\n\n\nKirkwood and Sterne (2010) focuses on more advanced topics in statistics, such as inference, hypothesis testing and modelling. However, it approaches these from an applications perspective. While all of the applications it uses are from medical statistics, the authors give sufficient context that you do not need to be familiar with this area before reading. This is a very readable book, with a moderate amount of mathematical detail. I find myself revisiting it quite often.\n\n\n\n\n\n\n\nWood (2015) gives an introduction to the core topics in statistics aimed at new graduate-level students. It is mathematically dense but written in an approachable manner and (unsurprisingly) covers all the core ideas of statistics. This means that is often a good source to get an overview of a topic and to cover the key points in that area quickly. It is probably wise to supplement this with a more applied text to see worked examples and to a more detailed text for topics that you need to explore in greater detail.\n\n\n\n\n\n\n\n\nPawitan (2001) focuses entirely on likelihood inference and covers both theory and applications in a great deal of detail. I highly recommend this to supplement frequentist topics covered in core statistics and the elements of statistical learning. It builds up from very little assumed knowledge but also goes on to cover some very advanced topics in later chapters.\n \n\n\n\n\n\n\n\nKendall, Stuart, and Ord (1987) is an alternative to In All Likelihood, aimed at a similar audience and level. Split over several volumes this is good to do a deep-dive into a particular topic but probably not one to try and read cover to cover!"
  },
  {
    "objectID": "blog/2022-10-10-statistics-books/index.html#bayesian-statistics",
    "href": "blog/2022-10-10-statistics-books/index.html#bayesian-statistics",
    "title": "Recommended Statistics Books",
    "section": "Bayesian Statistics",
    "text": "Bayesian Statistics\nWe only consider frequentist approaches to inference in this course. However, I would be remiss to not include some Bayesian texts and leave you with the impression that classical or frequentist approaches to statistics are the only option.\nMany of the topics we cover in supervised learning can be considered from a Bayesian perspective. A Bayesian statistician does not treat our model parameters as fixed but unknown quantities, instead they consider the parameters as random variables and use probability distributions to describe their (or our) beliefs about the parameter values.\nYou might find the following books useful, either during or after the Bayesian inference course. The former is more theoretical, while the latter has a more applied focus.\nKendall’s advanced theory of statistics. Vol. 2B, Bayesian inference. (O’Hagan and Forster 2004)\nBayesian Data Analysis - Gelman et al. (Gelman et al. 2013)"
  },
  {
    "objectID": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html",
    "href": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html",
    "title": "Good Enough Practices in Scientific Computing",
    "section": "",
    "text": "wilson2017good\nTitle: Good Enough Practices in Scientific Computing. {PLOS Computational Biology, 2017} (20 pages).\nAuthors: Greg Wilson, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt and Tracy K. Teal.\nKey words: computing, research skills, reproducibilty, guides.\nIn this paper by Wilson et al. (2017), a collection of experienced researchers and instructors give simple ways to implement good computing practices during a research project. They do this by providing a list of concrete recommendations that every researcher can adopt, regardless of their current computational skills. This is important to help the transition toward open, documented and reproducible research. The article is aimed specifically at people who are new to computational research but also contains useful guidance for more experienced researchers."
  },
  {
    "objectID": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#notes",
    "href": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#notes",
    "title": "Good Enough Practices in Scientific Computing",
    "section": "Notes",
    "text": "Notes\nThis article describes some of the best-practices in software development and how those ideas can be implemented in a reasearch project. This focus here is on implementing these approaches without requiring reseachers to learn how to use lots of peripheral technologies (for example git and LaTeX / markdown).\nAn earlier paper “Best Practices for Scientifc Computing” (Wilson et al. 2014), is aimed at those who have or would like to develop such peripheral skills."
  },
  {
    "objectID": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#suggested-best-practices",
    "href": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#suggested-best-practices",
    "title": "Good Enough Practices in Scientific Computing",
    "section": "Suggested Best Practices",
    "text": "Suggested Best Practices\nBest practices are grouped into 6 main themes.\n\n1. Data Management\n\nCreate the data you wish to see in the world\nRaw data should be created in a format that is ammenable to analysis and where multiple tables are used, a unique identifer used to link each record across these tables.\n\n\nKeep it backed up, keep it intact\nThis raw data should be backed up in more than one location and preserved during the analysis (i.e. not directly edited). When cleaning, handling and modelling the data keep a record of all steps used.\n\n\nShare the data\nTo allow your future self (and others) to access and cite your hard won data, submit it to a reputable DOI-issuing repository.\n\n\n\n2. Software\n\nScript files\nStart each script with a brief explanatory comment of its purpose and a description of any dependencies.\nWithin scripts, ruthlessly eliminate duplication. Do this by creating functions for any repeated operations and provide simple examples of how those functions work.\nWhen making functions and variables, give them meaningful names. As rule of thumb: fuctions are verbs, variables are nouns.\nIf you need your script to perform different actions, control this behaviour programmatically rather than by commenting/uncommenting sections of code.\n\n# Uncomment for weekly reports\noutput_dir <- paste0(\"weekly_reports/\",year,\"/\",week_of_year,\"/\")\n# Uncomment for annual reports\n#output_dir <- paste0(\"annual_reports/\",year,\"/\")\n\n\nreport_type = \"weekly\"\nyear = 2022\nweek_of_year = 21\n\nif (report_type == \"weekly\") {\n  output_dir <- paste0(\"weekly_reports/\",year,\"/\",week_of_year,\"/\")\n} else if (report_type == \"annual\") {\n  output_dir <- paste0(\"annual_reports/\",year,\"/\")\n} else {\n  stop(\"report_type should be 'weekly' or 'annual'.\")\n}\n\nSubmit the final code for your research project to to a reputable DOI-issuing repository.\n\n\nExternal Code\nBefore writing your own code, check if someone else got there first. Are there well-maintained software libraries that already do what you need?\nIf so, test the code (extensively!) before relying on it. Keep a record of what you have tested and add to this as you find awkward edge cases.\n\n\n\n3. Collaboration\n\nCollaborating within your team\nCreate a single file called README giving an overview of your project. This should describe aim of the project and how to get started working with the data/code/writing. A good rule of thumb is to write this as though it were for either a new-starter on your team. Future you will thank you!\nCreate a shared to-do list for the project in a file called TODO and decide on how you will communicate during the project. For example, what channels will you use for group meetings, quick questions, assigning tasks and setting deadlines?\n\n\nOpening up to the wider world\nAdd another file called LICENSE giving the licensing information for the project. This says who can use it and for what purposes. No license implies you are keeping all rights and nobody is allowed to reuse or modify the materials. For more information on licenses see choosealicense.com or The Open Source Guide. Consult your company’s legal folks as needed.\nCreate a final file called CITATION letting other people know how they should give proper attribution to your work if they use it.\n\n\n\n4. Project Organisation\nEach project should be self-contained in its own directory (folder) and this directory should be named after the project.\nCreate subdirectories called:\n\ndocs/ for all text documents associated with the project\ndata/raw/ for all raw data and metadata\ndata/derived/ for all data files during cleanup and analysis\nsrc for all code you write as part of this project\nbin for all external code or compiled programs that you use in this project\n\nWhen adding files and subdirectories within this structure, name these to clearly reflect their content or function.\n\n\n5. Tracking Changes\nAs soon as any file is created by a human, back it up in multiple locations. If you make a huge file, then consult your IT folks about how to store and back it up.\nAdd a file called CHANGELOG to the docs subfolder. Use this to track all changes made within the project by all contributers, describing when the changes happened and why they were made.\nKeep these changes as small as possible and share among collaborators frequently to avoid getting out of sync.\nMake a Copy the entire project whenever a significant change has been made.\nBetter yet, use a dedicated version control system such as git if that is a realistic option.\n\n\n6. Manuscripts\nPick one and stick to it within each project. The former has a much lower bar to entry and has most of the benefits of the latter (other than manuscripts being stored in the same place as everything else).\n\nWrite the manuscript using online tools with rich formatting, change tracking and reference management. (e.g. Overleaf, Google Docs)\nWrite the manuscript in plain text format the permits version control (e.g. tex + git or markdown + git)"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html",
    "href": "blog/2022-10-07-rhetorical-precis/index.html",
    "title": "Writing a rhetorical précis",
    "section": "",
    "text": "Photo by Maksym Kaharlytskyi on Unsplash"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#what-is-a-rhetorical-precis",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#what-is-a-rhetorical-precis",
    "title": "Writing a rhetorical précis",
    "section": "What is a rhetorical precis?",
    "text": "What is a rhetorical precis?\n\n\n\nA rhetorical precis is a short summary and analysis of a piece of writing, which considers both the content and the delivery of the piece.\nA rhetorical precis serves to summarise and analyse the text through:\n\nan accurate bibliographic reference to the text,\na list of keywords relating to the text,\na highly structured four-sentence paragraph providing a summary and analysis of the text."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#why-write-one",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#why-write-one",
    "title": "Writing a rhetorical précis",
    "section": "Why write one?",
    "text": "Why write one?\nKeeping a rhetorical precis for each text that you read is a fantasitc way to build the skills of active reading and succinct writing. A rhetorical precis is more informative than a bib entry and more easily reviewed (read: waded through) than a stack of annotated papers.\nTaken collectively, a set of rhetorical precis summaries provide a reading record that can be a tremendously useful when trying to recall the contents of a paper or book long after you originally read it."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#how-to-store-them",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#how-to-store-them",
    "title": "Writing a rhetorical précis",
    "section": "How to store them?",
    "text": "How to store them?\nWriting and storing these reading summaries electronically can make them even more useful. This allows you to search for topics, target audiences or keywords.\nFor this reason it can be helpful to keep them all together in one word document or plain text file. Alternatively, having a single folder with each summary as a plain text or markdown file works well if you are comfortable with searching at the command line. The same can be achieved by writing these summaries within a reference manager, if that is something you are invested in already."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#definition",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#definition",
    "title": "Writing a rhetorical précis",
    "section": "Definition",
    "text": "Definition\nJust to prove that I’m not making all this up:\n\nA rhetorical precis analyzes both the content (the what) and the delivery (the how) of a unit of spoken or written discourse. It is a highly structured four-sentence paragraph blending summary and analysis. Each of the four sentences requires specific information; students are expected to use brief quotations (to convey a sense of the author’s style and tone) and to include a terminal bibliographic reference. Practicing this sort of writing fosters precision in both reading and writing, forcing a writer to employ a variety of sentence structures and to develop a discerning eye for connotative shades of meaning.  Attribution: lumenlearning.com"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#format",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#format",
    "title": "Writing a rhetorical précis",
    "section": "Format",
    "text": "Format\nFour sentences summarising the aim of the work, how this is addressed, why it is important and a description of the target audience.\n\nName of author, [optional phrase describing author], genre and title of work, date in parentheses (additional publishing information in parentheses); a rhetorically accurate verb (such as “asserts,” “argues,” suggests,” “implies,” claims,” etc.); a THAT clause containing the major assertion or thesis statement of the work.\nAn explanation of how the author develops and/or supports the thesis, usually in chronological order.\nA statement of the author’s purpose followed by an “in order to” phrase.\nA description of the intended audience and/or the essay’s tone"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#a-self-indulgent-example",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#a-self-indulgent-example",
    "title": "Writing a rhetorical précis",
    "section": "A (self-indulgent) example",
    "text": "A (self-indulgent) example\nHere is a rather self-indulgent example of a rhetorical precis.\n\nvarty2021inference\nTitle: Inference for extreme earthquake magnitudes accounting for a time-varying measurement process. {ArXiV preprint, 2021} (20 pages).\nAuthors: Zak Varty, Jonathan Tawn, Peter Atkinson and Stijn Bierman.\nKey words: extreme value, earthquake, threshold selection, magnitude of completion, seismology, bootstrap.\nIn this paper, Varty et al (2021) propose a new threshold selection method for modelling earthquake catalogues, where the magnitude distribution is stationary but detection of small events improves over time. The paper generalises the Gutenberg-Richter law to the GPD and uses metrics based on PP and QQ plots to balance between bias and variance when selecting a time-varying threshold. This procedure more than doubles the usable catalogue size for Groningen earthquakes and gives the first emprircal evidence that the magnitude distribution in this region has a finite upper end point. The paper is targeted at applied and research statisticians with an interest in EVT but would also be accessible to a statistically-minded seismologist."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#a-template-for-new-entries",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#a-template-for-new-entries",
    "title": "Writing a rhetorical précis",
    "section": "A template for new entries",
    "text": "A template for new entries\n\nfirstauthorYYYYkeyword\nTitle: Title goes here. {Journal, YYYY} (NN pages).\nAuthors: Author One, Author Two and Author Three. (optional affiliations)\nKey words: key word 1, key word 2, key work 3.\n\nWhat is the document and what does it say?\nHow do they do / show this?\nWhy are they bothering to do this in the first place?\nWho is the intended audience for this work?\n\nIn this DOC_TYPE, AUTHOUR VERB that THESIS_STATEMENT. They DO/SHOW this by ACTIONS. This is important to PEOPLE because REASONS. This work would be useful when PEOPLE are doing ACTIVITY.\n\n`firstauthorYYYYkeyword`\n\n**Title:** _Title goes here. {Journal, YYYY} (NN pages)._\n\n**Authors:** _Author One, Author Two and Author Three. (optional affiliations)_\n\n**Key words:** _key word 1_, _key word 2_, _key work 3_. \n\n1. _What_ is the document and _what_ does it say? \n2. _How_ do they do / show this?\n3. _Why_ are they bothering to do this in the first place?\n4. _Who_ is the intended audience for this work?\n\nIn this DOC_TYPE, AUTHOUR VERB that THESIS\\_STATEMENT.\nThey DO/SHOW this by ACTIONS. \nThis is important to PEOPLE because REASONS. \nThis work would be useful when PEOPLE are doing ACTIVITY."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Zak Varty",
    "section": "",
    "text": "Zak Varty is a statistician, data scientist and educator.\nHe is a Teaching Fellow in Statistics at Imperial College London where he teaches courses and supervises research projects in statistics, data science and data ethics.\nOutside of work, Zak enjoys spending time reading, running and baking tasty treats."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Zak Varty",
    "section": "Education",
    "text": "Education\nPhD Statistics and Operational Research | 2017 - 2021\nTitle: Statistical Modelling of Induced Earthquakes\nInstitution: Lancaster University, UK.\nMRes Statistics and Operational Research | 2016 - 2017\nGrade: Distinction\nInstitution: Lancaster University, UK.\nMSci Mathematics and Statistics (Study Abroad) | 2012 - 2016\nGrade: First Class (Honours)\nInstitution: Lancaster University, UK and University of Western Ontario, CA."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Zak Varty",
    "section": "Experience",
    "text": "Experience\nTeaching Fellow, Imperial College London | Aug 2021 - present\nI teach on the online MSc in Machine Learning and Data Science course, support undergraduate student development, and supervise research projects at the undergraduate and postgraduate level.\nLecturer, Lancaster University | Feb 2021 - Apr 2021\nFixed term lecturship in department of Mathematics and Statistics. I adapted for online delivery and co-delivered a postgraduate course in extreme value theory.\nStatistical Contractor, Shell Global Solutions, NL. | Oct 2019 - Jan 2020\nFour month placement as a statistical contractor. I worked within the statistics and data-science teams on two projects related to corrosion management.\nIntern, Huawei Technologies, CHN. | Jul - Aug 2016\nFully sponsored internship in Beijing and Shenzhen for high performing STEM undergraduates. Immersive education in large-scale ICT infrastructure, intercultural relations and international business management.\nReseach Intern, Lancaster University, UK. | Jul - Sept 2015\nSummer research project within STOR-i CDT, investigating modelling techniques to estimate pharmacokinetic parameters from small data sets."
  },
  {
    "objectID": "about.html#outreach",
    "href": "about.html#outreach",
    "title": "Zak Varty",
    "section": "Outreach",
    "text": "Outreach\nI was an active member of the Royal Statistical Society’s Lancashire & East Cumbria local group from 2017-2020. The group organises seminars and events to bring together those with an interest in data both within and outside of academia. I served as treasurer for the group for two years from 2017-2019.\nI have also been involved in delivering university open days since 2016. At these events I have delivered mini-lectures to promote statistical thinking, raise awareness of available funding, and to promote the option of studying abroad."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "academic skills\n\n\ncomputing\n\n\nreproducible research\n\n\nreading summary\n\n\n\n\nReading Summary of Wilson et al. (2017).\n\n\n\n\n\n\nOct 19, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata visualisation\n\n\n\n\nRecreating a plot of record breaking temperatures by the BBC data journalism team.\n\n\n\n\n\n\nOct 15, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata visualisation\n\n\ntidy tuesday\n\n\n\n\nTidy Tuesday 2022 || Week 41\n\n\n\n\n\n\nOct 12, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nacademic skills\n\n\nreading\n\n\n\n\nBook recommendations for those looking to reinforce their knowledge of undergraduate and advanced statistics.\n\n\n\n\n\n\nOct 10, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nacademic skills\n\n\nwriting\n\n\nreading\n\n\n\n\nStructured summaries to remember and retrieve what you read\n\n\n\n\n\n\nOct 7, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nquarto\n\n\ntutorial\n\n\n\n\nAdding a blog within a quarto website\n\n\n\n\n\n\nSep 26, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nQuarto\n\n\nTemplates\n\n\n\n\nA minimal first post\n\n\n\n\n\n\nSep 26, 2022\n\n\nZak Varty\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2022-10-11-ravelry-tidy-tuesday/index.html",
    "href": "blog/2022-10-11-ravelry-tidy-tuesday/index.html",
    "title": "Tidy Tuesday: Ravelry Yarn",
    "section": "",
    "text": "I have made several plots using {ggplot2} before, but this was my first attempt at making one aesthetically pleasing (forgive the pun).\nWhen making this plot I learned about using custom font, colours, annotations and arrows from a lot of @nrennie’s past examples.\nCode and figure down below ↓\n\n\nCode\n# Load Packages ----\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(showtext)\n\n# Load Fonts ----\nfont_add_google(name = \"Indie Flower\", family = \"indie-flower\")\nfont_add_google(name = \"Permanent Marker\", family = \"marker\")\nshowtext_auto()\n\n# Load Data ----\nurl <- \"https://github.com/rfordatascience/tidytuesday/raw/master/data/2022/2022-10-11/yarn.csv\"\nyarn <- readr::read_csv(file = url)\n\n# Data Handling ----\n\nother_weight_names <- c(\n  \"Thread\",\n  \"Cobweb\",\n  \"Jumbo\",\n  \"DK / Sport\",\n  \"Aran / Worsted\",\n  \"No weight specified\")\n\nyarn_data <- yarn %>%\n  select(yarn_weight_name) %>%\n  mutate(yarn_weight_name = as.character(yarn_weight_name)) %>%\n  mutate_at(c(\"yarn_weight_name\"), ~replace_na(.,\"Missing\")) %>%\n  mutate(name = fct_collapse(yarn_weight_name, Other = other_weight_names)) %>%\n  mutate(name = fct_collapse(name, \"Double Knit\" = c(\"DK\"))) %>%\n  group_by(name) %>%\n  summarise(value = n())\n\n# Helper data frames for adding arrows to plot\narrow_df_1 <- data.frame(x1 = 27000, x2 = 27000, y1 = 7.5, y2 = 10.4)\narrow_df_2 <- data.frame(x1 = 27000, x2 = 19000, y1 = 7.5, y2 = 10)\n\n# Making Plot ----\n\nbar_colour <- \"#483248\"\nbg_colour <- \"#FEFBEA\"\ntitle_font <- \"marker\"\nmain_font <- \"indie-flower\"\n\nyarn_plot <- yarn_data %>%\n  ggplot(aes(y = reorder(name, value), x = value)) +\n  geom_bar(stat = \"identity\", fill = bar_colour) +\n  theme_void() +\n  ggtitle(\" \\n Yarn weights on Ravelry, ordered by frequency\",subtitle = \" \") +\n  theme(axis.title = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        text = element_text(family = main_font),\n        plot.background = element_rect(fill = bg_colour, colour = bg_colour),\n        panel.background = element_rect(fill = bg_colour, colour = bg_colour),\n        plot.title = element_text(family = title_font, size = 22, hjust = 0.5)\n  ) +\n  lims(x = c(0,28000)) +\n  geom_text(aes(label = name, x = 200),\n            color = bg_colour,\n            hjust = 0,\n            family = main_font,\n            size = 5) +\n  geom_text(aes(label = value),\n            hjust = 0,\n            nudge_x = 200,\n            color = bar_colour,\n            family = main_font,\n            size = 6) +\n  geom_text(aes(label = \"The most popular yarn weights \\n  are 'Fingering' and 'Double Knit'\",\n                x = 20000,\n                y = 6.7),\n            family = main_font,\n            size = 7) +\n  geom_text(aes(label = \"There were more missing yarn weights \\n than in all remaining categories combined\",\n                x = 18000,\n                y = 1.6),\n            family = main_font,\n            size  = 7) +\n  geom_text(aes(label = \"}\"),\n            x = 7000,\n            y = 1.5,\n            size = 19,\n            family = main_font) +\n  geom_text(aes(label = \"Tidy Tuesday 11 Oct 2022 | Data: Ravelry |  @zakvarty\"),\n            x = 29000,\n            y = 4.5,\n            size = 5,\n            family = main_font,\n            angle = 270) +\n  geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2),\n             data = arrow_df_1,\n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2),\n             data = arrow_df_2,\n             arrow = arrow(length = unit(0.03, \"npc\")))\n\nyarn_plot\n\n\n\n\n\nCode\n# Exported as 8x8 inch pdf and 800x700 png\n# (next time start by setting canvas size!)"
  },
  {
    "objectID": "blog/15-10-2022-BBC-temperature-plot/index.html",
    "href": "blog/15-10-2022-BBC-temperature-plot/index.html",
    "title": "Data Journalism: Recreating a Professional Plot",
    "section": "",
    "text": "On Friday 2022-10-14, the BBC Data Journalism Team released this excellent article about the record temperatures in the UK during this summer’s heatwave. The article has some amazing data visualisations, and draws on a recent Met Office report.\nI wanted to try and recreate one of the plots to test the limits of my ggplot knowledge. Since I had already tackled a stacked bar plot, I figured I might have a go at their dumbbell plot that shows the weather stations which exceeded their previous records largest margins.\n\n\n\nbbc temperature records dumbbell plot\n\n\nI couldn’t find the data source, so spent far too long with a printed copy of the original figure to make my own version of the data set.\nIt took a while, but I got most of the way there with it and am happy with the final result.\n\n\n\nmy attempt at recreating the same plot\n\n\nThere were a few things that still have me stumped, that I might revisit at some later date:\n\n\nLeft aligning title and caption (Thanks to Jack Davison for this!)\n\nUsing gradients on multiple parts of the plot\n\nUsing the YeOrRd gradient, rather than default blues\n\nAdding a non-BBC logo to the bottom right.\n\nIf anyone with superior ggplot skills would like to help with those or give pointers, then I would be most grateful!\nCode and figure down below ↓\n\n\nCode\n# Load packages ----\nlibrary(bbplot)\nlibrary(tidyverse)\nlibrary(showtext)\n\n# Import fonts ----\nfont_add_google(name = \"Roboto Slab\", family = \"roboto-slab\")\nfont_add_google(name = \"Roboto\", family = \"roboto\")\nshowtext_auto()\ntitle_font <- \"roboto-slab\"\nfont <- \"roboto\"\n\n# Input data (estimated values from article) ---\ntemperatures <- tibble::tribble(\n  ~location, ~max_prev, ~max_2022,\n  \"Cranwell\", 36.6, 39.9,\n  \"Nottingham\", 36.0, 39.8,\n  \"Bramham\", 33.5, 39.8,\n  \"Sutton Boningon\", 35.9, 39.4,\n  \"Sheffield\", 35.6, 39.4,\n  \"Leeming\", 34.4, 38.7,\n  \"Goudhurst\", 34.7, 37.9,\n  \"Whitby\", 33.1, 37.8,\n  \"Bradford\", 33.9, 37.8,\n  \"High Mowthorpe\", 33.1, 37.2,\n  \"Blackpool\", 33.6, 37.2,\n  \"Durham\", 32.9, 36.9,\n  \"Preston\", 33.1, 36.5,\n  \"Morecambe\", 32.7, 36.4,\n  \"Stonyhurst\", 32.6, 36.3,\n  \"Keele\", 32.9, 36.2,\n  \"Bude\", 32.2, 36.1,\n  \"Buxton\", 32.7, 36.0,\n  \"Kielder Castle\", 29.6, 35.0,\n  \"Bala\", 31.9, 34.9\n)\n\n# Data preparation ----\n\n## For the points ----\ntemperatures <- temperatures |>\n  dplyr::mutate(max_ever = pmax(max_2022, max_prev))\n\ntemperatures$location <- forcats::fct_reorder(as.factor(temperatures$location), .x = temperatures$max_ever)\n\ntemp_long <- tidyr::pivot_longer(temperatures, cols = c(max_2022, max_prev), names_to = \"year\",values_to = \"temperature\")\n\n## For the bars ----\nn_interp <- 501\ntemp_interpolated <- tibble(rep(NA, n_interp*20))\ntemp_interpolated[[1]] <- rep(temperatures$location, each = n_interp)\ntemp_interpolated[[2]] <- rep(NA_real_, n_interp*20)\nnames(temp_interpolated) <- c(\"location\", \"interp_value\")\nfor (i in 1:20) {\n  temp_interpolated$interp_value[(1 + n_interp * (i - 1)):(n_interp*i)] <-\n    seq(temperatures$max_prev[i], temperatures$max_2022[i], length.out = n_interp)\n}\n\nstr_wrap_break <- function(x, break_limit) {\n  # Function from {usefunc} by N Rennie (https://github.com/nrennie/usefunc)\n  sapply(strwrap(x, break_limit, simplify = FALSE), paste, collapse = \"\\n\")\n}\n\ntitle_string <- \"Huge breaks from previous records in 2022\"\nsubtitle_string <- str_wrap_break(\"Stations with largest gaps between previous and new records, ordered by highest new temperature\",60)\ncaption_string <- \"Only includes active weather stations with at least 50 years of observations\"\n\n\n\n\n\np <- ggplot() +\n  geom_line(data = temp_interpolated, aes(x = interp_value, y = location, color = interp_value), lwd = 3) +\n  #\n  geom_label(aes(label = \"2022 record\", x = 38.7, y = 7.2), family = font, size = 6.5, label.size = NA) +\n  geom_curve(aes(x = 38, y = 7.9, xend = 36.9, yend = 8.9)) +\n  #\n  geom_text(aes(label = \"Previous \\n record\", x = 31, y = 11), family = font, size = 6.5) +\n  geom_curve(aes(xend = 32, yend = 11, x = 32.9, y = 8.9)) +\n  #\n  geom_label(aes(label = \"Biggest leap\", x = 33.7, y = 20), family = font, size = 6.5, label.size = NA) +\n  geom_label(aes(label = \"6.3C\", x = 34.0, y = 19), family = font, fontface=\"bold\", size = 6.5, label.size = NA) +\n  geom_curve(aes(xend = 34.7, yend = 19.5, x = 35.7, y = 18)) +\n  #\n  geom_point(data = temp_long, aes(x = temperature, y = location, fill = temperature), shape = 21, color = \"black\", size = 6) +\n  #\n  scale_x_continuous(breaks = seq(30, 40, by = 2.5),labels = paste0(seq(30, 40, by = 2.5),\"C\")) +\n  #\n  labs(title = title_string,\n       subtitle = subtitle_string,\n       caption = caption_string) +\n  #\n  theme(plot.title = element_text(family=title_font,\n                                  size=28,\n                                  face=\"bold\",\n                                  color=\"#222222\"),\n        plot.subtitle = element_text(family=font,\n                                     size=18,\n                                     margin=ggplot2::margin(9,0,9,0)),\n        plot.caption = element_text(family = font, size = 14,hjust = 0),\n        plot.title.position = 'plot',\n        plot.caption.position = 'plot',\n        axis.title = ggplot2::element_blank(),\n        axis.text = ggplot2::element_text(family=font,\n                                          size=18,\n                                          color=\"grey47\"),\n        legend.position = \"none\",\n        title = element_text(),\n        axis.line.x = element_line(size = 0.7, linetype = \"solid\"),\n        axis.ticks.y = element_blank(),\n        axis.ticks.length.x = unit(7, units = \"points\" ),\n        axis.text.x = element_text(margin=margin(t = 15, b = 10)),\n        panel.grid.minor = ggplot2::element_blank(),\n        panel.grid.major.y = ggplot2::element_line(color=\"#cbcbcb\"),\n        panel.grid.major.x = ggplot2::element_blank(),\n        panel.background = ggplot2::element_blank(),\n  )\n\np"
  },
  {
    "objectID": "blog/2022-10-15-BBC-temperature-plot/index.html",
    "href": "blog/2022-10-15-BBC-temperature-plot/index.html",
    "title": "Data Journalism: Recreating a Professional Plot",
    "section": "",
    "text": "On Friday 2022-10-14, the BBC Data Journalism Team released this excellent article about the record temperatures in the UK during this summer’s heatwave. The article has some amazing data visualisations, and draws on a recent Met Office report.\nI wanted to try and recreate one of the plots to test the limits of my ggplot knowledge. Since I had already tackled a stacked bar plot, I figured I might have a go at their dumbbell plot that shows the weather stations which exceeded their previous records largest margins.\n\n\n\nbbc temperature records dumbbell plot\n\n\nI couldn’t find the data source, so spent far too long with a printed copy of the original figure to make my own version of the data set.\nIt took a while, but I got most of the way there with it and am happy with the final result.\n\n\n\nmy attempt at recreating the same plot\n\n\nThere were a few things that still have me stumped, that I might revisit at some later date:\n\n\nLeft aligning title and caption (Thanks to Jack Davison for this!)\n\nUsing gradients on multiple parts of the plot\n\nUsing the YeOrRd gradient, rather than default blues\n\nAdding a non-BBC logo to the bottom right.\n\nIf anyone with superior ggplot skills would like to help with those or give pointers, then I would be most grateful!\nCode and figure down below ↓\n\n\nCode\n# Load packages ----\nlibrary(bbplot)\nlibrary(tidyverse)\nlibrary(showtext)\n\n# Import fonts ----\nfont_add_google(name = \"Roboto Slab\", family = \"roboto-slab\")\nfont_add_google(name = \"Roboto\", family = \"roboto\")\nshowtext_auto()\ntitle_font <- \"roboto-slab\"\nfont <- \"roboto\"\n\n# Input data (estimated values from article) ---\ntemperatures <- tibble::tribble(\n  ~location, ~max_prev, ~max_2022,\n  \"Cranwell\", 36.6, 39.9,\n  \"Nottingham\", 36.0, 39.8,\n  \"Bramham\", 33.5, 39.8,\n  \"Sutton Boningon\", 35.9, 39.4,\n  \"Sheffield\", 35.6, 39.4,\n  \"Leeming\", 34.4, 38.7,\n  \"Goudhurst\", 34.7, 37.9,\n  \"Whitby\", 33.1, 37.8,\n  \"Bradford\", 33.9, 37.8,\n  \"High Mowthorpe\", 33.1, 37.2,\n  \"Blackpool\", 33.6, 37.2,\n  \"Durham\", 32.9, 36.9,\n  \"Preston\", 33.1, 36.5,\n  \"Morecambe\", 32.7, 36.4,\n  \"Stonyhurst\", 32.6, 36.3,\n  \"Keele\", 32.9, 36.2,\n  \"Bude\", 32.2, 36.1,\n  \"Buxton\", 32.7, 36.0,\n  \"Kielder Castle\", 29.6, 35.0,\n  \"Bala\", 31.9, 34.9\n)\n\n# Data preparation ----\n\n## For the points ----\ntemperatures <- temperatures |>\n  dplyr::mutate(max_ever = pmax(max_2022, max_prev))\n\ntemperatures$location <- forcats::fct_reorder(as.factor(temperatures$location), .x = temperatures$max_ever)\n\ntemp_long <- tidyr::pivot_longer(temperatures, cols = c(max_2022, max_prev), names_to = \"year\",values_to = \"temperature\")\n\n## For the bars ----\nn_interp <- 501\ntemp_interpolated <- tibble(rep(NA, n_interp*20))\ntemp_interpolated[[1]] <- rep(temperatures$location, each = n_interp)\ntemp_interpolated[[2]] <- rep(NA_real_, n_interp*20)\nnames(temp_interpolated) <- c(\"location\", \"interp_value\")\nfor (i in 1:20) {\n  temp_interpolated$interp_value[(1 + n_interp * (i - 1)):(n_interp*i)] <-\n    seq(temperatures$max_prev[i], temperatures$max_2022[i], length.out = n_interp)\n}\n\nstr_wrap_break <- function(x, break_limit) {\n  # Function from {usefunc} by N Rennie (https://github.com/nrennie/usefunc)\n  sapply(strwrap(x, break_limit, simplify = FALSE), paste, collapse = \"\\n\")\n}\n\ntitle_string <- \"Huge breaks from previous records in 2022\"\nsubtitle_string <- str_wrap_break(\"Stations with largest gaps between previous and new records, ordered by highest new temperature\",60)\ncaption_string <- \"Only includes active weather stations with at least 50 years of observations\"\n\n\n\n\n\np <- ggplot() +\n  geom_line(data = temp_interpolated, aes(x = interp_value, y = location, color = interp_value), lwd = 3) +\n  #\n  geom_label(aes(label = \"2022 record\", x = 38.7, y = 7.2), family = font, size = 6.5, label.size = NA) +\n  geom_curve(aes(x = 38, y = 7.9, xend = 36.9, yend = 8.9)) +\n  #\n  geom_text(aes(label = \"Previous \\n record\", x = 31, y = 11), family = font, size = 6.5) +\n  geom_curve(aes(xend = 32, yend = 11, x = 32.9, y = 8.9)) +\n  #\n  geom_label(aes(label = \"Biggest leap\", x = 33.7, y = 20), family = font, size = 6.5, label.size = NA) +\n  geom_label(aes(label = \"6.3C\", x = 34.0, y = 19), family = font, fontface=\"bold\", size = 6.5, label.size = NA) +\n  geom_curve(aes(xend = 34.7, yend = 19.5, x = 35.7, y = 18)) +\n  #\n  geom_point(data = temp_long, aes(x = temperature, y = location, fill = temperature), shape = 21, color = \"black\", size = 6) +\n  #\n  scale_x_continuous(breaks = seq(30, 40, by = 2.5),labels = paste0(seq(30, 40, by = 2.5),\"C\")) +\n  #\n  labs(title = title_string,\n       subtitle = subtitle_string,\n       caption = caption_string) +\n  #\n  theme(plot.title = element_text(family=title_font,\n                                  size=28,\n                                  face=\"bold\",\n                                  color=\"#222222\"),\n        plot.subtitle = element_text(family=font,\n                                     size=18,\n                                     margin=ggplot2::margin(9,0,9,0)),\n        plot.caption = element_text(family = font, size = 14,hjust = 0),\n        plot.title.position = 'plot',\n        plot.caption.position = 'plot',\n        axis.title = ggplot2::element_blank(),\n        axis.text = ggplot2::element_text(family=font,\n                                          size=18,\n                                          color=\"grey47\"),\n        legend.position = \"none\",\n        title = element_text(),\n        axis.line.x = element_line(size = 0.7, linetype = \"solid\"),\n        axis.ticks.y = element_blank(),\n        axis.ticks.length.x = unit(7, units = \"points\" ),\n        axis.text.x = element_text(margin=margin(t = 15, b = 10)),\n        panel.grid.minor = ggplot2::element_blank(),\n        panel.grid.major.y = ggplot2::element_line(color=\"#cbcbcb\"),\n        panel.grid.major.x = ggplot2::element_blank(),\n        panel.background = ggplot2::element_blank(),\n  )\n\np"
  }
]