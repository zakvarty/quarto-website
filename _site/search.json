[
  {
    "objectID": "personal/reading-record.html",
    "href": "personal/reading-record.html",
    "title": "Zak Varty",
    "section": "",
    "text": "This is where I record the books I read each year and the books I might like to read in the future.\n\n\n\nStatistical Analysis of Spatial Point Patterns - Diggle\nKlara and The Sun - Kazuo Ishiguro\nThe Goldfinch - Donna Tartt\n\n\n\n\n\nHow to do nothing - Jenny Odell\nHer Body and Other Parties - Carmen Maria Machado\nSomething wicked this way comes - Ray Bradbury\nThe Hole - Hye-Young Pyun, Sora Kim-Russell (Translator)\nGender swapped fairytales - Karrie Fransman & Jonathan Plackett\nLuster - Rven Leilani\nIn the house in the dark of the woods - Laird Hunt\nMexican Gothic - Silvia Moreno-Garcia\n\n\n\n\n\nMeanwhile in Dopamine City - DBC Pierre\nTyll - Daniel Kemlmann\nHow to win friends and influence people - Dale Carnegie\nThe Mr Porter guide to a better day\nProfessor Higgins’s Problem Collection - Peter M Higgins\nThe discomfort of Evening - Marieke Lucas Rijneveld\nBlood and guts in high school - Kathy Acker\nThe Nickel Boys - Colson Whitehead\nThe Need - Helen Phillips\nThe silence of the girls - Pat Barker\nThe ginger child - Patrick Flanery\nBonjour Tristesse - Francois Sagan\nThe Outsiders - S. E. Hinton\nMicro Avdentures - Alastair Humphreys\nDecline and Fall - Evelyn Waugh\n\n\n\n\n\nThe Virgin Suicides - Jeffrey Eugenides\nData Science Ethics - David Martens\nWeapons of Math Destruction - Cathy O’Neil\nEither/Or - Elif Batuman\nGoodbye, things - Fumio Sasaki\nFour thousand weeks - Oliver Burkeman\nMy Year of Rest and Relaxation - Ottessa Moshfegh\nEarthlings - Sayaka Murata\nThe Elements of Academic Style - Eric Hayot\n…\n\n\n\n\n\nThe Secret History - Donna Tartt\nFahrenheit 451 - Ray Bradbury\nDune Messiah - Frank Herbert\nOn Earth We Are Briefly Gorgeous - Ocean Vuong\nWow, no thank you - Samantha Irby\nMath without numbers - Milo Beckman\nThe Penelopiad - Margaret Atwood\nIntroduction to Statistical Learning\nBoy Parts - Eliza Clark\nHow to take smart notes - Sonke Ahrens\nIf we were villians - M. L. Rio\nThe Windup Bird Chronicle, Book 1- Haruki Murakami\nThe Windup Bird Chronicle, Book 2 - Haruki Murakami\nThe Windup Bird Chronicle, Book 3 - Haruki Murakami\nTreacle Walker - Alan Garner\nImmune - Philipp Dettmer (DNF)\nThe Beauty of Everyday Things - Seotsu Yanagi\nBetter Data Visualizations - Jonathan Schwabish\nScientifically Speaking - Jo Filshie Browning\nThe Effect - Nick Huntington-Klein\n…\n\n\n\n\n\nThe authenticity project - Clare Pooley ♣️\nDune - Frank Herbert\nHow to make the world add up: Ten rules for thinking differently about numbers - Tim Harford\nThe Thursday murder club - Richard Osman ♣️\nThe Testaments - Margaret Atwood\nConversations with friends - Sally Rooney\nThe housekeeper and the professor - by Yōko Ogawa, Stephen Snyder (Translator)\nThe dice man - Luke Rhinehart ♣️\nIntroduction to statistical modelling of extreme values - Stuart Coles\nStories of the law and how it is broken - The Secret Barrister\nCollege Science Teaching - Terry McGlynn (2021-04-07)\nIn the dream house - Carmen Maria Machado ♣️\nRadio Silence - Alice Oseman\nYour House is on Fire, Your Children All Gone - Stefan Kiesbye\nNick and Charlie - Alice Oseman\nSolaris - Stanisław Lem ♣️\nShow your work - Austin Kleon\nCirce - Madeline Miller (2021-06-19)\nConvenience Store Woman - Sayaka Murata\nOn being different - Merle Miller\nEating Animals - Jonathan Saffran Foer\nPiranesi - Susan Clarke\nHappy git with R - Jenny Bryan\nThe inheritance of loss - Kiran Desai ♣️\nWhy I am no longer talking (to white people) about race - Reni Eddo-Lodge\nReputation - Lex Croucher ♣️ (2021-08-24)\nThe Ethical Algorithm - Michael Kearns and Aaron Roth (2021-09-02)\nEverything I never told you - Celeste Ng\nInvisible Women - Caroline Criado Perez\nDigital Minimalism - Cal Newport\nThe Best of Me - David Sedaris\nWicked - Gregory Maguire\n\n\n\n\n\nWhat I talk about when I talk about running - Haruki Murakami\nGuys knit - Sockmatician\nBrit(ish) - Afua Hirsch\nSemicolon - Cecilia Watson\nThe conscious closet - Elizabeth L. Cline\nA little book of language - David Crystal\nHardboiled wonderland and the end of the world - Haruki Murakami\nThe Topeka school - Ben Lerner\nNormal people - Sally Rooney\nShow your work - Austin Kleon\n1Q84 (Book 1) - Haruki Murakami\n1Q84 (Book 2) - Haruki Murakami\n1Q84 (Book 3) - Haruki Murakami\nThe body - Bill Bryson\nYou too can have a body like mine - Alexandra Kleeman\nI’m thinking of ending things - Iain Reed\nSupper club - Lara Williams\nGirl, woman, other - Bernadine Evaristo ♣️\nLolita - Vladimir Nabokov ♣️\nSeverance - Ling Ma\nThe school of life - Alain de Botton\n…\n\n\n\n\n\nHave you eaten Grandma? - Giles Brandreth\nAn edited life - Anna Newton\nWeight expectations - Dave Chawner\nSpirits of the season - Tanya Kirk\nKilling Comendatore - Haruki Murakami\nHappy ever after - Paul Dolan\nDress your family in corduroy and denim - David Sedaris\nHear the wind sing - Haruki Murakami\nPinball 1973 - Haruki Murakami\nCall me by your name - Andre Aciman\nThe visual display of quantitative information - Edward R. Tufte\nThe elephant vanishes - Haruki Murakami\nA brave new world - Aldous Huxley\nThe missing piece - Shell Silverstein\nThe joy of work - Bruce Daisley\nJoined up writing -Roger McGough\nThe idiot - Elif Batuman\nThe slow professor - Maggie Berg & Barbara Seeber\nThe history boys - Alan Bennett\nThe handmaid’s tale - Margaret Atwood\nClassical mechanics - Leonard Susskind & George Hrabovsky\nIndistractable - Nir Eyal\nAre you experienced - William Sutcliffe\nCalypso - David Sedaris\nNo one is too small to make a difference - Greta Thunberg\n\n\n\n\n\nBridget Jones’ diary - Helen Fielding\nThe mathematics of Christmas - Hannah Fry\nCall me by your name - Andre Aciman\nTalking to my daughter about the economy - Yanis Varoufakis\nIrresistable - Adam Alter\nSympathy - Olivia Sudjic\nMoranifesto - Caitlin Moran\niGen - Jean Twenge\nHagseed - Margaret Atwood\nNorse Mythology - Neil Gaiman\nUpper North Tynedale - Beryl Charlton\nTalk like TED - Carmine Gallo\nThe boy on the Bridge - Mike Carey\nMen without women - Haruki Murakami\nSouth of the border, West of the sun - Haruki Murakami\nQueer city - Peter Ackroyd\nLess - Andrew Sean Greer\nWhy the Dutch are different - Ben Coates\nRest - Alex Soojung-Kim Pang\nWicked - Gregory Maguire\nWomen and power - Mary Beard\nGirls will be girls - Emer O’Toole\nThe boy in the striped pajamas - John Boyne\n:heavy_multiplication_x:\n\n\n\n\n\nThe throwback - Tom Sharpe\nSeven brief lessons on physics - Carlo Rovelli\nThe Circle - Dave Eggers\nFive on brexit island - Bruno Vincent\nYou too can have a body like mine - Alexandra Kleeman\nThe wish - Bill Griffin\nHappy - Derren Brown\nA little gay history - R.B. Parkinson\nNina is not OK - Shappi Khorsandi\nThe life-changing magic of tidying - Marie Kondo\nThe vegetarian - Han Kang\nRespectable - Lyndsey Hanley\nThe great indoors - Ben Highmore\nSolitude - Michael Harris\nIcecream for breakfast - Laure Jane Williams\nAmerican Gods - Neil Gaiman\nBossy pants - Tina Fey\nQuiet - Susan Cain\nHarry Potter and the philospoher’s stone - J.K. Rowling\nHarry Potter and the chamber of secrets - J.K. Rowling\nGerald’s game - Stephen King\nThe handmaid’s tale - Margaret Atwood\nNot working - Lisa Owens\nKafka on the Shore - Haruki Murakami"
  },
  {
    "objectID": "professional/teaching.html",
    "href": "professional/teaching.html",
    "title": "Zak Varty",
    "section": "",
    "text": "I am fortunate to have had the opportunity to teach in a variety of roles. These have included:\n\nDeveloping and teaching a number of modules in statistics, data science and data ethics. These were predominantly at the postgradute-level and include courses designed for in-person and remote learning.\nSupervising undergraduate, postgraduate and doctoral research projects.\nAdapting and leading short courses on scientific writing and communication.\nRunning workshops and computer labs for undergraduate and postgraduate modules.\nSpeaking at univerisity open days and providing one-to-one tuition to high school students.\n\nI am an associate fellow of the Higher Education Academy, which you can learn more about here. I am currently working toward full fellowship to further develop my teaching practice.\n\n\n\n\n\nYear\nCourse\nRole\n\n\n\n\n2022-23\nData Science\nLecturer\n\n\n\nEthics in Data Science I, II and III\nLecturer\n\n\n\nData Ethics for Digital Chemistry\nLecturer\n\n\n\nY1 research projects: point process models\nLecturer\n\n\n2021-22\nSupervised Learning\nLecturer\n\n\n\nEthics in Data Science I\nLecturer\n\n\n\nEthics in Data Science II\nLecturer\n\n\n\nData Ethics for Digital Chemistry\nLecturer\n\n\n\nY1 research projects: point process models\nLecturer\n\n\n—\n—\n—\n\n\n2020-21\nMATH562/582: Extreme Value Theory\nLecturer\n\n\n\nMATH331: Bayesian Inference\nGraduate teaching assistant\n\n\n\nMATH330: Likelihood Inference\nGraduate teaching assistant\n\n\n2019-20\nDSCI485: Introduction to LaTeX\nCo-leading short course\n\n\n\nMATH566: Longitudinal Data Analysis\nGraduate teaching assistant\n\n\n2018-19\nSTOR-i Internship: Introduction to LaTeX\nCo-leading short course\n\n\n\nDSCI485: Introduction to LaTeX\nCo-leading short course\n\n\n\nMATH562: Extreme Value Theory\nGraduate teaching assistant\n\n\n\nMATH235: Statistics II\nGraduate teaching assistant\n\n\n\nMATH240: Project Skills\nGraduate teaching assistant\n\n\n\nMATH330: Likelihood Inference\nGraduate teaching assistant\n\n\n\nMATH230: Probability II\nGraduate teaching assistant\n\n\n2017-18\nSTOR-i Internship: Research Project\nSupervisor\n\n\n\nSTOR-i Internship: Introduction to LaTeX\nCo-leading short course\n\n\n\nDSCI485: Introduction to LaTeX\nCo-leading short course\n\n\n\nMATH235: Statistics II\nGraduate teaching assistant\n\n\n\nMATH465: Bayesian Inference\nGraduate teaching assistant\n\n\n\nMATH330: Likelihood Inference\nGraduate teaching assistant\n\n\n\nMATH230: Probability II\nGraduate teaching assistant\n\n\n2015-16\nLAB100: Introduction to R\nGraduate teaching assistant\n\n\n2011-13\nGCSE & A-level Mathematics\nPrivate tutor"
  },
  {
    "objectID": "professional/research.html",
    "href": "professional/research.html",
    "title": "Zak Varty",
    "section": "",
    "text": "My research interests lie in the intersection between applied and methodological statistics.\nI am interested in modelling the rich, complex data sets that arise from environmental and industrial processes. I have a particular interest in developing efficient inference methods for non-standard data generating mechanisms. My past projects focused on missing data, predictive analytics, extreme value analysis and point process modelling.\n\n\n\nMy research focused on developing statistical models to describe and understand earthquakes that are caused by gas extraction in the Netherlands.\nHuman-induced earthquakes are usually smaller in magnitude and fewer in number than their tectonic counterparts. This low-data setting is a challenge to statistical modelling and necessitates the inclusion of domain-expert knowledge. If this low-data setting was not challenging enough, changes in gas extraction and earthquake detection lead to further complications and inefficiencies if standard modelling approaches are used.\nMy PhD research developed statistical methodology to make most efficient use of the limited available data and extended existing earthquake models to improve understanding of these induced seismic events.\nYou can find a copy of my thesis on github.\n\n\n\n\n\nVarty, Z., Tawn J.A., Atkinson P.M. and Bierman S. (2021). Inference for extreme earthquake mangitudes accounting for a time-varying measurement process. (Submitted, preprint on arXiv)\n\n\n\n\n\n\n\n\n\n\n\nDate\nEvent\nLocation\n\n\n\n\nSep 2022\nRoyal Statistical Society conference\nAberdeen, UK.\n\n\nJun 2022\nM_max workshop\nAmsterdam, NL.\n\n\nJan 2021\nCRG Extremes workshop\nRemote.\n\n\nMay 2020\nSTOR-i time-series and spatial statistics workshop\nRemote.\n\n\nSept 2019\nInterfaces in extreme value theory workshop\nLancaster, UK.\n\n\nSept 2019\nRoyal Statistical Society conference\nBelfast, UK.\n\n\nAug 2019\nInternational statistical seismology workshop (StatSei11)\nHakone, JPN.\n\n\nJul 2019\nGRASPA (Italian Environmetics Society)\nPescara, IT.\n\n\nJan 2019\nSTOR-i annual conference\nLancaster, UK.\n\n\nJan 2018\nSTOR-i annual conference\nLancaster, UK.\n\n\n\n\n\n\n\nA review of simulated annealing techniques: Simulated annealing is a metahuristic technique mainly used for combinatorial optimisation. Applications, parallelisation and extensions of the technique were reviewed.\nInference on censored networks: Networks are censored when existing nodes or edges are not observed. Methods for inference under different types of missingness were explored. Master’s project supervised by Dr. Christopher Nemeth.\nComputionally intensive methods for modelling houshold epidemics: Approximate Bayesian Computation was utilised to allow inference on disease models with intractable likelihoods. Master’s dissertation supervised by Prof. Peter Neal."
  },
  {
    "objectID": "professional/talks.html",
    "href": "professional/talks.html",
    "title": "Zak Varty",
    "section": "",
    "text": "Title: Incorporating Ethics into the Data Science Curriculum\nSlides \nAbstract:\nData-driven decision making is now pervasive and impacts us all. Your data is used by others to make decisions about who you are, how you will behave, and what options should be made available to you. Predictive models are used to decide anything from the promotion that is offered to you by a retailer through to whether your loan application is granted by a bank.\nThe ways in which these predictive models can fail mathematically form a core part of the training for an aspiring statistician or data scientist. In contrast, the potential for ethical failures in these same models is rarely covered in-depth during as part of this initial training. As a result, these modes of failure are often not considered until those predictive models have been put into production and are actively causing harm. We argue that to prevent this harm, the ethical impacts of using data to make decisions must be made core to the curriculum of both statistics and data science.\nThis talk will describe how this may be done in a way that is appealing to an audience with a strong mathematical focus and that does not require the authoring of extended essays or moral treaties. The discussion is structured around the development of a post-graduate course in the Ethics of Data Science, but the core ideas are salient to all statistics training. Throughout, we give actionable ways in which these topics may be integrated into statistical training at all levels.\n\n\n\n\n\nSlides from talks will be added here."
  },
  {
    "objectID": "professional/bookmarks.html",
    "href": "professional/bookmarks.html",
    "title": "Zak Varty",
    "section": "",
    "text": "Thread by Sarah Sheffield (@sarahsheffield) advice for new faculty.\nCourse Free 10 week course on google drive about writing academic an academic syllabus. Course by @DLabree, recommended by Elena Aydarova (@aydarova).\nTweet by Gero Grams (@GeroGrams) on how to say no to things\nThread by Mine Dogucu (@MineDogucu) on learning/teaching (with) R. See also:\n\nCompilation of free R resources\nBayesian inference book\nTeaching webpage\nBlog on data pedagogy\n\nThread by Wes Kao (@wes_kao) on managing up\nThread by Vrinda Nair (@VnVrinda) on 22 tools for your PhD Journey\nManaging research careers tool by Edinburgh University Thread and Webpage\nTalk by Laura Albert (@lauraalbertphd) on time management: Do less, Do it faster, Do it at the right time.\n\n\n\n\n\nThread by Matt Betts on 8 questions to consider before starting a PhD.\nThread by Maram Duncan (@MCDuncanLab) on the hidden curriculum for new grad students\nThread by @LifeAfterMyPhD on 5 low-stakes steps to set yourself up for an industry job search\nThread by @mathladyhazel on the best math books for self-learners\nThread by Alex Eble (@alexeble) on adivce for thriving in a PhD. Full document here. Note: has US and economics focus, but translates well.\nWebsite Stats Notes in the British Medical Journal. Like a dictionary but for stats words and methods.\nBook Esstential Math for Data Scienceby Thomas Nield. Recommended by Vicki Boykis for those tooking for an intro/refresher on linear algebra, probability and statistics. ### Agent based modelling\nPaper review of agent based model (preprint of JEL - chunky at 90 pages!)\n\n\n\n\n\nVideo Lectures by Steven Strogatz (@stevenstrogatz) on asymptotics and pertubation methods.\n\n\n\n\n\nLecture Notes by Matt Blackwell, “Causal Inference with Applications”\nPaper The taboo against explicit causal inference in nonexperimental psychology. Suggested by Brian Nosel (@BrianNosek)\nThread by Volodymyr Kuleshov (@volokuleshov) about the ICML 2022 tutorial on Causality and Fairness\nVideo Science before statistics: Causal Inference by Richard McElreath (3 hour crash course in causal inference)\nVideo Series Statistical Rethinking (2022) by Richard McElreath on Youtube\n\n\n\n\n\nArticle on setting up a private .gitignore to keep a clean codebase\nBook / website on package development in Python. (Think Hadley & Bryan’s R packages but for Python) Recommended by @EmilyRiederer\nBook The missing readme by Chris Riccomini and Dmitriy Ryaboy\nPaper Ten Simple Rules for Taking Advantage of Git and GitHub\nSildes by Ariel Muldoon (@aosmith16) on “More git and github - collaborators, merege conflicts and pull requests”\n\n\n\n\n\nThread by Alex Gold (@alexkgold) on getting started with Docker. Free online book\n\n\n\n\n\nArticle by architecture notes (@arcnotes) on “Things you should know about databases”\n\n\n\n\n\nThread by R Ladies - Sources of Messy(ish) data\n\n\n\n\n\nThe Verge - AI Drug development maske chemical weapons\nNature - AI Drug development maske chemical weapons\nRichard McElreath recommendation of paper by Xiao-Li Meng on how data quality influences effective sample size\nThread by Santiago (@svpino) on imbalanced datasets\nTweet by Adam Kruchten (@AdamKruchten) on when we care about marginal or conditional effects.\nASA article on the 2020 work and salary survey, showing women tend to earn less in base salary and total income but that in a regression gender is not significant predictor of total income.\nNPR story Where Google find that men are underpaid\nThread by Paul Hunermund (@PHunermund) on the above google article.\nPreprint on reconstructing large portions of training data from a trained neural network\nPaper Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. Suggested by Abeba Birhane (@Abebab)\nPaper Big data loses to good data. Unrepresentative big surveys significantly overestimated US vaccine uptake.\nPaper On extending LinearSHAP, TreeSHAP and DeepSHAP to RKHS-SHAP. Found through tweet by Siu Lun Chau (@Chau9991).\nChapter 33 on interpretability of book Probabilistic Machine Learning: Advacned Topicsby Kevin Murphy (@sirbayes), Been Kim (@_beenkim) and others.\nBook by Claire McKay Bowen “Protecting your privacy in a data-driven world”\nTweet by Rasha Shrain (@rashaben) requesting reading materials on p-values and p-hacking\nCourse 12 week reading course on Ehics and Data Science by Rohan Alexander\n\n\n\n\n\nThread by Indrajeet Patil (@patilindrajeets) on the effective use of colours in data vis.\nR Package {performance} for aesthetically pleasing ggplot sytle diagnostic plots. (The qq plot even has tolerance intervals!)\nBlog Post by Thomas Mock on Creating and using custom ggplot2 themes\nBlog Posts by Ameila McNamara (@AmeliaMN) about Histograms and Kernel Density Estimation\n\n\n\n\n\nCourse 10-week reading list on “History of Statistics and Data Sciences” by Rohan Alexander\n\n\n\n\n\nTweet by Steve Bauman (@realstevebauman) pointing out that Github’s markdown now supports note and warning blockquotes\nTweet by Zev Ross on using the character ├ to get aesthetically pleasing subsections in RStudio\n\n\n\n\n\nStatus code 400 meme by @da_667\nThe binary search tree actually exists by Ahmad Awais (@MrAhmadAwais)\nData Science Dinosaur A computer science python eating a statistics elephant\n\n\n\n\n\nThread by Carl Bergstrom (@CT_Bergstrom) and Ryan McGee (@RS_McGee) telling the story of a paper using a comic strip and stick-figure Darwin.\nThread by Tessa Davis on slide design to keep your audience engaged\nThread by Dorsa Amir (@DorsaAmir) on slide design\nWebsite OpenPeeps - Open Source hand drawn individual characters\nBlog Post by Kate Jolly (@katejolly6) on designing slides in xaringan with xaringanthemer and css.\n\n\n\n\n\nBlog Post on double descent in neural network performance ### Optimisation\nVideo by Trefor Bazett (@TreforBazett) on using Lagrange multipliers to solve constrained optimisation problems\nVideo Series by @3blue1brown on constrained optimisation (hosted on khan academy)\n\n\n\n\n\nThread by Pau Labarta Bajo (@paulabartabajo_)\n\n\n\n\n\nCourse Material By Rick Schoenberg on point process models\nBlog Post by Benjamin Cretois on fitting point process models in stan.\n\n\n\n\n\nTweet by Francisco Yirá (@francisco_yira) about designing a personal learning plan.\n\n\n\n\n\nSamatha Csik - Creating Quarto Websites\n\n\n\n\n\nThread by Tom Carpenter (@tcarpenter216) on translating dplyr skills to SQL\nOnline resources for learning SQL, as recommended by Ijeoma Okereafor (@MeetIjeoma)\n\nhttp://sqlbolt.com\nhttp://w3schools.com/sql\nhttp://mode.com/sql-tutorial\nhttp://sqlteaching.com\nhttp://SQLZoo.net\nhttp://selectstarsql.com\n\nhttps://pgexercises.com\n\nSQL games recommended by Vikas Rajputin (@vikasrajputin)\n\n(SQL Island)[https://sql-island.informatik.uni-kl.de/] (In German but chrome translation is pretty good)\n(SQL Murder Mystery)[https://mystery.knightlab.com/]\n(SQL Polic Department)[https://sqlpd.com/]\n\n\n\n\n\n\nJenny Bryan - Naming Things (Slides)\nTalk by David Robinson on “The unreasonable effectiveness of public work”\n\n\n\n\n\nBooks on writing suggested by Helen Sword, author of Stylish Academic Writing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zak Varty",
    "section": "",
    "text": "I am a statistician, data scientist and educator.\nI develop statistical methodology and data science solutions to solve challenging data problems. I am particularly interested in modelling environmental and industrial processes.\nI work as a teaching fellow at Imperial College London. In my work, I lead courses and supervise research projects in statistics, data science and data ethics.\nPreviously, I completed my PhD in Statistics and Operational Research within the STOR-i CDT at Lancaster University. My PhD work combined point process models with techniques from extreme value theory and Bayesian inference to model induced earthquake activity.\nWhen I am not in front of my computer, I enjoy distance running 🏃 and I love to read 📖. Ideally, these activities are accompanied by a good cup of coffee and a large slice of cake.\nFeel free to contact me if you have any questions!"
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html",
    "href": "blog/2022-12-14-apis-and-httr/index.html",
    "title": "Aquiring Data via an API",
    "section": "",
    "text": "You can’t always rely on tidy, tabular data to land on your desk. Sometimes you are going to have to go out and gather data for yourself.\nI’m not suggesting you will need to do this manually, but you will likely need to get data from the internet that’s been made publicly or privately available to you.\nThis might be information from a webpage that you gather yourself, or data shared with you by a collaborator using an API.\nIn this second blog post we will cover the basics of obtaining data via an API. This material draws together the Introduction to APIs book by Brian Cooksey and the DIY web data section of STAT545 at the University of British Columbia."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#why-do-i-need-to-know-about-apis",
    "href": "blog/2022-12-14-apis-and-httr/index.html#why-do-i-need-to-know-about-apis",
    "title": "Aquiring Data via an API",
    "section": "Why do I need to know about APIs?",
    "text": "Why do I need to know about APIs?\n\nAn API, or application programming interface, is a set of rules that allows different software applications to communicate with each other.\n\nAs a data scientist, you will often need to access data that is stored on remote servers or in cloud-based services. APIs provide a convenient way for data scientists to programmatically retrieve this data, without having to manually download data sets or and process them locally on their own computer.\nThis has multiple benefits including automation and standardisation of data sharing.\n\nAutomation: It is much faster for a machine to process a data request than a human. Having a machine handling data requests also scales much better as either the number or the complexity of data requests grows. Additionally, there is a lower risk of introducing human error. For example, a human might accidentally share the wrong data, which can have serious legal repercussions.\nStandardisation: Having a machine process data requests requires the format of these requests and the associated responses to be standardised. This allows data sharing and retrieval to become a reproducible and programmatic aspect of our work."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#what-is-an-api",
    "href": "blog/2022-12-14-apis-and-httr/index.html#what-is-an-api",
    "title": "Aquiring Data via an API",
    "section": "What is an API?",
    "text": "What is an API?\nSo then, if APIs are so great, what exactly are they?\nIn human-to-human communication, the set of rules governing acceptable behaviour is known as etiquette. Depending on when or where you live, social etiquette can be rather strict. The rules for computer-to-computer communication take this to a whole new level, because with machines there can be no room left for interpretation.\nThe set of rules governing interactions between computers or programmes is known as a protocol.\nAPIs provide a standard protocol for different programs to interact with one another. This makes it easier for developers to build complex systems by leveraging the functionality of existing services and platforms. The benefits of working in a standardised and modular way apply equally well to sharing data as they do to writing code or organising files.\nThere are two sides to communication and when machines communicate these are known as the server and the client.\n\n\n\nServers can seem intimidating, because unlike your laptop or mobile phone they don’t have their own input and output devices; they have no keyboard, no monitor, and no a mouse. Despite this, servers are just regular computers that are designed to store data and run programmes. Servers don’t have their own input or output devices because they are intended to be used remotely, via another computer. There is no need for a screen or a mouse if the user is miles away. Nothing scary going on here!\nPeople often find clients much less intimidating - they are simply any other computer or application that might contact the sever."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#http",
    "href": "blog/2022-12-14-apis-and-httr/index.html#http",
    "title": "Aquiring Data via an API",
    "section": "HTTP",
    "text": "HTTP\nThis leads us one step further down the rabbit-hole. An API is a protocol that defines the rules of how applications communicate with one another. But how does this communication happen?\nHTTP (Hypertext Transfer Protocol) is the dominant mode communication on the World Wide Web. You can see the secure version of HTTP, HTTPS, at the start of most web addresses up at the top of your browser. For example:\nhttps://www.zakvarty.com/blog\nHTTP is the foundation of data communication on the web and is used to transfer files (such as text, images, and videos) between web servers and clients.\n\n\n\nTo understand HTTP communications, I find it helpful to imagine the client and the server as being a customer and a waiter at a restaurant. The client makes some request to the server, which then tries to comply before giving a response. The server might respond to confirm that the request was completed successfully. Alternatively, the server might respond with an error message, which is (hopefully) informative about why the request could not be completed.\nThis request-response model is the basis for HTTP, the communication system used by the majority of APIs."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#http-requests",
    "href": "blog/2022-12-14-apis-and-httr/index.html#http-requests",
    "title": "Aquiring Data via an API",
    "section": "HTTP Requests",
    "text": "HTTP Requests\nAn HTML request consists of:\n\nUniform Resource Locator (URL) [unique identifier for a thing]\nMethod [tells server the type of action requested by client]\nHeaders [meta-information about request, e.g. device type]\nBody [Data the client wants to send to the server]\n\n\n\n\n\nURL\nThe URL in a HTTP request specifies where that request is going to be made, for example http://example.com.\n\n\nMethod\nThe action that the client wants to take is indicated by a set of well-defined methods or HTTP verbs. The most common HTTP verbs are GET, POST, PUT, PATCH, and DELETE.\nThe GET verb is used to retrieve a resource from the server, such as a web page or an image. The POST verb is used to send data to the server, such as when submitting a form or uploading a file. The PUT verb is used to replace a resource on the server with a new one, while the PATCH verb is used to update a resource on the server without replacing it entirely. Finally, the DELETE verb is used to delete a resource from the server.\nIn addition to these common HTTP verbs, there are also several less frequently used verbs. These are used for specialized purposes, such as requesting only the headers of a resource, or testing the connectivity between the client and the server.\n\n\nHeader\nThe request headers contain meta-information about the request. This is where information about the device type would be included within the request.\n\n\nBody\nFinally, the body of the request contains the data that the client is providing to the server."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#http-responses",
    "href": "blog/2022-12-14-apis-and-httr/index.html#http-responses",
    "title": "Aquiring Data via an API",
    "section": "HTTP Responses",
    "text": "HTTP Responses\nWhen the server receives a request it will attempt to fulfil it and then send a response back to the client.\n\n\n\nA response has a similar structure to a request apart from:\n\nresponses do not have a URL,\nresponses do not have a method,\nresponses have a status code.\n\n\nStatus Codes\nThe status code is a 3 digit number, each of which has a specific meaning. Some common error codes that you might (already have) come across are:\n\n200: Success,\n404: Page not found (all 400s are errors),\n503: Page down.\n\nIn a data science context, a successful response will return the requested data within the data field. This will most likely be given in JSON or XML format."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#authentication",
    "href": "blog/2022-12-14-apis-and-httr/index.html#authentication",
    "title": "Aquiring Data via an API",
    "section": "Authentication",
    "text": "Authentication\nNow that we know how applications communicate, you might ask how we can control who has access to the API and what types of request they can make. This can be done by the server setting appropriate permissions for each client. But then how does the server verify that the client is really who is claims to be?\nAuthentication is a way to ensure that only authorized clients are able to access an API. This is typically done by the server requiring each client to provide some secret information that uniquely identifies them, whenever they make requests to the API. This information allows the API server to validate the authenticity this user before it authorises the request.\n\nBasic Authentication\nThere are various ways to implement API authentication.\nBasic authentication involves each legitimate client having a username and password. An encrypted version of these is included in the Authorization header of the HTTP request. If the hear matches with the server’s records then the request is processed. If not, then a special status code (401) is returned to the client.\nBasic authentication is dangerous because it does not put any restrictions on what a client can do once they are authorised. Additional, individualised restrictions can be added by using an alternative authentication scheme.\n\n\nAPI Key Authentication\nAn API key is long, random string of letters and numbers that is assigned to each authorised user. An API key is distinct from the user’s password and keys are typically issued by the service that provides an API. Using keys rather than basic authentication allows the API provider to track and limit the usage of their API.\nFor example, a provider may issue a unique API key to each developer or organization that wants to use the API. The provider can then limit access to certain data. They could also limit the number of requests that each key can make in a given time period or prevent access to certain administrative functions, like changing passwords or deleting accounts.\nUnlike Basic Authentication, there is no standard way of a client sharing a key with the server. Depending on the API this might be in the Authorization field of the header, at the end of the URL (http://example.com?api_key=my_secret_key), or within the body of the data."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#api-wrappers",
    "href": "blog/2022-12-14-apis-and-httr/index.html#api-wrappers",
    "title": "Aquiring Data via an API",
    "section": "API wrappers",
    "text": "API wrappers\nWe’ve learned a lot about how the internet works. Fortunately, a lot of the time we won’t have to worry about all of that new information other than for debugging purposes.\nIn the best case scenario, a very kind developer has written a “wrapper” function for the API. These wrappers are functions in R that will construct the HTML request for you. If you are particularly lucky, the API wrapper will also format the response for you, converting it from XML or JSON back into an R object that is ready for immediate use."
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#geonames-wrapper",
    "href": "blog/2022-12-14-apis-and-httr/index.html#geonames-wrapper",
    "title": "Aquiring Data via an API",
    "section": "{geonames} wrapper",
    "text": "{geonames} wrapper\nrOpenSci has a curated list of many wrappers for accessing scientific data using R. We will focus on the GeoNames API, which gives open access to a geographical database. To access this data, we will use wrapper functions provided by the {geonames} package.\nThe aim here is to illustrate the important steps of getting started with a new API.\n\nSet up\nBefore we can get any data from the GeoNames API, we first need to do a little bit of set up.\n\nInstall and load {geonames} from CRAN\n\n\n#install.packages(\"geonames\")\nlibrary(geonames)\n\n\nCreate a user account for the GeoNames API\n\n\n\nActivate the account (see activation email)\n\n\n\nEnable the free web services for your GeoNames account by logging in at this link.\nTell R your credentials for GeoNames.\n\n\n\n\n\n\n\nWarning\n\n\n\nWe could use the following code to tell R our credentials, but we absolutely should not.\n\noptions(geonamesUsername=\"example_username\")\n\nThis would save our username as an environment variable, but it also puts our API credentials directly into the script. If we share the script with our others (internally, externally or publicly) we would be sharing our credentials too. Not good!"
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#keep-it-secret-keep-it-safe",
    "href": "blog/2022-12-14-apis-and-httr/index.html#keep-it-secret-keep-it-safe",
    "title": "Aquiring Data via an API",
    "section": "Keep it Secret, Keep it Safe",
    "text": "Keep it Secret, Keep it Safe\nThe solution to this problem is to add our credentials as environment variables in our .Rprofile rather than in this script. The .Rprofile is an R script that is run at the start of every session. IT can be created and edited directly, but can also be created and edited from within R.\nTo make/open your .Rprofile use the edit_r_profile() function from the {usethis} package.\n\nlibrary(usethis)\nusethis::edit_r_profile()\n\nWithin this file, add options(geonamesUsername=\"example_username\") on a new line, remembering to replace example_username with your own GeoNames username.\nThe final step is to check this this file ends with a blank line, save it and restart R. Then we are all set to start using {geonames}.\nThis set up procedure is indicative of most API wrappers, but of course the details will vary between each API. This is why good documentation is important!"
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#using-geonames",
    "href": "blog/2022-12-14-apis-and-httr/index.html#using-geonames",
    "title": "Aquiring Data via an API",
    "section": "Using {geonames}",
    "text": "Using {geonames}\nGeoNames has a whole host of different geo-datasets that you can explore. As a first example, let’s get all of the geo-tagged wikipedia articles that are within 1km of Imperial College London.\n\nimperial_coords <- list(lat = 51.49876, lon = -0.1749)\nsearch_radius_km <- 1\n\nimperial_neighbours <- geonames::GNfindNearbyWikipedia(\n  lat = imperial_coords$lat,\n  lng = imperial_coords$lon, \n  radius = search_radius_km,\n  lang = \"en\",                # english language articles\n  maxRows = 500              # maximum number of results to return \n)\n\nLooking at the structure of imperial_neighbours we can see that it is a data frame with one row per geo-tagged wikipedia article.\n\nstr(imperial_neighbours)\n\n'data.frame':   204 obs. of  13 variables:\n $ summary     : chr  \"The Department of Mechanical Engineering is responsible for teaching and research in mechanical engineering at \"| __truncated__ \"Imperial College Business School is a global business school located in London. The business school was opened \"| __truncated__ \"Exhibition Road is a street in South Kensington, London which is home to several major museums and academic est\"| __truncated__ \"Imperial College School of Medicine (ICSM) is the medical school of Imperial College London in England, and one\"| __truncated__ ...\n $ elevation   : chr  \"20\" \"18\" \"19\" \"24\" ...\n $ feature     : chr  \"edu\" \"edu\" \"landmark\" \"edu\" ...\n $ lng         : chr  \"-0.1746\" \"-0.1748\" \"-0.17425\" \"-0.1757\" ...\n $ distance    : chr  \"0.0335\" \"0.0494\" \"0.0508\" \"0.0558\" ...\n $ rank        : chr  \"81\" \"91\" \"90\" \"96\" ...\n $ lang        : chr  \"en\" \"en\" \"en\" \"en\" ...\n $ title       : chr  \"Department of Mechanical Engineering, Imperial College London\" \"Imperial College Business School\" \"Exhibition Road\" \"Imperial College School of Medicine\" ...\n $ lat         : chr  \"51.498524\" \"51.4992\" \"51.4989722222222\" \"51.4987\" ...\n $ wikipediaUrl: chr  \"en.wikipedia.org/wiki/Department_of_Mechanical_Engineering%2C_Imperial_College_London\" \"en.wikipedia.org/wiki/Imperial_College_Business_School\" \"en.wikipedia.org/wiki/Exhibition_Road\" \"en.wikipedia.org/wiki/Imperial_College_School_of_Medicine\" ...\n $ countryCode : chr  NA \"AE\" NA \"GB\" ...\n $ thumbnailImg: chr  NA NA NA NA ...\n $ geoNameId   : chr  NA NA NA NA ...\n\n\nTo confirm we have the correct location we can inspect the title of the first five neighbours.\n\nimperial_neighbours$title[1:5]\n\n[1] \"Department of Mechanical Engineering, Imperial College London\"             \n[2] \"Imperial College Business School\"                                          \n[3] \"Exhibition Road\"                                                           \n[4] \"Imperial College School of Medicine\"                                       \n[5] \"Department of Civil and Environmental Engineering, Imperial College London\"\n\n\nNothing too surprising here, mainly departments of the college and Exhibition Road, which runs along one side of the campus. These sorts of check are important - I initially forgot the minus in the longitude and was getting results in East London!"
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#what-if-there-is-no-wrapper",
    "href": "blog/2022-12-14-apis-and-httr/index.html#what-if-there-is-no-wrapper",
    "title": "Aquiring Data via an API",
    "section": "What if there is no wrapper?",
    "text": "What if there is no wrapper?\nIf there is not a wrapper function, we can still access APIs fairly easilty using the {httr} package.\nWe will look at an example using OMDb, which is an open source version of IMDb, to get information about the movie Mean Girls.\nTo use the OMDB API you will once again need to request a free API key, follow a verification link and add your API key to your .Rprofile.\n\n# Add this to .Rprofile, pasting in your own API key\noptions(OMDB_API_Key = \"PASTE YOUR KEY HERE\")\n\nYou can then restart R and safely access your API key from within your R session.\n\n# Load your API key into the current R session,\nombd_api_key <- getOption(\"OMDB_API_Key\")\n\nUsing the documentation for the API, requests have URLs of the following form, where terms in angular brackets should be replaced by you.\nhttp://www.omdbapi.com/?t=<TITLE>&y=<YEAR>&plot=<LENGTH>&r=<FORMAT>&apikey=<API_KEY>\nWith a little bit of effort, we can write a function that composes this type of request URL for us. We will using the {glue} package to help us join strings together.\n\n#' Compose search requests for the OMBD API\n#'\n#' @param title String defining title to search for. Words are separated by \"+\".\n#' @param year String defining release year to search for\n#' @param plot String defining whether \"short\" or \"full\" plot is returned\n#' @param format String defining return format. One of \"json\" or \"xml\"\n#' @param api_key String defining your OMDb API key.\n#'\n#' @return String giving a OMBD search request URL\n#'\n#' @examples \n#' omdb_url(\"mean+girls\", \"2004\", \"short\", \"json\", getOption(OMBD_API_Key))\n#' \nomdb_url <- function(title, year, plot, format, api_key) {\n  glue::glue(\"http://www.omdbapi.com/?t={title}&y={year}&plot={plot}&r={format}&apikey={api_key}\")\n}\n\nRunning the example we get:\n\nmean_girls_request <- omdb_url(\n  title = \"mean+girls\",\n  year =  \"2004\",\n  plot = \"short\",\n  format =  \"json\",\n  api_key =  getOption(\"OMDB_API_Key\"))\n\nWe can then use the {httr} package to construct our request and store the response we get.\n\nresponse <- httr::GET(url = mean_girls_request)\nhttr::status_code(response)\n\n[1] 200\n\n\nThankfully it was a success! If you get a 401 error code here, check that you have clicked the activation link for your API key.\nThe full structure of the response is quite complicated, but we can easily extract the requested data using content()\n\nhttr::content(response)\n\n$Title\n[1] \"Mean Girls\"\n\n$Year\n[1] \"2004\"\n\n$Rated\n[1] \"PG-13\"\n\n$Released\n[1] \"30 Apr 2004\"\n\n$Runtime\n[1] \"97 min\"\n\n$Genre\n[1] \"Comedy\"\n\n$Director\n[1] \"Mark Waters\"\n\n$Writer\n[1] \"Rosalind Wiseman, Tina Fey\"\n\n$Actors\n[1] \"Lindsay Lohan, Jonathan Bennett, Rachel McAdams\"\n\n$Plot\n[1] \"Cady Heron is a hit with The Plastics, the A-list girl clique at her new school, until she makes the mistake of falling for Aaron Samuels, the ex-boyfriend of alpha Plastic Regina George.\"\n\n$Language\n[1] \"English, German, Vietnamese, Swahili\"\n\n$Country\n[1] \"United States, Canada\"\n\n$Awards\n[1] \"7 wins & 25 nominations\"\n\n$Poster\n[1] \"https://m.media-amazon.com/images/M/MV5BMjE1MDQ4MjI1OV5BMl5BanBnXkFtZTcwNzcwODAzMw@@._V1_SX300.jpg\"\n\n$Ratings\n$Ratings[[1]]\n$Ratings[[1]]$Source\n[1] \"Internet Movie Database\"\n\n$Ratings[[1]]$Value\n[1] \"7.1/10\"\n\n\n$Ratings[[2]]\n$Ratings[[2]]$Source\n[1] \"Rotten Tomatoes\"\n\n$Ratings[[2]]$Value\n[1] \"84%\"\n\n\n$Ratings[[3]]\n$Ratings[[3]]$Source\n[1] \"Metacritic\"\n\n$Ratings[[3]]$Value\n[1] \"66/100\"\n\n\n\n$Metascore\n[1] \"66\"\n\n$imdbRating\n[1] \"7.1\"\n\n$imdbVotes\n[1] \"385,107\"\n\n$imdbID\n[1] \"tt0377092\"\n\n$Type\n[1] \"movie\"\n\n$DVD\n[1] \"21 Sep 2004\"\n\n$BoxOffice\n[1] \"$86,058,055\"\n\n$Production\n[1] \"N/A\"\n\n$Website\n[1] \"N/A\"\n\n$Response\n[1] \"True\""
  },
  {
    "objectID": "blog/2022-12-14-apis-and-httr/index.html#wrapping-up",
    "href": "blog/2022-12-14-apis-and-httr/index.html#wrapping-up",
    "title": "Aquiring Data via an API",
    "section": "Wrapping up",
    "text": "Wrapping up\nWe have learned a bit more about how the internet works, the benefits of using an API to share data and how to request data from Open APIs.\nWhen obtaining data from the internet it’s vital that you keep your credentials safe, and that don’t do more work than is needed.\n\nKeep your API keys out of your code. Store them in your .Rprofile (and make sure this is not under version control!)\nScraping is always a last resort. Is there an API already?\nWriting your own code to access an API can be more painful than necessary.\nDon’t repeat other people, if a suitable wrapper exists then use it."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html",
    "href": "blog/2022-12-01-rvest/index.html",
    "title": "Web Scraping with {rvest}",
    "section": "",
    "text": "You can’t always rely on tidy, tabular data to land on your desk. Sometimes you are going to have to go out and gather data for yourself.\nI’m not suggesting you will need to do this manually, but you will likely need to get data from the internet that’s been made publicly or privately available to you.\nThis might be information from a webpage that you gather yourself, or data shared with you by a collaborator using an API.\nIn this first blog post we will cover the basics of scraping webpages, following the vignette for the {rvest} package."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#what-is-a-webpage",
    "href": "blog/2022-12-01-rvest/index.html#what-is-a-webpage",
    "title": "Web Scraping with {rvest}",
    "section": "What is a webpage?",
    "text": "What is a webpage?\nBefore we can even hope to get data from a webpage, we first need to understand what a webpage is.\nWebpages are written in a similar way to LaTeX: the content and styling of webpages are handled separately and are coded using plain text files.\nIn fact, websites go one step further than LaTeX. The content and styling of websites are written in different files and in different languages. HTML (HyperText Markup Language) is used to write the content and then CSS (Cascading Style Sheets) are used to control the appearance of that content when it’s displayed to the user."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#html",
    "href": "blog/2022-12-01-rvest/index.html#html",
    "title": "Web Scraping with {rvest}",
    "section": "HTML",
    "text": "HTML\nA basic HTML page with no styling applied might look something like this:\n<html>\n<head>\n  <title>Page title</title>\n</head>\n<body>\n  <h1 id='first'>A level 1 heading</h1>\n  <p>Hello World!</p>\n  <p>Here is some plain text &amp; <b>some bold text.</b></p>\n  <img src='myimg.png' width='100' height='100'>\n</body>\n\nHTML elements\n\nJust like XML data files, HTML has a hierarchical structure. This structure is crafted using HTML elements. Each HTML element is made up of of a start tag, optional attributes, an end tag.\nWe can see each of these in the first level header, where <h1> is the opening tag, id='first' is an additional attribute and </h1> is the closing tag. Everything between the opening and closing tag are the contents of that element. There are also some special elements that consist of only a single tag and its optional attributes. An example of this is the <img> tag.\nSince < and > are used for start and end tags, you can’t write them directly in an HTML document. Instead, you have to use escape characters. This sounds fancy, but it’s just an alternative way to write characters that serve some special function within a language.\nYou can write greater than &gt; and less than as &lt;. You might notice that those escapes use an ampersand (&). This means that if you want a literal ampersand on your webpage, you have to escape too using &amp;.\nThere are a wide range of possible HTML tags and escapes. We’ll cover the most common tags in this lecture and you don’t need to worry about escapes too much because {rvest} will automatically handle them for you.\n\n\nImportant HTML Elements\nIn all, there are in excess of 100 HTML elements. The most important ones for you to know about are:\n\nThe <html> element, that must enclose every HTML page. The <html> element must have two child elements within it. The <head> element contains metadata about the document, like the page title that is shown in the browser tab and the CSS style sheet that should be applied. The <body> element then contains all of the content that you see in the browser.\nBlock elements are used to give structure to the page. These are elements like headings, sub-headings and so on from <h1> all the way down to <h6>. This category also contains paragraph elements <p>, ordered lists <ol> unordered lists <ul>.\nFinally, inline tags like <b> for bold, <i> for italics, and <a> for hyperlinks are used to format text inside block elements.\n\nWhen you come across a tag that you’ve never seen before, you can find out what it does with just a little bit of googling. A good resource here is the MDN Web Docs which are produced by Mozilla, the company that makes the Firefox web browser. The W3schools website is another great resource for web development and coding resources more generally."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#html-attributes",
    "href": "blog/2022-12-01-rvest/index.html#html-attributes",
    "title": "Web Scraping with {rvest}",
    "section": "HTML Attributes",
    "text": "HTML Attributes\nWe’ve seen one example of a header with an additional attribute. More generally, all tags can have named attributes. These attributes are contained within the opening tag and look something like:\n<tag attribute1='value1' attribute2='value2'>element contents</tag>\nTwo of the most important attributes are id and class. These attributes are used in conjunction with the CSS file to control the visual appearance of the page. These are often very useful to identify the elements that you are interested in when scraping data off a page."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#css-selectors",
    "href": "blog/2022-12-01-rvest/index.html#css-selectors",
    "title": "Web Scraping with {rvest}",
    "section": "CSS Selectors",
    "text": "CSS Selectors\nThe Cascading Style Sheet is used to describe how your HTML content will be displayed. To do this, CSS has it’s own system for selecting elements of a webpage, called CSS selectors.\nCSS selectors define patterns for locating the HTML elements that a particular style should be applied to. A happy side-effect of this is that they can sometimes be very useful for scraping, because they provide a concise way of describing which elements you want to extract.\nCSS Selectors can work on the level of an element type, a class, or a tag and these can be used in a nested (or cascading) way.\n\nThe p selector will select all paragraph <p> elements.\nThe .title selector will select all elements with class “title”.\nThe p.special selector will select all<p> elements with class “special”.\nThe #title selector will select the element with the id attribute “title”.\n\nWhen you want to select a single element id attributes are particularly useful because that must be unique within a html document. Unfortunately, this is only helpful if the developer added an id attribute to the element(s) you want to scrape!\nIf you want to learn more CSS selectors I recommend starting with the fun CSS dinner tutorial to build a base of knowledge and then using the W3schools resources as a reference to explore more webpages in the wild."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#which-attributes-and-selectors-do-you-need",
    "href": "blog/2022-12-01-rvest/index.html#which-attributes-and-selectors-do-you-need",
    "title": "Web Scraping with {rvest}",
    "section": "Which Attributes and Selectors Do You Need?",
    "text": "Which Attributes and Selectors Do You Need?\nTo scrape data from a webpage, you first have to identify the tag and attribute combinations that you are interested in gathering.\nTo find your elements of interest, you have three options. These go from hardest to easiest but also from most to least robust.\n\nright click + “inspect page source” (F12)\nright click + “inspect”\nRvest Selector Gadget (very useful but fallible)\n\nInspecting the source of some familiar websites can be a useful way to get your head around these concepts. Beware though that sophisticated webpages can be quite intimidating. A good place to start is with simpler, static websites such as personal websites, rather than the dynamic webpages of online retailers or social media platforms."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#reading-html-with-rvest",
    "href": "blog/2022-12-01-rvest/index.html#reading-html-with-rvest",
    "title": "Web Scraping with {rvest}",
    "section": "Reading HTML with {rvest}",
    "text": "Reading HTML with {rvest}\nWith {rvest}, reading a html page can be as simple as loading in tabular data.\n\nhtml <- rvest::read_html(\"https://www.zakvarty.com/professional/teaching.html\")\n\nThe class of the resulting object is an xml_document. This type of object is from the low-level package {xml2}, which allows you to read xml files into R.\n\nclass(html)\n\n[1] \"xml_document\" \"xml_node\"    \n\n\nWe can see that this object is split into several components: first is some metadata on the type of document we have scraped, followed by the head and then the body of that html document.\n\nhtml\n\n{html_document}\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body class=\"nav-fixed\">\\n\\n<div id=\"quarto-search-results\"></div>\\n  <he ...\n\n\nWe have several possible approaches to extracting information from this document."
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#extracting-html-elements",
    "href": "blog/2022-12-01-rvest/index.html#extracting-html-elements",
    "title": "Web Scraping with {rvest}",
    "section": "Extracting HTML elements",
    "text": "Extracting HTML elements\nIn {rvest} you can extract a single element with html_element(), or all matching elements with html_elements(). Both functions take a document object and one or more CSS selectors as inputs.\n\nlibrary(rvest)\nhtml %>% html_elements(\"h1\")\n## {xml_nodeset (1)}\n## [1] <h1>Teaching</h1>\nhtml %>% html_elements(\"h2\")\n## {xml_nodeset (2)}\n## [1] <h2 id=\"toc-title\">On this page</h2>\n## [2] <h2 class=\"anchored\" data-anchor-id=\"course-history\">Course History</h2>\nhtml %>% html_elements(\"p\")\n## {xml_nodeset (2)}\n## [1] <p>I am fortunate to have had the opportunity to teach in a variety of ro ...\n## [2] <p>I am an associate fellow of the Higher Education Academy, which you ca ...\n\nYou can also combine and nest these selectors. For example you might want to extract all links that are within paragraphs and all second level headers.\n\nhtml %>% html_elements(\"p a,h2\")\n\n{xml_nodeset (3)}\n[1] <h2 id=\"toc-title\">On this page</h2>\n[2] <a href=\"https://www.advance-he.ac.uk/fellowship/associate-fellowship\">he ...\n[3] <h2 class=\"anchored\" data-anchor-id=\"course-history\">Course History</h2>"
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#extracting-data-from-html-elements",
    "href": "blog/2022-12-01-rvest/index.html#extracting-data-from-html-elements",
    "title": "Web Scraping with {rvest}",
    "section": "Extracting Data From HTML Elements",
    "text": "Extracting Data From HTML Elements\nNow that we’ve got the elements we care about extracted from the complete document. But how do we get the data we need out of those elements?\nYou’ll usually get the data from either the contents of the HTML element or else from one of it’s attributes. If you’re really lucky, the data you need will already be formatted for you as a HTML table or list.\n\nExtracting text\nThe functions rvest::html_text() and rvest::html_text2() can be used to extract the plain text contents of an HTML element.\n\nhtml %>% \n  html_elements(\"#teaching li\") %>% \n  html_text2()\n\n[1] \"one-to-one tuition for high school students;\"                                   \n[2] \"running workshops and computer labs for undergraduate and postgraduate modules;\"\n[3] \"delivering short courses on scientific communication and LaTeX;\"                \n[4] \"supervising an undergraduate research project;\"                                 \n[5] \"developing and lecturing postgraduate modules in statistics and data science.\"  \n\n\nThe difference between html_text() and html_text2() is in how they handle whitespace. In HTML whitespace and line breaks have very little influence over how the code is interpreted by the computer (this is similar to R but very different from Python). html_text() will extract the text as it is in the raw html, while html_text2() will do its best to extract the text in a way that gives you something similar to what you’d see in the browser.\n\n\nExtracting Attributes\nAttributes are also used to record information that you might like to collect. For example, the destination of links are stored in the href attribute and the source of images is stored in the src attribute.\nAs an example of this, consider trying to extract the twitter link from the icon in the page footer. This is quite tricky to locate in the html source, so I used the Selector Gadget to help find the correct combination of elements.\n\nhtml %>% html_element(\".compact:nth-child(1) .nav-link\")\n\n{html_node}\n<a class=\"nav-link\" href=\"https://www.twitter.com/zakvarty\">\n[1] <i class=\"bi bi-twitter\" role=\"img\">\\n</i>\n\n\nTo extract the href attribute from the scraped element, we use the rvest::html_attr() function.\n\nhtml %>% \n  html_elements(\".compact:nth-child(1) .nav-link\") %>% \n  html_attr(\"href\")\n\n[1] \"https://www.twitter.com/zakvarty\"\n\n\nNote: rvest::html_attr() will always return a character string (or list of character strings). If you are extracting an attribute that describes a quantity, such as the width of an image, you’ll need to convert this from a string to your required data type. For example, of the width is measures in pixels you might use as.integer().\n\n\nExtracting tables\nHTML tables are composed in a similar, nested manner to LaTeX tables.\nThere are four main elements to know about that make up an HTML table:\n\n<table>,\n<tr> (table row),\n<th> (table heading),\n<td> (table data).\n\nHere’s our simple example data, formatted as an HTML table:\n\nhtml_2 <- minimal_html(\"\n  <table>\n    <tr>\n      <th>Name</th>\n      <th>Number</th>\n    </tr>\n    <tr>\n      <td>A</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>B</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>C</td>\n      <td>3</td>\n    </tr>\n  </table>\n  \")\n\nSince tables are a common way to store data, {rvest} includes a useful function html_table() that converts directly from an HTML table into a tibble.\n\nhtml_2 %>% \n  html_element(\"table\") %>% \n  html_table()\n\n# A tibble: 3 × 2\n  Name  Number\n  <chr>  <int>\n1 A          1\n2 B          2\n3 C          3\n\n\nApplying this to our real scraped data we can easily extract the table of taught courses.\n\nhtml %>% \n  html_element(\"table\") %>% \n  html_table()\n\n# A tibble: 25 × 3\n   Year      Course                                   Role                      \n   <chr>     <chr>                                    <chr>                     \n 1 \"2021-22\" Supervised Learning                      Lecturer                  \n 2 \"\"        Ethics in Data Science I                 Lecturer                  \n 3 \"\"        Ethics in Data Science II                Lecturer                  \n 4 \"—\"       —                                        —                         \n 5 \"2020-21\" MATH562/582: Extreme Value Theory        Lecturer                  \n 6 \"\"        MATH331: Bayesian Inference              Graduate teaching assista…\n 7 \"\"        MATH330: Likelihood Inference            Graduate teaching assista…\n 8 \"2019-20\" DSCI485: Introduction to LaTeX           Co-leading short course   \n 9 \"\"        MATH566: Longitudinal Data Analysis      Graduate teaching assista…\n10 \"2018-19\" STOR-i Internship: Introduction to LaTeX Co-leading short course   \n# … with 15 more rows"
  },
  {
    "objectID": "blog/2022-12-01-rvest/index.html#tip-for-building-tibbles",
    "href": "blog/2022-12-01-rvest/index.html#tip-for-building-tibbles",
    "title": "Web Scraping with {rvest}",
    "section": "Tip for Building Tibbles",
    "text": "Tip for Building Tibbles\nWhen scraping data from a webpage, your end-goal is typically going to be constructing a data.frame or a tibble.\nIf you are following our description of tidy data, you’ll want each row to correspond some repeated unit on the HTML page. In this case, you should\n\nUse html_elements() to select the elements that contain each observation unit;\nUse html_element() to extract the variables from each of those observations.\n\nTaking this approach guarantees that you’ll get the same number of values for each variable, because html_element() always returns the same number of outputs as inputs. This is vital when you have missing data - when not every observation unit has a value for every variable of interest.\nAs an example, consider this extract of text about the starwars dataset.\n\nstarwars_html <- minimal_html(\"\n  <ul>\n    <li><b>C-3PO</b> is a <i>droid</i> that weighs <span class='weight'>167 kg</span></li>\n    <li><b>R2-D2</b> is a <i>droid</i> that weighs <span class='weight'>96 kg</span></li>\n    <li><b>Yoda</b> weighs <span class='weight'>66 kg</span></li>\n    <li><b>R4-P17</b> is a <i>droid</i></li>\n  </ul>\n  \")\n\nThis is an unordered list where each list item corresponds to one observational unit (one character from the starwars universe). The name of the character is given in bold, the character species is specified in italics and the weight of the character is denoted by the .weight class. However, some characters have only a subset of these variables defined: for example Yoda has no species entry.\nIf we try to extract each element directly, our vectors of variable values are of different lengths. We don’t know where the missing values should be, so we can’t line them back up to make a tibble.\n\nstarwars_html %>% html_elements(\"b\") %>% html_text2()\n## [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_html %>% html_elements(\"i\") %>% html_text2()\n## [1] \"droid\" \"droid\" \"droid\"\nstarwars_html %>% html_elements(\".weight\") %>% html_text2()\n## [1] \"167 kg\" \"96 kg\"  \"66 kg\"\n\nWhat we should do instead is start by extracting all of the list item elements using html_elements(). Once we have done this, we can then use html_element() to extract each variable for all characters. This will pad with NAs, so that we can collate them into a tibble.\n\nstarwars_characters <- starwars_html %>% html_elements(\"li\")\n\nstarwars_characters %>% html_element(\"b\") %>% html_text2()\n## [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_characters %>% html_element(\"i\") %>% html_text2()\n## [1] \"droid\" \"droid\" NA      \"droid\"\nstarwars_characters %>% html_element(\".weight\") %>% html_text2()\n## [1] \"167 kg\" \"96 kg\"  \"66 kg\"  NA\n\n\ntibble::tibble(\n  name = starwars_characters %>% html_element(\"b\") %>% html_text2(),\n  species = starwars_characters %>% html_element(\"i\") %>% html_text2(),\n  weight = starwars_characters %>% html_element(\".weight\") %>% html_text2()\n)\n\n# A tibble: 4 × 3\n  name   species weight\n  <chr>  <chr>   <chr> \n1 C-3PO  droid   167 kg\n2 R2-D2  droid   96 kg \n3 Yoda   <NA>    66 kg \n4 R4-P17 droid   <NA>"
  },
  {
    "objectID": "blog/2022-10-11-ravelry-tidy-tuesday/index.html",
    "href": "blog/2022-10-11-ravelry-tidy-tuesday/index.html",
    "title": "Tidy Tuesday: Ravelry Yarn",
    "section": "",
    "text": "I have made several plots using {ggplot2} before, but this was my first attempt at making one aesthetically pleasing (forgive the pun).\nWhen making this plot I learned about using custom font, colours, annotations and arrows from a lot of @nrennie’s past examples.\nCode and figure down below ↓\n\n\nCode\n# Load Packages ----\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(showtext)\n\n# Load Fonts ----\nfont_add_google(name = \"Indie Flower\", family = \"indie-flower\")\nfont_add_google(name = \"Permanent Marker\", family = \"marker\")\nshowtext_auto()\n\n# Load Data ----\nurl <- \"https://github.com/rfordatascience/tidytuesday/raw/master/data/2022/2022-10-11/yarn.csv\"\nyarn <- readr::read_csv(file = url)\n\n# Data Handling ----\n\nother_weight_names <- c(\n  \"Thread\",\n  \"Cobweb\",\n  \"Jumbo\",\n  \"DK / Sport\",\n  \"Aran / Worsted\",\n  \"No weight specified\")\n\nyarn_data <- yarn %>%\n  select(yarn_weight_name) %>%\n  mutate(yarn_weight_name = as.character(yarn_weight_name)) %>%\n  mutate_at(c(\"yarn_weight_name\"), ~replace_na(.,\"Missing\")) %>%\n  mutate(name = fct_collapse(yarn_weight_name, Other = other_weight_names)) %>%\n  mutate(name = fct_collapse(name, \"Double Knit\" = c(\"DK\"))) %>%\n  group_by(name) %>%\n  summarise(value = n())\n\n# Helper data frames for adding arrows to plot\narrow_df_1 <- data.frame(x1 = 27000, x2 = 27000, y1 = 7.5, y2 = 10.4)\narrow_df_2 <- data.frame(x1 = 27000, x2 = 19000, y1 = 7.5, y2 = 10)\n\n# Making Plot ----\n\nbar_colour <- \"#483248\"\nbg_colour <- \"#FEFBEA\"\ntitle_font <- \"marker\"\nmain_font <- \"indie-flower\"\n\nyarn_plot <- yarn_data %>%\n  ggplot(aes(y = reorder(name, value), x = value)) +\n  geom_bar(stat = \"identity\", fill = bar_colour) +\n  theme_void() +\n  ggtitle(\" \\n Yarn weights on Ravelry, ordered by frequency\",subtitle = \" \") +\n  theme(axis.title = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        text = element_text(family = main_font),\n        plot.background = element_rect(fill = bg_colour, colour = bg_colour),\n        panel.background = element_rect(fill = bg_colour, colour = bg_colour),\n        plot.title = element_text(family = title_font, size = 22, hjust = 0.5)\n  ) +\n  lims(x = c(0,28000)) +\n  geom_text(aes(label = name, x = 200),\n            color = bg_colour,\n            hjust = 0,\n            family = main_font,\n            size = 5) +\n  geom_text(aes(label = value),\n            hjust = 0,\n            nudge_x = 200,\n            color = bar_colour,\n            family = main_font,\n            size = 6) +\n  geom_text(aes(label = \"The most popular yarn weights \\n  are 'Fingering' and 'Double Knit'\",\n                x = 20000,\n                y = 6.7),\n            family = main_font,\n            size = 7) +\n  geom_text(aes(label = \"There were more missing yarn weights \\n than in all remaining categories combined\",\n                x = 18000,\n                y = 1.6),\n            family = main_font,\n            size  = 7) +\n  geom_text(aes(label = \"}\"),\n            x = 7000,\n            y = 1.5,\n            size = 19,\n            family = main_font) +\n  geom_text(aes(label = \"Tidy Tuesday 11 Oct 2022 | Data: Ravelry |  @zakvarty\"),\n            x = 29000,\n            y = 4.5,\n            size = 5,\n            family = main_font,\n            angle = 270) +\n  geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2),\n             data = arrow_df_1,\n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2),\n             data = arrow_df_2,\n             arrow = arrow(length = unit(0.03, \"npc\")))\n\nyarn_plot\n\n\n\n\n\nCode\n# Exported as 8x8 inch pdf and 800x700 png\n# (next time start by setting canvas size!)"
  },
  {
    "objectID": "blog/2022-09-26-adding-a-quarto-blog/index.html",
    "href": "blog/2022-09-26-adding-a-quarto-blog/index.html",
    "title": "Setting up a quarto blog",
    "section": "",
    "text": "Steps\n\nCreate a subdirectory of the website called blog/. This has sub-folders for each blog post and will contain the files of metadata that are common to all blog posts (e.g. default settings for YAML headers information and a bibliography file). \nCreate a listing page called blog.qmd in the root directory. This will become the blog “landing page” and what we will point to from the website header. \nAdd a “Blog” header item to the _quarto.yml file for the website and set the link: for this to be blog.qmd \nAdded a simple example post to the blog/ directory. See for example my hello-world post. \nAdjust the default YAML parameters for the blog posts by making the file blog/_metadata.yml. These default values can be overwritten by specifying them again in the YAML header at the top of any individual post. For examples of what you might want to include see my file or the projects section of the quarto docs. \nAdd a simple bibliography file, called library.bib or similar to the blog/ directory. Set this as the default bibliography file for each blog post by adding bibliography: ../library.bib to blog/_metadata.yml. \n(optional) Create a post template so that you don’t have to memorise header fields. \nSet your “Hello, World!” and template posts to have draft: true in their headers. This will prevent them from showing up on your website. \nSet your “Hello, World!” and template posts to have freeze: true in their headers. This will prevent any code in them from re-running each time the website is rendered.\n\nFreezing the code within posts will improve the build speed, as well as make the website more stable and portable. See the quarto docs on freezing posts for more details. My current plan is to have this as false by default and change to true on publication of each post.\n\n\nChecking that references work\nI have set up a single bibtex file in which to store references for all posts. This lives in the blog/ directory and is set as the default bibliography parameter for each post in the file blog/_metadata.yml.\nThis is an in-line reference to Wan et al. (2020) written as @citationkey. Parenthetical references, such as (Wan et al. 2020), are written using [@citationkey]. These can be strung together by separating each citation key with a semicolon, for example (Wan et al. 2020, 2020).\nTo let people know the license your work is under and how they should cite your blog posts you can use the appendix-style argument. This can be added to the YAML header of individual blog posts or you can specify a default value in blog/_metadata.yml. There are three options for this parameter:\n\ndefault does some nice formatting and makes the text a bit smaller than the rest of the article;\nplain matches the style of the rest of your post;\nnone does not add any citation details to the end of your post.\n\nI’m currently using some pretty hacky CSS to style this website so am limited to the latter two options for now. In the process of writing this article I stumbled across some neat SCSS that I hope will fix this issue that I have made for myself! [Update: I changed to SCSS and this is now fixed!]\nNote: When adding references to your posts, make sure that the site-URL field in your website’s quarto.yml does not have a trailing slash - this will be copied into the reference and break the links.\n\n\n\n\n\nReferences\n\nWan, Phyllis, Tiandong Wang, Richard A Davis, and Sidney I Resnick. 2020. “Are Extreme Value Estimation Methods Useful for Network Data?” Extremes 23 (1): 171–95.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{varty2022,\n  author = {Zak Varty},\n  title = {Setting up a Quarto Blog},\n  date = {2022-09-26},\n  url = {https://www.zakvarty.com/blog/2022-09-26-adding-a-quarto-blog},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZak Varty. 2022. “Setting up a Quarto Blog.” September 26,\n2022. https://www.zakvarty.com/blog/2022-09-26-adding-a-quarto-blog."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html",
    "href": "blog/2023-01-06-data-wrangling/index.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Okay, so you’ve got some data. Great start!\nYou might have had it handed to you by a collaborator, requested it via an API or scraped it from the raw html of a webpage. In the worst case scenario, you’re an actual scientist (not just a data one) and you spent the last several months of your life painstakingly measuring flower petals or car parts. Now we really want to do something useful with that data.\nWe’ve seen already how you can load the data into R and pivot between wider and longer formats, but that probably isn’t enough to satisfy your curisity. You want to be able to view your data, manipulate and subset it, create new variables from exisiting ones and cross-reference your dataset with others. All of these are things possible in R and are known under various collective names including data manipulation, data munging and data wrangling.\nI’ve decided to use the term data wranging here. That’s because data manipulation sounds boring a.f. and data munging is both unpleasant to say and makes me imagine we are squelching through some sort of information swamp.\nIn what follows I’ll give a fly-by tour of tools for data wrangling in R, showing some examples along the way. I’ll focus on some of the most common and useful operations and link out to some more extensive guides for wrangling your data in R, that you can refer back to as you need them."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#example-data-sets",
    "href": "blog/2023-01-06-data-wrangling/index.html#example-data-sets",
    "title": "Data Wrangling",
    "section": "Example Data Sets",
    "text": "Example Data Sets\nTo demonstrate some standard skills we will use some standard datasets that come built into any R installation. These are the penguins data set from {palmerpenguins} and the mtcars data set.\n\nlibrary(palmerpenguins)\npengins <- palmerpenguins::penguins\ncars <- datasets::mtcars"
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#viewing-your-data",
    "href": "blog/2023-01-06-data-wrangling/index.html#viewing-your-data",
    "title": "Data Wrangling",
    "section": "Viewing Your Data",
    "text": "Viewing Your Data\n\nView()\nThe View() function can be used to crease a spreadsheet-like view of your data. In RStudio this will open as a new tab.\nView() will work for any “matrix-like” R object, such as a tibble, data frame, vector or matrix. Note the capital letter - the function is called View(), not view().\n\nView(penguins)\n\n\n\n\nScreenshot of RStduio files pane, containg a spreadsheet view of the palmer penguins data set.\n\n\n\n\nhead()\nFor large data sets, you might not want (or be able to) view it all at once. You can then use head() to view the first few rows. The integer argument n specifies the number of rows you would like to return.\n\nhead(x = pengins, n = 3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\n\nstr()\nAn alternative way to view the a large data set, or one with a complicated format is to examine its structure with str(). This is a useful way to inspect the structure of list-like objects, particularly when they’ve got a nested structure.\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\n\n\nnames()\nFinally, if you just want to access the variable names you can do so with the names() function from base R.\n\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\nSimilarly, you can explicitly access the row and column names of a data frame or tibble using colnames() or rownames().\n\ncolnames(cars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\nrownames(cars)\n\n [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n[10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n[13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n[16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n[19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n[22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n[25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n[28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n[31] \"Maserati Bora\"       \"Volvo 142E\"         \n\n\nIn the cars data, the car model are stored as the row names. This doesn’t really jive with our idea of tidy data - we’ll see how to fix that shortly."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#renaming-variables",
    "href": "blog/2023-01-06-data-wrangling/index.html#renaming-variables",
    "title": "Data Wrangling",
    "section": "Renaming Variables",
    "text": "Renaming Variables\n\ncolnames()\nThe function colnames() can be used to set as well as to retrieve column names.\n\ncars_renamed <- cars \ncolnames(cars_renamed)[1] <- \"miles_per_gallon\"\ncolnames(cars_renamed)\n\n [1] \"miles_per_gallon\" \"cyl\"              \"disp\"             \"hp\"              \n [5] \"drat\"             \"wt\"               \"qsec\"             \"vs\"              \n [9] \"am\"               \"gear\"             \"carb\"            \n\n\n\n\ndplyr::rename()\nWe can also use functions from {dplyr} to rename columns. Let’s alter the second column name.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ncars_renamed <- rename(.data = cars_renamed, cylinders = cyl)\ncolnames(cars_renamed)\n\n [1] \"miles_per_gallon\" \"cylinders\"        \"disp\"             \"hp\"              \n [5] \"drat\"             \"wt\"               \"qsec\"             \"vs\"              \n [9] \"am\"               \"gear\"             \"carb\"            \n\n\nThis could be done as part of a pipe, if we were making many alterations.\n\ncars_renamed <- cars_renamed %>% \n  rename(displacement = disp) %>% \n  rename(horse_power = hp) %>% \n  rename(rear_axel_ratio = drat)\n\ncolnames(cars_renamed)\n\n [1] \"miles_per_gallon\" \"cylinders\"        \"displacement\"     \"horse_power\"     \n [5] \"rear_axel_ratio\"  \"wt\"               \"qsec\"             \"vs\"              \n [9] \"am\"               \"gear\"             \"carb\"            \n\n\nWhen using the dplyr function you have to remember the format new_name = old_name. This matches the format used to create a data frame or tibble, but is the opposite order to the python function of the same name and often catches people out.\nIn the section (#creating-new-variables) on creating new variables, we will see an alternative way of doing this by copying the column and then deleting the original."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#subsetting",
    "href": "blog/2023-01-06-data-wrangling/index.html#subsetting",
    "title": "Data Wrangling",
    "section": "Subsetting",
    "text": "Subsetting\n\nBase R\nIn base R you can extract rows, columns and combinations thereof using index notation.\n\n# First row\npenguins[1, ]\n\n# A tibble: 1 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n# First Column \npenguins[ , 1]\n\n# A tibble: 344 × 1\n   species\n   <fct>  \n 1 Adelie \n 2 Adelie \n 3 Adelie \n 4 Adelie \n 5 Adelie \n 6 Adelie \n 7 Adelie \n 8 Adelie \n 9 Adelie \n10 Adelie \n# … with 334 more rows\n\n# Rows 2-3 of columns 1, 2 and 4\npenguins[2:3, c(1, 2, 4)]\n\n# A tibble: 2 × 3\n  species island    bill_depth_mm\n  <fct>   <fct>             <dbl>\n1 Adelie  Torgersen          17.4\n2 Adelie  Torgersen          18  \n\n\nUsing negative indexing you can remove rows or columns\n\n# Drop all but first row\npenguins[-(2:344), ]\n\n# A tibble: 1 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\n# Drop all but first column \npenguins[ , -(2:8)]\n\n# A tibble: 344 × 1\n   species\n   <fct>  \n 1 Adelie \n 2 Adelie \n 3 Adelie \n 4 Adelie \n 5 Adelie \n 6 Adelie \n 7 Adelie \n 8 Adelie \n 9 Adelie \n10 Adelie \n# … with 334 more rows\n\n\nYou can also select rows or columns by their names. This can be done using the bracket syntax ([ ]) or the dollar syntax ($).\n\npengins[ , \"species\"]\n\n# A tibble: 344 × 1\n   species\n   <fct>  \n 1 Adelie \n 2 Adelie \n 3 Adelie \n 4 Adelie \n 5 Adelie \n 6 Adelie \n 7 Adelie \n 8 Adelie \n 9 Adelie \n10 Adelie \n# … with 334 more rows\n\npenguins$species\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo\n\n\nSince penguins is a tibble, these return different types of object. Sub-setting a tibble with bracket syntax will return a tibble, while extracting a column using the dollar syntax returns a vector of values.\n\n\nfilter() and select()\n{dplyr} has two functions for subsetting, filter() subsets by rows and select() subsets by column.\nIn both functions you list what you would like to retain. Filter and select calls can be piped together to subset based on row and column values.\n\npenguins %>% \n  select(species, island,body_mass_g)\n\n# A tibble: 344 × 3\n   species island    body_mass_g\n   <fct>   <fct>           <int>\n 1 Adelie  Torgersen        3750\n 2 Adelie  Torgersen        3800\n 3 Adelie  Torgersen        3250\n 4 Adelie  Torgersen          NA\n 5 Adelie  Torgersen        3450\n 6 Adelie  Torgersen        3650\n 7 Adelie  Torgersen        3625\n 8 Adelie  Torgersen        4675\n 9 Adelie  Torgersen        3475\n10 Adelie  Torgersen        4250\n# … with 334 more rows\n\n\n\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(body_mass_g > 6000)\n\n# A tibble: 2 × 3\n  species island body_mass_g\n  <fct>   <fct>        <int>\n1 Gentoo  Biscoe        6300\n2 Gentoo  Biscoe        6050\n\n\nSubsetting rows can be inverted by negating the filter() statement.\n\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(!(body_mass_g > 6000))\n\n# A tibble: 340 × 3\n   species island    body_mass_g\n   <fct>   <fct>           <int>\n 1 Adelie  Torgersen        3750\n 2 Adelie  Torgersen        3800\n 3 Adelie  Torgersen        3250\n 4 Adelie  Torgersen        3450\n 5 Adelie  Torgersen        3650\n 6 Adelie  Torgersen        3625\n 7 Adelie  Torgersen        4675\n 8 Adelie  Torgersen        3475\n 9 Adelie  Torgersen        4250\n10 Adelie  Torgersen        3300\n# … with 330 more rows\n\n\nand dropping columns can done by selecting all columns except the one(s) you want to drop.\n\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(!(body_mass_g > 6000)) %>% \n  select(!c(species, island))\n\n# A tibble: 340 × 1\n   body_mass_g\n         <int>\n 1        3750\n 2        3800\n 3        3250\n 4        3450\n 5        3650\n 6        3625\n 7        4675\n 8        3475\n 9        4250\n10        3300\n# … with 330 more rows"
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#creating-new-variables",
    "href": "blog/2023-01-06-data-wrangling/index.html#creating-new-variables",
    "title": "Data Wrangling",
    "section": "Creating New Variables",
    "text": "Creating New Variables\n\nBase R\nWe can create new variables in base R by assigning a vector of the correct length to a new column name.\n\ncars_renamed$weight <- cars_renamed$wt\n\nIf we then drop the original column from the data frame, this gives us an alternative way of renaming columns.\n\ncars_renamed <- cars_renamed[ ,-which(names(cars_renamed) == \"wt\")]\nhead(cars_renamed, n = 5)\n\n                  miles_per_gallon cylinders displacement horse_power\nMazda RX4                     21.0         6          160         110\nMazda RX4 Wag                 21.0         6          160         110\nDatsun 710                    22.8         4          108          93\nHornet 4 Drive                21.4         6          258         110\nHornet Sportabout             18.7         8          360         175\n                  rear_axel_ratio  qsec vs am gear carb weight\nMazda RX4                    3.90 16.46  0  1    4    4  2.620\nMazda RX4 Wag                3.90 17.02  0  1    4    4  2.875\nDatsun 710                   3.85 18.61  1  1    4    1  2.320\nHornet 4 Drive               3.08 19.44  1  0    3    1  3.215\nHornet Sportabout            3.15 17.02  0  0    3    2  3.440\n\n\nOne thing to be aware of is that this operation does not preserve column ordering.\nGenerally speaking, code that relies on columns being in a specific order is fragile - it breaks easily. If possible, you should try to write your code in another way that’s robust to column reordering. I’ve done that here when removing the wt column by looking up the column index as part of my code, rather than assuming it will always be the fourth column.\n\n\ndplyr::mutate()\nThe function from {dplyr} to create new columns is mutate(). Let’s create another column that has the car’s weight in kilogrammes rather than tonnes.\n\ncars_renamed <- cars_renamed %>% \n  mutate(weight_kg = weight * 1000)\n\ncars_renamed %>% \n  select(miles_per_gallon, cylinders, displacement, weight, weight_kg) %>% \n  head(n = 5)\n\n                  miles_per_gallon cylinders displacement weight weight_kg\nMazda RX4                     21.0         6          160  2.620      2620\nMazda RX4 Wag                 21.0         6          160  2.875      2875\nDatsun 710                    22.8         4          108  2.320      2320\nHornet 4 Drive                21.4         6          258  3.215      3215\nHornet Sportabout             18.7         8          360  3.440      3440\n\n\nYou can also create new columns that combine multiple other columns\n\ncars_renamed <- cars_renamed %>% \n  mutate(cylinder_adjusted_mpg = miles_per_gallon / cylinders)\n\n\n\nrownames_to_column()\nOne useful example of adding an additional row to a data frame is to convert its row names to a column of the data fame.\n\ncars %>% \n  mutate(model = rownames(cars_renamed)) %>% \n  select(mpg, cyl, model) %>% \n  head(n = 5)\n\n                   mpg cyl             model\nMazda RX4         21.0   6         Mazda RX4\nMazda RX4 Wag     21.0   6     Mazda RX4 Wag\nDatsun 710        22.8   4        Datsun 710\nHornet 4 Drive    21.4   6    Hornet 4 Drive\nHornet Sportabout 18.7   8 Hornet Sportabout\n\n\nTherea neat function called rownames_to_column() in {tibble} which will add this as the first column and remove the row names all in one step.\n\ncars %>% \n  tibble::rownames_to_column(var = \"model\") %>% \n  head(n = 5)\n\n              model  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\nrowids_to_column()\nAnother function from {tibble} adds the row id of each observation as a new column. This is often useful when ordering or combining tables.\n\ncars %>% \n  tibble::rowid_to_column(var = \"row_id\") %>% \n  head(n = 5)\n\n  row_id  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1      1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2      2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3      3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4      4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5      5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#summaries",
    "href": "blog/2023-01-06-data-wrangling/index.html#summaries",
    "title": "Data Wrangling",
    "section": "Summaries",
    "text": "Summaries\nThe summarise() function allows you to collapse a data frame into a single row, which gives a summary statistic of your choosing.\nThis can be used to calculate a single summary\n\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm))\n\n# A tibble: 1 × 1\n  average_bill_length_mm\n                   <dbl>\n1                     NA\n\n\nSince we have missing values, we might instead want to calculate the mean of the recorded values.\n\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  average_bill_length_mm\n                   <dbl>\n1                   43.9\n\n\nWe can also use summarise() to gather multiple summaries in a single data frame.\n\nbill_length_mm_summary <- penguins %>% \n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n\nbill_length_mm_summary\n\n# A tibble: 1 × 8\n   mean median   min   q_0   q_1   q_2   q_3   q_4\n  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  43.9   44.4  59.6  32.1  39.2  44.4  39.2  59.6\n\n\nIn all this isn’t overly exciting. You might, rightly, wonder why you’d want to use these summarise() calls when we could just use the simpler base R calls directly.\nOne benefit is that the summarise calls ensure consistent output. However, the main advantage comes when you want to apply these summaries to distinct subgroups of the data."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#grouped-operations",
    "href": "blog/2023-01-06-data-wrangling/index.html#grouped-operations",
    "title": "Data Wrangling",
    "section": "Grouped Operations",
    "text": "Grouped Operations\nThe real benefit of summarise() comes from its combination with group_by(). This allows to you calculate the same summary statistics for each level of a factor with only one additional line of code. Here we’re re-calculating the same set of summary statistics we just found for all penguins, but for each individual species.\n\npenguins %>% \n  group_by(species) %>%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 9\n  species    mean median   min   q_0   q_1   q_2   q_3   q_4\n  <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Adelie     38.8   38.8  46    32.1  36.8  38.8  36.8  46  \n2 Chinstrap  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n3 Gentoo     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\n\nYou can group by multiple factors to calculate summaries for each distinct combination of levels within your data set. Here we group by combinations of species and the island to which they belong.\n\npenguin_summary_stats <- penguins %>% \n  group_by(species, island) %>%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\npenguin_summary_stats\n\n# A tibble: 5 × 10\n# Groups:   species [3]\n  species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n  <fct>     <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\n\n\nUngrouping\nBy default, each call to summarise() will undo one level of grouping. This means that our previous result was still grouped by species.\n(We can see this by examining the structure of the returned data frame. The first line tells us that this this is an S3 object of class “grouped_df”, which inherits its properties from a “tbl_df”, whose properties in turn come from “tbl” and “data.frame” objects.)\n\nclass(penguin_summary_stats)\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nSince we have grouped by two variables, R expects us to use two summaries before returning a data frame (or tibble) that is not grouped. One way to satisfy this is to use apply a second summary at the species level of grouping.\n\npenguin_summary_stats %>% \n  summarise_all(mean, na.rm = TRUE)\n\nWarning in mean.default(island, na.rm = TRUE): argument is not numeric or\nlogical: returning NA\n\nWarning in mean.default(island, na.rm = TRUE): argument is not numeric or\nlogical: returning NA\n\nWarning in mean.default(island, na.rm = TRUE): argument is not numeric or\nlogical: returning NA\n\n\n# A tibble: 3 × 10\n  species   island  mean median   min   q_0   q_1   q_2   q_3   q_4\n  <fct>      <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Adelie        NA  38.8   38.7  45.2  33.4  37.0  38.7  37.0  45.2\n2 Chinstrap     NA  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n3 Gentoo        NA  47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\n\nHowever, we won’t always want to do apply another summary. In that case, we can undo the grouping using ungroup(). Remembering to ungroup is a gotcha and cause of confusion when working with multiple-group summaries.\n\nungroup(penguin_summary_stats)\n\n# A tibble: 5 × 10\n  species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n  <fct>     <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\n\nThere’s an alternative method to achieve the same thing in a single step when using {dplyr} versions 1.0.0 and above. This is to to set the .groups parameter of the summarise() function call, which determines the grouping of the returned data frame.\nThe .groups parameter and can take 4 possible values:\n\n“drop_last”: dropping the last level of grouping (The only option before v1.0.0);\n“drop”: All levels of grouping are dropped;\n“keep”: Same grouping structure as .data;\n“rowwise”: Each row is its own group.\n\nBy default, “drop_last” is used if all the results have 1 row and “keep” is used otherwise.\n\n\nReordering Factors\nR stored factors as integer values, which it then maps to a set of labels. Only factor levels that appear in your data will be assigned a coded integer value and the mapping between factor levels and integers will depend on the order that the labels appear in your data.\nThis can be annoying, particularly when your factor levels relate to properties that aren’t numerical but do have an inherent ordering to them. In the example below, we have the t-shirt size of twelve people.\n\ntshirts <- tibble::tibble(\n  id = 1:12, \n  size = as.factor(c(\"L\", NA, \"M\", \"S\", \"XS\", \"M\", \"XXL\", \"L\", \"XS\", \"M\", \"L\", \"S\"))\n)\n\nlevels(tshirts$size)\n\n[1] \"L\"   \"M\"   \"S\"   \"XS\"  \"XXL\"\n\n\nAnnoyingly, the sizes aren’t in order and extra large is not included because it is not included in the sample. This leads to awkward summary tables (and plots).\n\ntshirts %>% group_by(size) %>% summarise(count = n())\n\n# A tibble: 6 × 2\n  size  count\n  <fct> <int>\n1 L         3\n2 M         3\n3 S         2\n4 XS        2\n5 XXL       1\n6 <NA>      1\n\n\nWe can fix this by creating a new variable with the factors explicitly coded in the correct order. We also need to sepecify that we should not drop empty groups as part of group_by().\n\ntidy_tshirt_levels <- c(\"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\", NA)\n\ntshirts %>% \n  mutate(size_tidy = factor(size, levels = tidy_tshirt_levels)) %>% \n  group_by(size_tidy, .drop = FALSE ) %>% \n  summarise(count = n())\n\n# A tibble: 7 × 2\n  size_tidy count\n  <fct>     <int>\n1 XS            2\n2 S             2\n3 M             3\n4 L             3\n5 XL            0\n6 XXL           1\n7 <NA>          1"
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#be-aware-factors",
    "href": "blog/2023-01-06-data-wrangling/index.html#be-aware-factors",
    "title": "Data Wrangling",
    "section": "Be Aware: Factors",
    "text": "Be Aware: Factors\nAs we have seen a little already, categorical variables can cause issues when wrangling and presenting data in R. All of these problems are solvable using base R techniques but the {forcats} package provides tools for the most common of these problems. This includes functions for changing the order of factor levels or the values with which they are associated.\nSome examples functions from the package include:\n\nfct_reorder(): Reordering a factor by another variable.\nfct_infreq(): Reordering a factor by the frequency of values.\nfct_relevel(): Changing the order of a factor by hand.\nfct_lump(): Collapsing the least/most frequent values of a factor into “other”.\n\nExamples of each of these can be found in the forcats vignette or the factors chapter of R for data science."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#be-aware-strings",
    "href": "blog/2023-01-06-data-wrangling/index.html#be-aware-strings",
    "title": "Data Wrangling",
    "section": "Be Aware: Strings",
    "text": "Be Aware: Strings\nWorking with and analysing text data is a skill unto itself. However, it is useful to be able to do some basic manipulation of character strings programatically.\nBecause R was developed as a statistical programming language, it is well suited to the computational and modelling aspects of working with text data but the base R string manipulation functions can be a bit unwieldy at times.\nThe {stringr} package aims to combat this by providing useful helper functions for a range of text management problems. Even when not analysing text data these can be useful, for example to remove prefixes on a lot of column names.\n\n\nWarning: `as_data_frame()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\nℹ The deprecated feature was likely used in the tibble package.\n  Please report the issue at <\u001b]8;;https://github.com/tidyverse/tibble/issues\u0007https://github.com/tidyverse/tibble/issues\u001b]8;;\u0007>.\n\n\nSuppose we wanted to keep only the text following an underscore in these column names. We could do that by using a regular expression to extract lower-case or upper-case letters which follow an underscore.\n\nhead(poorly_named_df)\n\n# A tibble: 6 × 11\n  observatio…¹   V1_A   V2_B   V3_C    V4_D    V5_E   V6_F   V7_G   V8_H    V9_I\n         <int>  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1            1 -0.199 -0.729  1.14   0.0205 -0.714  -1.14  0.786  -0.926 -0.740 \n2            2 -0.956 -1.71  -0.184  1.44   -1.97   -0.470 0.0393  1.42  -2.49  \n3            3  0.585 -1.06  -1.43   0.361   0.0141  0.585 1.01   -1.90  -0.0825\n4            4  0.624  1.17   0.263  1.72   -0.0808  0.707 1.93   -0.675  0.448 \n5            5  0.425 -0.190 -0.360 -0.443   0.134  -0.339 0.0577  1.21  -1.38  \n6            6  0.169  1.40   1.82  -0.999   1.14   -0.377 0.471   0.934  0.604 \n# … with 1 more variable: V10_J <dbl>, and abbreviated variable name\n#   ¹​observation_id\n\n\n\nstringr::str_extract(names(poorly_named_df), pattern = \"(?<=_)([a-zA-Z]+)\")\n\n [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\" \n\n\nAlternatively, can avoid using regular expressions. We can split each column name at the underscore and keep only the second part of each string.\n\n# split column names at underscores and inspect structure of resuting object\nsplit_strings <- stringr::str_split(names(poorly_named_df), pattern = \"_\")\nstr(split_strings)\n\nList of 11\n $ : chr [1:2] \"observation\" \"id\"\n $ : chr [1:2] \"V1\" \"A\"\n $ : chr [1:2] \"V2\" \"B\"\n $ : chr [1:2] \"V3\" \"C\"\n $ : chr [1:2] \"V4\" \"D\"\n $ : chr [1:2] \"V5\" \"E\"\n $ : chr [1:2] \"V6\" \"F\"\n $ : chr [1:2] \"V7\" \"G\"\n $ : chr [1:2] \"V8\" \"H\"\n $ : chr [1:2] \"V9\" \"I\"\n $ : chr [1:2] \"V10\" \"J\"\n\n# keep only the second element of each character vector in the list\npurrr::map_chr(split_strings, function(x){x[2]})\n\n [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\" \n\n\nAgain, unless you plan to work extensively with text data, I would recommend that you look up such string manipulations as you need them. The strings section of R for Data Science is a useful starting point."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#be-aware-date-times",
    "href": "blog/2023-01-06-data-wrangling/index.html#be-aware-date-times",
    "title": "Data Wrangling",
    "section": "Be Aware: Date-Times",
    "text": "Be Aware: Date-Times\nRemember all the fuss we made about storing dates in the ISO standard format? That was because dates and times are complicated enough to work with before adding extra ambiguity.\n\\[ \\text{YYYY} - \\text{MM} - \\text{DD}\\]\nDates, times and time intervals have to reconcile two factors: the physical orbit of the Earth around the sun and the social and geopolitical mechanisms that determine how we measure and record the passing of time. This makes the history of date and time records fascinating and can make working with this type of data complicated.\nMoving from larger to smaller time spans: leap years alter the number of days in a year, months are of variable length (with February’s length changing from year to year). If your data are measured in a place that uses daylight saving, then one day a year will be 23 hours long and another will be 25 hours long. To make things worse, the dates and the hour at which the clocks change are not uniform across countries, which might be in distinct time zones that themselves change over time.\nEven at the level of minutes and seconds we aren’t safe - since the Earth’s orbit is gradually slowing down a leap second is added approximately every 21 months. Nor are things any better when looking at longer time scales or across cultures, where we might have to account for different calendars: months are added removed and altered over time, other calendar systems still take different approaches to measuring time and using different units and origin points.\nWith all of these issues you have to be very careful when working with date and time data. Functions to help you with this can be found in the {lubridate} package, with examples in the dates and times chapter of R for data science."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#be-aware-relational-data",
    "href": "blog/2023-01-06-data-wrangling/index.html#be-aware-relational-data",
    "title": "Data Wrangling",
    "section": "Be Aware: Relational Data",
    "text": "Be Aware: Relational Data\nWhen the data you need are stored across two or more data frames you need to be able to cross-reference those and match up values for observational unit. This sort of data is know as relational data, and is used extensively in data science.\nThe variables you use to match observational units across data frames are known as keys. The primary key belongs to the first table and the foreign key belongs to the secondary table. There are various ways to join these data frames, depending on if you want to retain.\n\nJoin types\nYou might want to keep only observational units that have key variables values in both data frames, this is known as an inner join.\n\n\n\nInner join diagram. Source: R for Data Science\n\n\nYou might instead want to keep all units from the primary table but pad with NAs where there is not a corresponding foreign key in the second table. This results in an (outer) left-join.\n\n\n\nDiagram for left, right and outer joins. Source: R for Data Science\n\n\nConversely, you might keep all units from the second table but pad with NAs where there is not a corresponding foreign key in the primary table. This is imaginatively named an (outer) right-join.\nThe final common join type is an outer join, in which all observational units from either table are retained and all missing values are padded with NAs. This is known as an (outer) full join.\nThings get more complicated when keys do not uniquely identify observational units in one or both tables. I’d recommend you start exploring these ideas with the relational data chapter of R for Data Science.\n\n\nWhy and where to learn more\nWorking with relational data is essential to getting any data science up and running out in the wilds of reality. This is because businesses and companies don’t store all of their data in a huge single csv file. For one this isn’t very efficient, because most cells would be empty. Secondly, it’s not a very secure approach, since you can’t grant partial access to the data. That’s why information is usually stored in many data frames (more generically known as tables) within one or more databases.\nThese data silos are created, maintained, accessed and destroyed using a relational data base management system. These management systems use code to manage and access the stored data, just like we have seen in the dplyr commands above. You might well have heard of the SQL programming language (and its many variants), which is a popular language for data base management and is the inspiration for the dplyr package and verbs.\nIf you’d like to learn more then there are many excellent introductory SQL books and courses, I’d recommend picking one that focuses on data analysis or data science unless you really want to dig into efficient storage and querying of databases."
  },
  {
    "objectID": "blog/2023-01-06-data-wrangling/index.html#wrapping-up",
    "href": "blog/2023-01-06-data-wrangling/index.html#wrapping-up",
    "title": "Data Wrangling",
    "section": "Wrapping up",
    "text": "Wrapping up\nWe have:\n\nLearned how to wrangle tabular data in R with {dplyr}\nMet the idea of relational data and {dplyr}’s relationship to SQL\nBecome aware of some tricky data types and packages that can help."
  },
  {
    "objectID": "blog/2022-10-15-BBC-temperature-plot/index.html",
    "href": "blog/2022-10-15-BBC-temperature-plot/index.html",
    "title": "Data Journalism: Recreating a Professional Plot",
    "section": "",
    "text": "On Friday 2022-10-14, the BBC Data Journalism Team released this excellent article about the record temperatures in the UK during this summer’s heatwave. The article has some amazing data visualisations, and draws on a recent Met Office report.\nI wanted to try and recreate one of the plots to test the limits of my ggplot knowledge. Since I had already tackled a stacked bar plot, I figured I might have a go at their dumbbell plot that shows the weather stations which exceeded their previous records largest margins.\n\n\n\nbbc temperature records dumbbell plot\n\n\nI couldn’t find the data source, so spent far too long with a printed copy of the original figure to make my own version of the data set.\nIt took a while, but I got most of the way there with it and am happy with the final result.\n\n\n\nmy attempt at recreating the same plot\n\n\nThere were a few things that still have me stumped, that I might revisit at some later date:\n\n\nLeft aligning title and caption (Thanks to Jack Davison for this!)\n\nUsing gradients on multiple parts of the plot\n\nUsing the YeOrRd gradient, rather than default blues\n\nAdding a non-BBC logo to the bottom right.\n\nIf anyone with superior ggplot skills would like to help with those or give pointers, then I would be most grateful!\nCode and figure down below ↓\n\n\nCode\n# Load packages ----\nlibrary(bbplot)\nlibrary(tidyverse)\nlibrary(showtext)\n\n# Import fonts ----\nfont_add_google(name = \"Roboto Slab\", family = \"roboto-slab\")\nfont_add_google(name = \"Roboto\", family = \"roboto\")\nshowtext_auto()\ntitle_font <- \"roboto-slab\"\nfont <- \"roboto\"\n\n# Input data (estimated values from article) ---\ntemperatures <- tibble::tribble(\n  ~location, ~max_prev, ~max_2022,\n  \"Cranwell\", 36.6, 39.9,\n  \"Nottingham\", 36.0, 39.8,\n  \"Bramham\", 33.5, 39.8,\n  \"Sutton Boningon\", 35.9, 39.4,\n  \"Sheffield\", 35.6, 39.4,\n  \"Leeming\", 34.4, 38.7,\n  \"Goudhurst\", 34.7, 37.9,\n  \"Whitby\", 33.1, 37.8,\n  \"Bradford\", 33.9, 37.8,\n  \"High Mowthorpe\", 33.1, 37.2,\n  \"Blackpool\", 33.6, 37.2,\n  \"Durham\", 32.9, 36.9,\n  \"Preston\", 33.1, 36.5,\n  \"Morecambe\", 32.7, 36.4,\n  \"Stonyhurst\", 32.6, 36.3,\n  \"Keele\", 32.9, 36.2,\n  \"Bude\", 32.2, 36.1,\n  \"Buxton\", 32.7, 36.0,\n  \"Kielder Castle\", 29.6, 35.0,\n  \"Bala\", 31.9, 34.9\n)\n\n# Data preparation ----\n\n## For the points ----\ntemperatures <- temperatures |>\n  dplyr::mutate(max_ever = pmax(max_2022, max_prev))\n\ntemperatures$location <- forcats::fct_reorder(as.factor(temperatures$location), .x = temperatures$max_ever)\n\ntemp_long <- tidyr::pivot_longer(temperatures, cols = c(max_2022, max_prev), names_to = \"year\",values_to = \"temperature\")\n\n## For the bars ----\nn_interp <- 501\ntemp_interpolated <- tibble(rep(NA, n_interp*20))\ntemp_interpolated[[1]] <- rep(temperatures$location, each = n_interp)\ntemp_interpolated[[2]] <- rep(NA_real_, n_interp*20)\nnames(temp_interpolated) <- c(\"location\", \"interp_value\")\nfor (i in 1:20) {\n  temp_interpolated$interp_value[(1 + n_interp * (i - 1)):(n_interp*i)] <-\n    seq(temperatures$max_prev[i], temperatures$max_2022[i], length.out = n_interp)\n}\n\nstr_wrap_break <- function(x, break_limit) {\n  # Function from {usefunc} by N Rennie (https://github.com/nrennie/usefunc)\n  sapply(strwrap(x, break_limit, simplify = FALSE), paste, collapse = \"\\n\")\n}\n\ntitle_string <- \"Huge breaks from previous records in 2022\"\nsubtitle_string <- str_wrap_break(\"Stations with largest gaps between previous and new records, ordered by highest new temperature\",60)\ncaption_string <- \"Only includes active weather stations with at least 50 years of observations\"\n\n\n\n\n\np <- ggplot() +\n  geom_line(data = temp_interpolated, aes(x = interp_value, y = location, color = interp_value), lwd = 3) +\n  #\n  geom_label(aes(label = \"2022 record\", x = 38.7, y = 7.2), family = font, size = 6.5, label.size = NA) +\n  geom_curve(aes(x = 38, y = 7.9, xend = 36.9, yend = 8.9)) +\n  #\n  geom_text(aes(label = \"Previous \\n record\", x = 31, y = 11), family = font, size = 6.5) +\n  geom_curve(aes(xend = 32, yend = 11, x = 32.9, y = 8.9)) +\n  #\n  geom_label(aes(label = \"Biggest leap\", x = 33.7, y = 20), family = font, size = 6.5, label.size = NA) +\n  geom_label(aes(label = \"6.3C\", x = 34.0, y = 19), family = font, fontface=\"bold\", size = 6.5, label.size = NA) +\n  geom_curve(aes(xend = 34.7, yend = 19.5, x = 35.7, y = 18)) +\n  #\n  geom_point(data = temp_long, aes(x = temperature, y = location, fill = temperature), shape = 21, color = \"black\", size = 6) +\n  #\n  scale_x_continuous(breaks = seq(30, 40, by = 2.5),labels = paste0(seq(30, 40, by = 2.5),\"C\")) +\n  #\n  labs(title = title_string,\n       subtitle = subtitle_string,\n       caption = caption_string) +\n  #\n  theme(plot.title = element_text(family=title_font,\n                                  size=28,\n                                  face=\"bold\",\n                                  color=\"#222222\"),\n        plot.subtitle = element_text(family=font,\n                                     size=18,\n                                     margin=ggplot2::margin(9,0,9,0)),\n        plot.caption = element_text(family = font, size = 14,hjust = 0),\n        plot.title.position = 'plot',\n        plot.caption.position = 'plot',\n        axis.title = ggplot2::element_blank(),\n        axis.text = ggplot2::element_text(family=font,\n                                          size=18,\n                                          color=\"grey47\"),\n        legend.position = \"none\",\n        title = element_text(),\n        axis.line.x = element_line(size = 0.7, linetype = \"solid\"),\n        axis.ticks.y = element_blank(),\n        axis.ticks.length.x = unit(7, units = \"points\" ),\n        axis.text.x = element_text(margin=margin(t = 15, b = 10)),\n        panel.grid.minor = ggplot2::element_blank(),\n        panel.grid.major.y = ggplot2::element_line(color=\"#cbcbcb\"),\n        panel.grid.major.x = ggplot2::element_blank(),\n        panel.background = ggplot2::element_blank(),\n  )\n\np"
  },
  {
    "objectID": "blog/2022-09-26-hello-world/index.html",
    "href": "blog/2022-09-26-hello-world/index.html",
    "title": "Hello, World!",
    "section": "",
    "text": "Some Code\n\n\nCode\npar(bg = NA)\nplot(\n    x = mtcars$mpg,\n    y = mtcars$cyl,\n    xlab = \"miles per gallon\",\n    ylab = \"cyclider count\",\n    pch = 16)"
  },
  {
    "objectID": "blog/2022-10-10-statistics-books/index.html",
    "href": "blog/2022-10-10-statistics-books/index.html",
    "title": "Recommended Statistics Books",
    "section": "",
    "text": "While teaching a course on supervised learning last year, several students asked about what books I would recommended on statistical inference and modelling.\nFor context, the students on this course are all highly numerate and studying at the postgraduate level. What makes this request challenging is the broad range of student backgrounds, some students had a maths degree but the majority are trained and work as engineers, physicists or computer scientists.\nThis variety in backgrounds and exposure to undergraduate level statistics made recommending a single book difficult. Instead, I compiled a list of books that I have enjoyed or found useful. For each book I tried to give some guidance on whether it might match with their current statistical knowledge and what they are trying to achieve. I gave a brief description of the level and target audience of each text, which I reproduce below.\nWhen evaluating whether these resources might suit your current needs, I find it helpful to skim through a section on a topic that you already know (such as linear regression). This is usually the fastest and most reliable way to assess if the book is going to be a good fit for you.\nThis list is by no means exhaustive. If you know of any gems that I have not included in this list, please do let me know!\n\n\n\n\n\n\nRice (2007) covers the basics of probability and statistics usually contained in the first couple of undergraduate statistics courses. Generally the first university courses are a bit dry, building up the required knowledge to do interesting things. This book is slightly better than the average treatment in terms of readability and is fairly comprehensive, making it well suited as a reference text. This is a book full of all the stuff you might once have known but have now forgotten, or never studied before.\n\n\n\n\n\n\n\n\nKirkwood and Sterne (2010) focuses on more advanced topics in statistics, such as inference, hypothesis testing and modelling. However, it approaches these from an applications perspective. While all of the applications it uses are from medical statistics, the authors give sufficient context that you do not need to be familiar with this area before reading. This is a very readable book, with a moderate amount of mathematical detail. I find myself revisiting it quite often.\n\n\n\n\n\n\n\nWood (2015) gives an introduction to the core topics in statistics aimed at new graduate-level students. It is mathematically dense but written in an approachable manner and (unsurprisingly) covers all the core ideas of statistics. This means that is often a good source to get an overview of a topic and to cover the key points in that area quickly. It is probably wise to supplement this with a more applied text to see worked examples and to a more detailed text for topics that you need to explore in greater detail.\n\n\n\n\n\n\n\n\nPawitan (2001) focuses entirely on likelihood inference and covers both theory and applications in a great deal of detail. I highly recommend this to supplement frequentist topics covered in core statistics and the elements of statistical learning. It builds up from very little assumed knowledge but also goes on to cover some very advanced topics in later chapters.\n \n\n\n\n\n\n\n\nKendall, Stuart, and Ord (1987) is an alternative to In All Likelihood, aimed at a similar audience and level. Split over several volumes this is good to do a deep-dive into a particular topic but probably not one to try and read cover to cover!"
  },
  {
    "objectID": "blog/2022-10-10-statistics-books/index.html#bayesian-statistics",
    "href": "blog/2022-10-10-statistics-books/index.html#bayesian-statistics",
    "title": "Recommended Statistics Books",
    "section": "Bayesian Statistics",
    "text": "Bayesian Statistics\nWe only consider frequentist approaches to inference in this course. However, I would be remiss to not include some Bayesian texts and leave you with the impression that classical or frequentist approaches to statistics are the only option.\nMany of the topics we cover in supervised learning can be considered from a Bayesian perspective. A Bayesian statistician does not treat our model parameters as fixed but unknown quantities, instead they consider the parameters as random variables and use probability distributions to describe their (or our) beliefs about the parameter values.\nYou might find the following books useful, either during or after the Bayesian inference course. The former is more theoretical, while the latter has a more applied focus.\nKendall’s advanced theory of statistics. Vol. 2B, Bayesian inference. (O’Hagan and Forster 2004)\nBayesian Data Analysis - Gelman et al. (Gelman et al. 2013)"
  },
  {
    "objectID": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html",
    "href": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html",
    "title": "Good Enough Practices in Scientific Computing",
    "section": "",
    "text": "wilson2017good\nTitle: Good Enough Practices in Scientific Computing. {PLOS Computational Biology, 2017} (20 pages).\nAuthors: Greg Wilson, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt and Tracy K. Teal.\nKey words: computing, research skills, reproducibilty, guides.\nIn this paper by Wilson et al. (2017), a collection of experienced researchers and instructors give simple ways to implement good computing practices during a research project. They do this by providing a list of concrete recommendations that every researcher can adopt, regardless of their current computational skills. This is important to help the transition toward open, documented and reproducible research. The article is aimed specifically at people who are new to computational research but also contains useful guidance for more experienced researchers."
  },
  {
    "objectID": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#notes",
    "href": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#notes",
    "title": "Good Enough Practices in Scientific Computing",
    "section": "Notes",
    "text": "Notes\nThis article describes some of the best-practices in software development and how those ideas can be implemented in a reasearch project. This focus here is on implementing these approaches without requiring reseachers to learn how to use lots of peripheral technologies (for example git and LaTeX / markdown).\nAn earlier paper “Best Practices for Scientifc Computing” (Wilson et al. 2014), is aimed at those who have or would like to develop such peripheral skills."
  },
  {
    "objectID": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#suggested-best-practices",
    "href": "blog/2022-10-19-good-enough-practices-in-scientific-computing/index.html#suggested-best-practices",
    "title": "Good Enough Practices in Scientific Computing",
    "section": "Suggested Best Practices",
    "text": "Suggested Best Practices\nBest practices are grouped into 6 main themes.\n\n1. Data Management\n\nCreate the data you wish to see in the world\nRaw data should be created in a format that is ammenable to analysis and where multiple tables are used, a unique identifer used to link each record across these tables.\n\n\nKeep it backed up, keep it intact\nThis raw data should be backed up in more than one location and preserved during the analysis (i.e. not directly edited). When cleaning, handling and modelling the data keep a record of all steps used.\n\n\nShare the data\nTo allow your future self (and others) to access and cite your hard won data, submit it to a reputable DOI-issuing repository.\n\n\n\n2. Software\n\nScript files\nStart each script with a brief explanatory comment of its purpose and a description of any dependencies.\nWithin scripts, ruthlessly eliminate duplication. Do this by creating functions for any repeated operations and provide simple examples of how those functions work.\nWhen making functions and variables, give them meaningful names. As rule of thumb: fuctions are verbs, variables are nouns.\nIf you need your script to perform different actions, control this behaviour programmatically rather than by commenting/uncommenting sections of code.\n\n# Uncomment for weekly reports\noutput_dir <- paste0(\"weekly_reports/\",year,\"/\",week_of_year,\"/\")\n# Uncomment for annual reports\n#output_dir <- paste0(\"annual_reports/\",year,\"/\")\n\n\nreport_type = \"weekly\"\nyear = 2022\nweek_of_year = 21\n\nif (report_type == \"weekly\") {\n  output_dir <- paste0(\"weekly_reports/\",year,\"/\",week_of_year,\"/\")\n} else if (report_type == \"annual\") {\n  output_dir <- paste0(\"annual_reports/\",year,\"/\")\n} else {\n  stop(\"report_type should be 'weekly' or 'annual'.\")\n}\n\nSubmit the final code for your research project to to a reputable DOI-issuing repository.\n\n\nExternal Code\nBefore writing your own code, check if someone else got there first. Are there well-maintained software libraries that already do what you need?\nIf so, test the code (extensively!) before relying on it. Keep a record of what you have tested and add to this as you find awkward edge cases.\n\n\n\n3. Collaboration\n\nCollaborating within your team\nCreate a single file called README giving an overview of your project. This should describe aim of the project and how to get started working with the data/code/writing. A good rule of thumb is to write this as though it were for either a new-starter on your team. Future you will thank you!\nCreate a shared to-do list for the project in a file called TODO and decide on how you will communicate during the project. For example, what channels will you use for group meetings, quick questions, assigning tasks and setting deadlines?\n\n\nOpening up to the wider world\nAdd another file called LICENSE giving the licensing information for the project. This says who can use it and for what purposes. No license implies you are keeping all rights and nobody is allowed to reuse or modify the materials. For more information on licenses see choosealicense.com or The Open Source Guide. Consult your company’s legal folks as needed.\nCreate a final file called CITATION letting other people know how they should give proper attribution to your work if they use it.\n\n\n\n4. Project Organisation\nEach project should be self-contained in its own directory (folder) and this directory should be named after the project.\nCreate subdirectories called:\n\ndocs/ for all text documents associated with the project\ndata/raw/ for all raw data and metadata\ndata/derived/ for all data files during cleanup and analysis\nsrc for all code you write as part of this project\nbin for all external code or compiled programs that you use in this project\n\nWhen adding files and subdirectories within this structure, name these to clearly reflect their content or function.\n\n\n5. Tracking Changes\nAs soon as any file is created by a human, back it up in multiple locations. If you make a huge file, then consult your IT folks about how to store and back it up.\nAdd a file called CHANGELOG to the docs subfolder. Use this to track all changes made within the project by all contributers, describing when the changes happened and why they were made.\nKeep these changes as small as possible and share among collaborators frequently to avoid getting out of sync.\nMake a Copy the entire project whenever a significant change has been made.\nBetter yet, use a dedicated version control system such as git if that is a realistic option.\n\n\n6. Manuscripts\nPick one and stick to it within each project. The former has a much lower bar to entry and has most of the benefits of the latter (other than manuscripts being stored in the same place as everything else).\n\nWrite the manuscript using online tools with rich formatting, change tracking and reference management. (e.g. Overleaf, Google Docs)\nWrite the manuscript in plain text format the permits version control (e.g. tex + git or markdown + git)"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#introduction",
    "href": "blog/2023-01-16-minimal-R-package/index.html#introduction",
    "title": "Packaging your R code",
    "section": "Introduction",
    "text": "Introduction\nThis tutorial will guide you through the creation your first R package using the {usethis} and {testthat} packages.\nWe’ll walk through the steps of setting up a new R package, adding functions and documentation, and creating tests to ensure your package is reliable and easy to use. Whether you’re a seasoned R programmer or just getting started, this tutorial will provide you with the tools you need to create your own R package and share your work with others."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#what-is-a-package",
    "href": "blog/2023-01-16-minimal-R-package/index.html#what-is-a-package",
    "title": "Packaging your R code",
    "section": "What is a package",
    "text": "What is a package\nAn R package is a collection of R functions, data, and documentation that can be easily shared with and installed by others.\nPackages let you extending the functionality of the base R system, and are a fundamental unit of reproducible research. They can be created by anyone, and are easily distributable to others through the CRAN or GitHub.\nThe packages you create can be imported and used across multiple R scripts, making them an ideal way to share and use functions and data across projects. They can also be a good way to organize and structure your code, making it easier to test, document and maintain.\nIn this session I hope to convince you that if you can write an R function, you can write an R package."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#principle",
    "href": "blog/2023-01-16-minimal-R-package/index.html#principle",
    "title": "Packaging your R code",
    "section": "20:80 Principle",
    "text": "20:80 Principle\nProgramming and package development are huge topics. In this session we will not even cover 20% of everything there is to know. However, by focusing on the most important and the most common aspects of package development we will rapidly get a minimal package up and running.\nThese basics will cover ~80% of everything you ever need to do during package development. Resources that were used to develop this tutorial, and which will support that last ~20%, are listed below.\n\nHilary Parker blog post\nKarl Broman primer\nShannon Pileggi blog post\nR packages by Wickham and Bryan (Ch 2)\nWriting R extensions CRAN\n\n\n\n\nArtwork by Ailson Horst"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#tools-to-build-a-minimal-r-package",
    "href": "blog/2023-01-16-minimal-R-package/index.html#tools-to-build-a-minimal-r-package",
    "title": "Packaging your R code",
    "section": "Tools to build a minimal R package",
    "text": "Tools to build a minimal R package\nYou will need:\n\nR and Rstudio\n{devtools}\n{usethis}\n{testthat}\n{roxygen2}\n\nThis collection of software and R packages makes it easy to create, develop, document, test, check and share the packages you create.\n\n\n\n\n\n\nNote\n\n\n\nFor the “hardcore” folks you can do all of this by hand, but it is an absolute pain. These tools were developed for a reason."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#create-a-template-directory",
    "href": "blog/2023-01-16-minimal-R-package/index.html#create-a-template-directory",
    "title": "Packaging your R code",
    "section": "Create a template directory",
    "text": "Create a template directory\nOnce per package\n\nusethis::create_package(path = \"~/path/to_your_package/packageName\")\n\nThis should be:\n\nIn your home directory, near where your other projects live;\nNot in an existing project, package or git repo;\nNot your R library where your packages are installed.\n\nYou can find out where your R packages are installed using .libpaths(). This is where install.packages() saves packages and where library() looks for them by default.\n\n.libPaths()\n\n[1] \"/Users/zakvarty/R_libraries/4.2/cran\"                          \n[2] \"/Users/zakvarty/R_libraries/4.2/github\"                        \n[3] \"/Users/zakvarty/R_libraries/4.2/personal\"                      \n[4] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"\n\n\nNote that yours will probably only return one entry. I’ve set things up to keep my R packages separated according to where they came from and so that they save to the CRAN folder by default, because this is where I get most of my packages."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#what-to-call-your-package",
    "href": "blog/2023-01-16-minimal-R-package/index.html#what-to-call-your-package",
    "title": "Packaging your R code",
    "section": "What to call your package?",
    "text": "What to call your package?\nNaming things in hard.\nAim for something short, speakable. Snake, camel and pascal case are all acceptable, but aim for simplicity.\nSince packages group code, the package name should describe the group in some way.\n\nPersonal: zvtools, broman, ralph\nColours: rColourBrewer, PrettyCols, wesanderson\nData/Analysis type: lubridate, sp, spatstat, ismev\n\nI’ll follow the inspiration of {ismev} and name my minimal package eds, after the Effective Data Science module.\n\nusethis::create_package(path = \"~/Work/teaching/2022_data_science/eds\")\n\nThis will\n\nCreate a new R project at the specified path,\nCreates a template package within that project directory,\nOpens the project in a new RStudio session.\n\nIt will also output something like the following in the console:\n\n✔ Creating '/Users/zakvarty/Work/teaching/2022_data_science/eds/'\n✔ Setting active project to '/Users/zakvarty/Work/teaching/2022_data_science/eds'\n✔ Creating 'R/'\n✔ Writing 'DESCRIPTION'Package: eds\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R (parsed):\n    * First Last <first.last@example.com> [aut, cre] (YOUR-ORCID-ID)\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to\n    pick a license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n✔ Writing 'NAMESPACE'\n✔ Writing 'eds.Rproj'\n✔ Adding '^eds\\\\.Rproj$' to '.Rbuildignore'\n✔ Adding '.Rproj.user' to '.gitignore'\n✔ Adding '^\\\\.Rproj\\\\.user$' to '.Rbuildignore'\n✔ Opening '/Users/zakvarty/Work/teaching/2022_data_science/eds/' in new RStudio session\n✔ Setting active project to '<no active project>'\n> usethis::create_package(path = \"~/Work/teaching/2022_data_science/eds\")\n✔ Setting active project to '/Users/zakvarty/Work/teaching/2022_data_science/eds'Package: eds\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R (parsed):\n    * First Last <first.last@example.com> [aut, cre] (YOUR-ORCID-ID)\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to\n    pick a license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n\nIf we now look in the “files” pane, we will see that this template is very different from what we recommended for a general project. Before we get familiar with the structure of a package directory, let’s first check our package with devtools::check(). This function forms a key part of the package development workflow and does several things.\n\nUpdates documentation for the package and its contents\nBuilds the package\nChecks the structure and content of the built package\nReturns errors, warnings and notes to guide your next steps.\n\nYou should check your package often during development. It takes a bit of time but it is much easier to correct a few points at a time than to diagnose why a huge package is not building.\nLet’s take that advice and perform our first check.\n\ndevtools::check()\n\nWe will see a lot of output as the function works its way through over 50 different checks. Finally, the output ends with only one warning, telling us that we have not specified a (standard) license for our project within the DESCRIPTION file.\n\n...\n── R CMD check results ───────────────────────────────────── eds 0.0.0.9000 ────\nDuration: 5.3s\n\n❯ checking DESCRIPTION meta-information ... WARNING\n  Non-standard license specification:\n    `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\n  Standardizable: FALSE\n\n0 errors ✔ | 1 warning ✖ | 0 notes ✔\n\nThe license file specifies how others may use our code. We will use the permissive MIT license, which we will add using another function from {usethis}. There are similar helper functions to add other licenses, which you can investigate further at choosealicense.com\nThe {usethis} helper function will add a markdown and plain text version of the license to our directory and reference these in the DESCRIPTION.\n\nusethis::use_mit_license(copyright_holder = \"Zak Varty\")\n\n\n✔ Setting active project to '/Users/zakvarty/Work/teaching/2022_data_science/eds'\n✔ Setting License field in DESCRIPTION to 'MIT + file LICENSE'\n✔ Writing 'LICENSE'\n✔ Writing 'LICENSE.md'\n✔ Adding '^LICENSE\\\\.md$' to '.Rbuildignore'\n\nOur DESCRIPTION file should now look something like this.\n\nPackage: eds\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: What the package does (one paragraph).\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n\nWhile we are here we can fill out a few additional details. Where the template used argument order to define a person(), I have named these explicitly and removed the ORCID field.\nThe package Title should be short and in title case, while the package Description can be more detailed (a single paragraph) and split over multiple lines. Anything beyond the first line should be indented with four spaces (note: four spaces != 1 tab).\n\nPackage: eds\nTitle: Helper Functions for Effective Data Science 2022-23\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\n    given = c(\"Zak\",\"D\"),\n    family = \"Varty\",\n    email = \"my.email@imperial.ac.uk\",\n    role = c(\"aut\", \"cre\"))\nDescription: A minimal example R package created during the first lives session \n    of Effective Data Science 2022-23. This countains a simple function to \n    calculate a moving average.\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n\nLet’s now check the package again and see that this has resolved our issues\n\ndevtools::check()\n\n\n...\n✔  checking loading without being on the library search path ...\n─  checking examples ... NONE\n✔  checking for non-standard things in the check directory\n✔  checking for detritus in the temp directory\n   \n   \n── R CMD check results ─────────────────────────────────────────── eds 0.0.0.9000 ────\nDuration: 5.5s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔\n\nFantastic! Our package has no errors, warnings or notes."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#adding-a-first-function",
    "href": "blog/2023-01-16-minimal-R-package/index.html#adding-a-first-function",
    "title": "Packaging your R code",
    "section": "Adding a first function",
    "text": "Adding a first function\nThe first function we will add to the package is the helper function I wrote to left- and right-pad a vector with NAs.\n\npad_with_NAs <- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\n\nTo add this function to eds, we save it within the R/ directory. Until your package becomes large, it is good form to save each function in its own R file, named after the function. If the package becomes large, you can then think about refactoring your code to have one script for each family or group of functions.\nWe can create R/pad_with_NAs.R manually, or with a helper function from {usethis}. The helper function is useful because it will either open an existing R file or create one if it does not yet exist.\n\nusethis::use_r(\"pad_with_NAs\")\n#• Modify 'R/pad_with_NAs.R'\n#• Call `use_test()` to create a matching test file\n\nWe can then paste our function definition into this file and save it.\n\npad_with_NAs <- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#try-it-out",
    "href": "blog/2023-01-16-minimal-R-package/index.html#try-it-out",
    "title": "Packaging your R code",
    "section": "Try it out",
    "text": "Try it out\nTo try out our padding function, we need to make it available in our current R session. One way we could do this is to call source(R/pad_with_NAs.R). This doesn’t match with our experience of loading a package though - it would make pad_with_NAs() appear in our global environment pane.\nInstead, we can load all functions from the package using load_all(). This simulates the process of building, installing and loading {eds}, but is much faster than actually doing so. This speed difference becomes more and more beneficial as your package grows, allowing you to get an accurate sense of how users will experience your package even when you have functions that depend on each other or depend on functions from other packages.\n\ndevtools::load_all()\n#ℹ Loading eds\n\n\npad_with_NAs(x = 1:5, n_left = 2, n_right = 1)\n\n[1] NA NA  1  2  3  4  5 NA"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#adding-documentation",
    "href": "blog/2023-01-16-minimal-R-package/index.html#adding-documentation",
    "title": "Packaging your R code",
    "section": "Adding Documentation",
    "text": "Adding Documentation\nIt would be great if we had handy pop-ups to explain our function arguments and help pages to explain what our function does. We can add these using {roxygen2}. This package allows you to write markdown-like comments above function definitions that are automatically converted into documentation files in the man/ directory.\nTo add skeleton documentation, go to the menu and select Code > Insert Roxygen Skeleton. (This can also be done using the keyboard shortcut on that menu button when your cursor is inside the curly braces of the function.)\nNow our function padding function file should look something like this:\n\n#' Title\n#'\n#' @param x \n#' @param n_left \n#' @param n_right \n#'\n#' @return\n#' @export\n#'\n#' @examples\npad_with_NAs <- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\n\nFilling out the relevant fields, we get something like this.\n\n#' Add NAs to a vector\n#'\n#' @param x Vector to which NAs will be added.\n#' @param n_left Number of NAs to add before x.\n#' @param n_right Number of NAs to add after x.\n#'\n#' @return A vector containing x with the requested number of NA values before and after.\n#'\n#' @export\n#' @examples\n#' pad_with_NAs(1:5, n_left = 0, n_right = 3)\n#' pad_with_NAs(c(\"spider\", \"mouse\", \"cat\", \"dog\"), n_left = 1, n_right = 2)\n#'\npad_with_NAs <- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\n\nThe final step is to save these changes and then convert the comments to documentation using document().\n\ndevtools::document()\n# ℹ Updating eds documentation\n# ℹ Loading eds\n# Writing pad_with_NAs.Rd\n\nThis will allow you to preview the help file for pad_with_NAs(). I say preview here, rather than view because the documentation really only gets made when you build the package (which also adds things like links between help files and a package index). This is what the “Rendering development documentation …”” message is trying to remind you.\n\n?pad_with_NAs\n# ℹ Rendering development documentation for \"pad_with_NAs\""
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#install-your-package",
    "href": "blog/2023-01-16-minimal-R-package/index.html#install-your-package",
    "title": "Packaging your R code",
    "section": "Install your package",
    "text": "Install your package\nNow that we have a minimum viable package with a single function, let’s install the eds package. We do this using devtools::install().\n\n?devtools::install()\n\nWe can now load and use {eds} just like any other package.\n\nlibrary(eds)\nanimals <- c(\"spider\", \"mouse\",\"cat\")\npad_with_NAs(animals, n_left = 1,n_right = 0)\n\n[1] NA       \"spider\" \"mouse\"  \"cat\"   \n\n\n\nAside on setting installation path\nIf, like me, you want to install this to a non-default location then you can do this using withr::with_libpaths().\nI would like to install this to my sub-folder for personal R packages, which is the third element of my .libPaths vector.\n\n.libPaths()\n\n[1] \"/Users/zakvarty/R_libraries/4.2/cran\"                          \n[2] \"/Users/zakvarty/R_libraries/4.2/github\"                        \n[3] \"/Users/zakvarty/R_libraries/4.2/personal\"                      \n[4] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"\n\n\nI can do this by using {withr} to execute the same code but with my library paths temporarily replaced by only a single path, pointing to the personal packages sub-folder.\n\nwithr::with_libpaths(new = .libPaths()[3], code = devtools::install())\n\nThis is a bit of a handful to type repeatedly, so I’ve made a wrapper function for it in my eds package: eds::install_local()."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#functions-with-dependencies",
    "href": "blog/2023-01-16-minimal-R-package/index.html#functions-with-dependencies",
    "title": "Packaging your R code",
    "section": "Functions with dependencies",
    "text": "Functions with dependencies\n\nWithin package dependencies\nThe excellent thing about having a functions in a package is that they are all loaded together and don’t clutter the workspace.\nI created pad_with_NAs() as a helper function for rolling_average(). Whenever I loaded rolling_average() using src(), I had to remember to also source the padding function. Putting both functions in a package saves this worry. It also keeps my working environment focused on the problem I am solving (the data I want to smooth) rather than the tools I am using to solve that problem (functions and their helpers).\n\n\nBetween package dependencies\nWe’ve seen that functions within a package recognise each other and that we can make functions within our package available to users using the @export Roxygen tag. (It did this by adding export(pad_with_NAs) to the NAMESPACE file. Check if you don’t believe me.)\nWhat happens if we want to use another package’s functions in our package? We have three options, depending how many functions we want to use and how often we use them.\n\nRun usethis::use_package(\"package_name\") to declare a dependency in the DESCRIPTION file. Then use the :: notation to clearly specify the namespace (package) of the function you want to use. (I’ve been doing this same thing above to make it clear to you that some function are from devtools and others are from withr.)\nIn the Roxygen section of your function, use #' @importFrom pkg fun1 fun2 - if you prefer this over using ::. This can be useful if you use a couple of functions frequently to keep your code shorter and easier to read.\nIn the Roxygen section of your function, #' @import pkg - this imports all functions from a package and should be used very sparingly because it makes your package bulkier and increases the chance of namespace conflicts (where there is a function of the same name in two loaded packages).\n\n\nIf you are submitting your package to CRAN you need to delcalre all ALL other packages your code depends on. This includes the packages that come as standard with R (other than {base}), for example {stats}, {MASS}, and {graphics}.\n\n\n\nExample imports\n\nusethis::use_package(\"stats\")\n\n\n#' Simulate rounded Gaussian random variates\n#'\n#' @param n Number of observations. If length(n) > 1, the length is taken to be the number required.\n#' @param mu Vector of means.\n#' @param sigma Vector of standard deviations.\n#' @param digits Integer indicating the number of decimal places to be used in rounding. Negative values are used to round to a power of ten, so for example `digits = -2` rounds to the nearest hundred. (See 'Details' of `base::round()`).\n#'\n#' @return Vector of Gaussian random variates, rounded to to specified number of decimal places.\n#' @export\n#'\n#' @examples\n#' rnorm_rounded(n = 10)\nrnorm_rounded <- function(n, mu = 0, sigma = 1, digits = 0){ \n  raw_values <- stats::rnorm(n, mean = mu, sd = sigma)\n  rounded_values <- base::round(raw_values, digits)\n  return(rounded_values)\n}\n\nWe do not need to explicitly declare that round() is from base, or include in in a list of imported functions.\n\n#' Simulate rounded Gaussian random variates\n#'\n#' @param n Number of observations. If length(n) > 1, the length is taken to be the number required.\n#' @param mu Vector of means.\n#' @param sigma Vector of standard deviations.\n#' @param digits Integer indicating the number of decimal places to be used in rounding. Negative values are used to round to a power of ten, so for example `digits = -2` rounds to the nearest hundred. (See 'Details' of `base::round()`).\n#'\n#' @return Vector of Gaussian random variates, rounded to to specified number of decimal places.\n#' \n#' @importFrom stats rnorm\n#' @export\n#\"\n#' @examples\n#' rnorm_rounded(n = 10)\nrnorm_rounded <- function(n, mu = 0, sigma = 1, digits = 0){ \n  raw_values <- rnorm(n, mean = mu, sd = sigma)\n  rounded_values <- round(raw_values, digits)\n  return(rounded_values)\n}\n\nImporting the entire stats package would be overkill when we use only one function.\n\n#' Simulate rounded Gaussian random variates\n#'\n#' @param n Number of observations. If length(n) > 1, the length is taken to be the number required.\n#' @param mu Vector of means.\n#' @param sigma Vector of standard deviations.\n#' @param digits Integer indicating the number of decimal places to be used in rounding. Negative values are used to round to a power of ten, so for example `digits = -2` rounds to the nearest hundred. (See 'Details' of `base::round()`).\n#'\n#' @return Vector of Gaussian random variates, rounded to to specified number of decimal places.\n#' \n#' @import stats\n#' @export\n#\"\n#' @examples\n#' rnorm_rounded(n = 10)\nrnorm_rounded <- function(n, mu = 0, sigma = 1, digits = 0){ \n  raw_values <- rnorm(n, mean = mu, sd = sigma)\n  rounded_values <- round(raw_values, digits)\n  return(rounded_values)\n}\n\n\n\nAn import exception\nAn exception to the import procedure is the pipe from {magrittr}\nYou may want to import this to pipe within your function definitions, but having magritr::'%>%' in your code looks a right mess and defeats the readability benefits of piping.\nInstead, we have to take two steps.\n\nusethis::use_pipe()  # Creates R/utils-pipe.R and adds magrittr to DESCRIPTION\ndevtools::document() # Adds the pipe to your package's NAMESPACE"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#adding-tests",
    "href": "blog/2023-01-16-minimal-R-package/index.html#adding-tests",
    "title": "Packaging your R code",
    "section": "Adding Tests",
    "text": "Adding Tests\nWe can also add tests to the functions within our package. These tests will stay in a new directory called tests/ and will be run each time the package is built. This helps us to ensure that they currently work as we expect and that we do not break them when making alterations in the future.\nTo write and keep track of these tests, we will use {testthat}. To get started we will use the helper function usethis::use_testthat() to:\n\ncreate ⁠tests/testthat\ncreate tests/testthat.R\nadd {testthat} to the Suggests field of our package.\n\nThis needs to be done only once per package.\n\nusethis::use_testthat()\n# ✔ Adding 'testthat' to Suggests field in DESCRIPTION\n# ✔ Setting Config/testthat/edition field in DESCRIPTION to '3'\n# ✔ Creating 'tests/testthat/'\n# ✔ Writing 'tests/testthat.R'\n# • Call `use_test()` to initialize a basic test file and open it for editing.\n\nEach of our files of tests will live in the tests/testthat subdirectory in a file named after the function. We can create this file of tests by running usethis::use_test() while we have any of our package functions open. This will either open or create the relevant text file for us to edit or populate.\nThis needs to be done (at least) once for each function that we want to write tests for. Let’s focus on writing tests for pad_with_NAs().\nWith R/pad_with_NAs.R open, run usethis::use_test() to create and open a file to store the tests. Note that the naming convention here is different to our previous convention.\n\nusethis::use_test()\n# ✔ Writing 'tests/testthat/test-pad_with_NAs.R'\n# • Modify 'tests/testthat/test-pad_with_NAs.R'\n\nThis will have a dummy test included by default.\n\ntest_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n\nLet’s change this to match one of our examples and add another test to match the other example. To try out these tests as you write them, you’ll want to attach {testthat} to the current R session and make your package function available with load_all().\n\nlibrary(testthat)\ndevtools::load_all()\n# ℹ Loading eds\n\nIf you then try running the dummy test you should get something like:\n\ntest_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n# Test passed 😀\n\nWe can replace this by some simple checks that the funciton behaves as expected when given examples we wrote in the documentation and when we pad with zero NAs.\n\ntest_that(\"padding by 0 works\", {\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 1, n_right = 0), c(NA, 1, 2, 3))\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 0, n_right = 1), c(1, 2, 3, NA))\n})\n\ntest_that(\"examples work\", {\n  expect_equal(\n    object = pad_with_NAs(1:5, n_left = 0, n_right = 3), \n    expected = c(1:5, NA, NA, NA))\n  expect_equal(\n    object = pad_with_NAs(c(\"spider\", \"mouse\", \"cat\", \"dog\"), n_left = 1, n_right = 2),\n    expected = c(NA, \"spider\", \"mouse\", \"cat\", \"dog\", NA)\n  )\n})\n\nWe can run these tests individually, as we did before, or we can run all test in the file using the testthat::test_file().\n\ntestthat::test_file(\"tests/testthat/test-pad_with_NAs.R\")\n\n# [ FAIL 1 | WARN 0 | SKIP 0 | PASS 3 ]\n#\n# ── Failure (test-pad_with_NAs.R:10): examples work ─────────────────────────────\n# pad_with_NAs(...) (`actual`) not equal to \n# c(NA, \"spider\", \"mouse\", \"cat\", \"dog\", NA) (`expected`).\n#\n# `actual[4:7]`:   \"cat\" \"dog\" NA NA\n# `expected[4:6]`: \"cat\" \"dog\" NA   \n#\n# [ FAIL 1 | WARN 0 | SKIP 0 | PASS 3 ]\n\nFrom the test output, we can see that three of the tests are passing but one is failing. Using the additional output, we can figure out that I forgot the second NA in the test of the string example. Let’s add that back in.\n\ntest_that(\"padding by 0 works\", {\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 1, n_right = 0), c(NA, 1, 2, 3))\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 0, n_right = 1), c(1, 2, 3, NA))\n})\n\ntest_that(\"examples work\", {\n  expect_equal(\n    object = pad_with_NAs(1:5, n_left = 0, n_right = 3), \n    expected = c(1:5, NA, NA, NA))\n  expect_equal(\n    object = pad_with_NAs(c(\"spider\", \"mouse\", \"cat\", \"dog\"), n_left = 1, n_right = 2),\n    expected = c(NA, \"spider\", \"mouse\", \"cat\", \"dog\", NA, NA)\n  )\n})\n\nWe can also add some input checks to the function, in case the user tries to do something unexpected. We’ll add input checks to cover two cases where the user tries to:\n\nuse negative indexing to pad inwards with NAs;\npad an object that is not a vector.\n\nTo do this we will use the stopifnot() function. This is a useful shorthand to stop() function execution and return an error message based on a logical statement. For more detailed control of the error message or for less serious failings, which require only a warning() or a message(). Learn more about these in the Advanced R conditions chapter.\n\n#' Add NAs to a vector\n#'\n#' @param x Vector to which NAs will be added.\n#' @param n_left Number of NAs to add before x.\n#' @param n_right Number of NAs to add after x.\n#'\n#' @return A vector containing x with the requested number of NA values before and after.\n#'\n#' @export\n#' @examples\n#' pad_with_NAs(1:5, n_left = 0, n_right = 3)\n#' pad_with_NAs(c(\"spider\", \"mouse\", \"cat\", \"dog\"), n_left = 1, n_right = 2)\n#'\npad_with_NAs <- function(x, n_left, n_right){\n  # Input checks\n  stopifnot(n_left >= 0)\n  stopifnot(n_right >= 0)\n  stopifnot(class(x) %in% c(\"character\", \"complex\", \"integer\", \"logical\", \"numeric\", \"factor\"))\n\n  # Function Body\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\n\nWe can then add additional tests to confirm that we get errors, warnings or messages where we expect them. As our test file grows, we might want to organise our tests according to their purpose.\n\n# Typical Behaviour\n\ntest_that(\"examples work\", {\n  expect_equal(\n    object = pad_with_NAs(1:5, n_left = 0, n_right = 3),\n    expected = c(1:5, NA, NA, NA))\n  expect_equal(\n    object = pad_with_NAs(c(\"spider\", \"mouse\", \"cat\", \"dog\"), n_left = 1, n_right = 2),\n    expected = c(NA, \"spider\", \"mouse\", \"cat\", \"dog\", NA, NA)\n  )\n})\n\n# Edge Cases\n\ntest_that(\"padding by 0 works\", {\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 1, n_right = 0), c(NA, 1, 2, 3))\n  expect_equal(pad_with_NAs(x = 1:3, n_left = 0, n_right = 1), c(1, 2, 3, NA))\n})\n\n# Malformed Inputs\n\ntest_that(\"negative padding produces error\", {\n  expect_error(pad_with_NAs(x = 1:5, n_left = -1, n_right = 1))\n  expect_error(pad_with_NAs(x = 1:5, n_left = 1, n_right = -1))\n})\n\ntest_that(\"giving non-vector x produces error\", {\n  matrix_input <- diag(1:4)\n  list_input <- x = list(1:5, \"cat\")\n\n  expect_error(pad_with_NAs(x = matrix_input, n_left = 1, n_right = 1))\n  expect_error(pad_with_NAs(x = list_input, n_left = 1, n_right = 1))\n})\n\nWe can continue to run test files individually, but they will all be run when we call devtools::check().\n\ntestthat::test_file(\"tests/testthat/test-pad_with_NAs.R\")\n# [ FAIL 0 | WARN 0 | SKIP 0 | PASS 8 ]"
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#wrapping-up",
    "href": "blog/2023-01-16-minimal-R-package/index.html#wrapping-up",
    "title": "Packaging your R code",
    "section": "Wrapping up",
    "text": "Wrapping up\nTo wrap up let’s run one last check and install the package.\n\ndevtools::check()\n# ─ R CMD check results ─────────────────────────────────── eds 0.0.0.9000 ────\n# Duration: 12.1s\n#\n# 0 errors ✔ | 0 warnings ✔ | 0 notes ✔\n\nCongratulations, you’ve created your first R package!\nYou should now have a solid understanding of how to set up a new R package, add functions and documentation by using {usethis} and {devtools}. You also know how to use {testthat} and {roxygen22} to create tests to ensure your package is reliable and easy to use.\nYou should now be able to create your own R package and might want to consider using git to track its development and sharing it with others on Github.\nRemember, creating an R package is an iterative process, and it may take several rounds of testing and editing before you are satisfied with the final product. But with a little patience and persistence, you’ll be able to create a package that you and others can use with confidence."
  },
  {
    "objectID": "blog/2023-01-16-minimal-R-package/index.html#follow-up-tasks",
    "href": "blog/2023-01-16-minimal-R-package/index.html#follow-up-tasks",
    "title": "Packaging your R code",
    "section": "Follow up Tasks",
    "text": "Follow up Tasks\n\nAdd input checks and tests for my rolling_mean function.\nAdd your own function for finding the geometric mean of a vector.\nAdd a readme file for your package, using usethis::use_readme_rmd() and the relevant section of the R packages book for guidance.\n\n–>"
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html",
    "href": "blog/2022-12-13-git-remote-branches/index.html",
    "title": "Git: putting your new feature out into the world.",
    "section": "",
    "text": "So you’ve added a new feature to your project. You were responsible and didn’t do this in the main branch. Congrats on being responsible, have some brownie points.\nNow how the heck to you get everything moved across and delete your new-feature branch?"
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#check-that-your-local-main-is-up-to-date.",
    "href": "blog/2022-12-13-git-remote-branches/index.html#check-that-your-local-main-is-up-to-date.",
    "title": "Git: putting your new feature out into the world.",
    "section": "1. Check that your local main is up to date.",
    "text": "1. Check that your local main is up to date.\nBe in the main branch and pull any changes to the remote main branch.\ngit checkout main \ngit pull origin main\nThis might require you to resolve some merge conflicts, but these should be pretty straightforward if you are following a branch-and-merge workflow."
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#check-that-you-remote-new-feature-is-up-to-date.",
    "href": "blog/2022-12-13-git-remote-branches/index.html#check-that-you-remote-new-feature-is-up-to-date.",
    "title": "Git: putting your new feature out into the world.",
    "section": "2. Check that you remote new-feature is up to date.",
    "text": "2. Check that you remote new-feature is up to date.\nWe might mess things up while merging. Let’s make sure that if that happens we can get back to this good position with our new feature.\ngit checkout new-feature\ngit status\nIf needed: add, commit and push."
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#merge-any-changes-to-main-into-your-local-new-feature-branch",
    "href": "blog/2022-12-13-git-remote-branches/index.html#merge-any-changes-to-main-into-your-local-new-feature-branch",
    "title": "Git: putting your new feature out into the world.",
    "section": "3. Merge any changes to main into your local new-feature branch",
    "text": "3. Merge any changes to main into your local new-feature branch\nNext, we will make sure we have any changes to main moved across to our local new-feature branch.\ngit merge main\nThis might again require resolving some merge conflicts. Keep calm and take tea breaks are required."
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#commit-and-push-to-remote.",
    "href": "blog/2022-12-13-git-remote-branches/index.html#commit-and-push-to-remote.",
    "title": "Git: putting your new feature out into the world.",
    "section": "4. Commit and push to remote.",
    "text": "4. Commit and push to remote.\nNow that we have our local new-feature branch compatible with the remote main branch, lets push that to the remote.\ngit add <YOUR_FILES_TO_COMMIT>\ngit commit -m \"merge changes to main in preparation for PR\"\ngit push"
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#open-a-pull-request-on-github",
    "href": "blog/2022-12-13-git-remote-branches/index.html#open-a-pull-request-on-github",
    "title": "Git: putting your new feature out into the world.",
    "section": "5. Open a pull request on Github",
    "text": "5. Open a pull request on Github\nWait for someone to review approve your new feature (or wait a few hours/days and do it yourself for a solo project)."
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#delete-the-local-branch",
    "href": "blog/2022-12-13-git-remote-branches/index.html#delete-the-local-branch",
    "title": "Git: putting your new feature out into the world.",
    "section": "6. Delete the local branch",
    "text": "6. Delete the local branch\ngit checkout main\ngit branch --delete new-feature"
  },
  {
    "objectID": "blog/2022-12-13-git-remote-branches/index.html#delete-the-remote-branch",
    "href": "blog/2022-12-13-git-remote-branches/index.html#delete-the-remote-branch",
    "title": "Git: putting your new feature out into the world.",
    "section": "7. Delete the remote branch",
    "text": "7. Delete the remote branch\ngit push origin --delete new-feature\nOn older versions of git (< 1.7.0) you might need to use the alternative syntax below. This is effectively pushing nothing to the new-feature branch of origin.\ngit push origin :new-feature\nBam! You did it! Your new-feature is out there in the wild, making the world a marginally better place."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html",
    "href": "blog/2022-10-07-rhetorical-precis/index.html",
    "title": "Writing a rhetorical précis",
    "section": "",
    "text": "Photo by Maksym Kaharlytskyi on Unsplash"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#what-is-a-rhetorical-precis",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#what-is-a-rhetorical-precis",
    "title": "Writing a rhetorical précis",
    "section": "What is a rhetorical precis?",
    "text": "What is a rhetorical precis?\n\n\n\nA rhetorical precis is a short summary and analysis of a piece of writing, which considers both the content and the delivery of the piece.\nA rhetorical precis serves to summarise and analyse the text through:\n\nan accurate bibliographic reference to the text,\na list of keywords relating to the text,\na highly structured four-sentence paragraph providing a summary and analysis of the text."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#why-write-one",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#why-write-one",
    "title": "Writing a rhetorical précis",
    "section": "Why write one?",
    "text": "Why write one?\nKeeping a rhetorical precis for each text that you read is a fantasitc way to build the skills of active reading and succinct writing. A rhetorical precis is more informative than a bib entry and more easily reviewed (read: waded through) than a stack of annotated papers.\nTaken collectively, a set of rhetorical precis summaries provide a reading record that can be a tremendously useful when trying to recall the contents of a paper or book long after you originally read it."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#how-to-store-them",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#how-to-store-them",
    "title": "Writing a rhetorical précis",
    "section": "How to store them?",
    "text": "How to store them?\nWriting and storing these reading summaries electronically can make them even more useful. This allows you to search for topics, target audiences or keywords.\nFor this reason it can be helpful to keep them all together in one word document or plain text file. Alternatively, having a single folder with each summary as a plain text or markdown file works well if you are comfortable with searching at the command line. The same can be achieved by writing these summaries within a reference manager, if that is something you are invested in already."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#definition",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#definition",
    "title": "Writing a rhetorical précis",
    "section": "Definition",
    "text": "Definition\nJust to prove that I’m not making all this up:\n\nA rhetorical precis analyzes both the content (the what) and the delivery (the how) of a unit of spoken or written discourse. It is a highly structured four-sentence paragraph blending summary and analysis. Each of the four sentences requires specific information; students are expected to use brief quotations (to convey a sense of the author’s style and tone) and to include a terminal bibliographic reference. Practicing this sort of writing fosters precision in both reading and writing, forcing a writer to employ a variety of sentence structures and to develop a discerning eye for connotative shades of meaning.  Attribution: lumenlearning.com"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#format",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#format",
    "title": "Writing a rhetorical précis",
    "section": "Format",
    "text": "Format\nFour sentences summarising the aim of the work, how this is addressed, why it is important and a description of the target audience.\n\nName of author, [optional phrase describing author], genre and title of work, date in parentheses (additional publishing information in parentheses); a rhetorically accurate verb (such as “asserts,” “argues,” suggests,” “implies,” claims,” etc.); a THAT clause containing the major assertion or thesis statement of the work.\nAn explanation of how the author develops and/or supports the thesis, usually in chronological order.\nA statement of the author’s purpose followed by an “in order to” phrase.\nA description of the intended audience and/or the essay’s tone"
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#a-self-indulgent-example",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#a-self-indulgent-example",
    "title": "Writing a rhetorical précis",
    "section": "A (self-indulgent) example",
    "text": "A (self-indulgent) example\nHere is a rather self-indulgent example of a rhetorical precis.\n\nvarty2021inference\nTitle: Inference for extreme earthquake magnitudes accounting for a time-varying measurement process. {ArXiV preprint, 2021} (20 pages).\nAuthors: Zak Varty, Jonathan Tawn, Peter Atkinson and Stijn Bierman.\nKey words: extreme value, earthquake, threshold selection, magnitude of completion, seismology, bootstrap.\nIn this paper, Varty et al (2021) propose a new threshold selection method for modelling earthquake catalogues, where the magnitude distribution is stationary but detection of small events improves over time. The paper generalises the Gutenberg-Richter law to the GPD and uses metrics based on PP and QQ plots to balance between bias and variance when selecting a time-varying threshold. This procedure more than doubles the usable catalogue size for Groningen earthquakes and gives the first emprircal evidence that the magnitude distribution in this region has a finite upper end point. The paper is targeted at applied and research statisticians with an interest in EVT but would also be accessible to a statistically-minded seismologist."
  },
  {
    "objectID": "blog/2022-10-07-rhetorical-precis/index.html#a-template-for-new-entries",
    "href": "blog/2022-10-07-rhetorical-precis/index.html#a-template-for-new-entries",
    "title": "Writing a rhetorical précis",
    "section": "A template for new entries",
    "text": "A template for new entries\n\nfirstauthorYYYYkeyword\nTitle: Title goes here. {Journal, YYYY} (NN pages).\nAuthors: Author One, Author Two and Author Three. (optional affiliations)\nKey words: key word 1, key word 2, key work 3.\n\nWhat is the document and what does it say?\nHow do they do / show this?\nWhy are they bothering to do this in the first place?\nWho is the intended audience for this work?\n\nIn this DOC_TYPE, AUTHOUR VERB that THESIS_STATEMENT. They DO/SHOW this by ACTIONS. This is important to PEOPLE because REASONS. This work would be useful when PEOPLE are doing ACTIVITY.\n\n`firstauthorYYYYkeyword`\n\n**Title:** _Title goes here. {Journal, YYYY} (NN pages)._\n\n**Authors:** _Author One, Author Two and Author Three. (optional affiliations)_\n\n**Key words:** _key word 1_, _key word 2_, _key work 3_. \n\n1. _What_ is the document and _what_ does it say? \n2. _How_ do they do / show this?\n3. _Why_ are they bothering to do this in the first place?\n4. _Who_ is the intended audience for this work?\n\nIn this DOC_TYPE, AUTHOUR VERB that THESIS\\_STATEMENT.\nThey DO/SHOW this by ACTIONS. \nThis is important to PEOPLE because REASONS. \nThis work would be useful when PEOPLE are doing ACTIVITY."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Zak Varty",
    "section": "",
    "text": "Zak Varty is a statistician, data scientist and educator.\nHe is a Teaching Fellow in Statistics at Imperial College London where he teaches courses and supervises research projects in statistics, data science and data ethics.\nOutside of work, Zak enjoys spending time reading, running and baking tasty treats."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Zak Varty",
    "section": "Education",
    "text": "Education\nPhD Statistics and Operational Research | 2017 - 2021\nTitle: Statistical Modelling of Induced Earthquakes\nInstitution: Lancaster University, UK.\nMRes Statistics and Operational Research | 2016 - 2017\nGrade: Distinction\nInstitution: Lancaster University, UK.\nMSci Mathematics and Statistics (Study Abroad) | 2012 - 2016\nGrade: First Class (Honours)\nInstitution: Lancaster University, UK and University of Western Ontario, CA."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Zak Varty",
    "section": "Experience",
    "text": "Experience\nTeaching Fellow, Imperial College London | Aug 2021 - present\nI teach on the online MSc in Machine Learning and Data Science course, support undergraduate student development, and supervise research projects at the undergraduate and postgraduate level.\nLecturer, Lancaster University | Feb 2021 - Apr 2021\nFixed term lecturship in department of Mathematics and Statistics. I adapted for online delivery and co-delivered a postgraduate course in extreme value theory.\nStatistical Contractor, Shell Global Solutions, NL. | Oct 2019 - Jan 2020\nFour month placement as a statistical contractor. I worked within the statistics and data-science teams on two projects related to corrosion management.\nIntern, Huawei Technologies, CHN. | Jul - Aug 2016\nFully sponsored internship in Beijing and Shenzhen for high performing STEM undergraduates. Immersive education in large-scale ICT infrastructure, intercultural relations and international business management.\nReseach Intern, Lancaster University, UK. | Jul - Sept 2015\nSummer research project within STOR-i CDT, investigating modelling techniques to estimate pharmacokinetic parameters from small data sets."
  },
  {
    "objectID": "about.html#outreach",
    "href": "about.html#outreach",
    "title": "Zak Varty",
    "section": "Outreach",
    "text": "Outreach\nI was an active member of the Royal Statistical Society’s Lancashire & East Cumbria local group from 2017-2020. The group organises seminars and events to bring together those with an interest in data both within and outside of academia. I served as treasurer for the group for two years from 2017-2019.\nI have also been involved in delivering university open days since 2016. At these events I have delivered mini-lectures to promote statistical thinking, raise awareness of available funding, and to promote the option of studying abroad."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "R\n\n\ndata science\n\n\ntutorial\n\n\n\n\nA tutorial to guide you as you make your first (or hundredth) minimal R package.\n\n\n\n\n\n\nJan 19, 2023\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata science\n\n\ndata wrangling\n\n\ntutorial\n\n\n\n\nGet your data ducks in a line using base R and the tidyverse.\n\n\n\n\n\n\nJan 6, 2023\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata science\n\n\ndata acquisition\n\n\ntutorial\n\n\n\n\nPart 2 of a mini-series on aquiring data from the web, focusing on data aquisition via APIs.\n\n\n\n\n\n\nDec 14, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nworkflows\n\n\ngit\n\n\n\n\nA git workflow to put a local feature out in the wild, then tidy up after yourself.\n\n\n\n\n\n\nDec 13, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata science\n\n\ndata acquisition\n\n\ntutorial\n\n\n\n\nPart 1 of a mini-series on aquiring data from the web, focusing on scraping webpages using the {rvest} package.\n\n\n\n\n\n\nDec 1, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nacademic skills\n\n\ncomputing\n\n\nreproducible research\n\n\nreading summary\n\n\n\n\nReading Summary of Wilson et al. (2017).\n\n\n\n\n\n\nOct 19, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata visualisation\n\n\n\n\nRecreating a plot of record breaking temperatures by the BBC data journalism team.\n\n\n\n\n\n\nOct 15, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata visualisation\n\n\ntidy tuesday\n\n\n\n\nTidy Tuesday 2022 || Week 41\n\n\n\n\n\n\nOct 12, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nacademic skills\n\n\nreading\n\n\n\n\nBook recommendations for those looking to reinforce their knowledge of undergraduate and advanced statistics.\n\n\n\n\n\n\nOct 10, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nacademic skills\n\n\nwriting\n\n\nreading\n\n\n\n\nStructured summaries to remember and retrieve what you read\n\n\n\n\n\n\nOct 7, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nquarto\n\n\ntutorial\n\n\n\n\nAdding a blog within a quarto website\n\n\n\n\n\n\nSep 26, 2022\n\n\nZak Varty\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nQuarto\n\n\nTemplates\n\n\n\n\nA minimal first post\n\n\n\n\n\n\nSep 26, 2022\n\n\nZak Varty\n\n\n\n\n\n\nNo matching items"
  }
]